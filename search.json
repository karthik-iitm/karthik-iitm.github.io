[
  {
    "objectID": "topics/linear_algebra/notes/inverse.html",
    "href": "topics/linear_algebra/notes/inverse.html",
    "title": "Inverse and Determinant",
    "section": "",
    "text": "Let \\(A\\) be a square matrix of order \\(n\\). \\(A\\) is invertible if and only if \\(\\text{det}(A)\\neq0\\).",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "Inverse and Determinant"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/inverse.html#invertible-implies-non-zero-determinant",
    "href": "topics/linear_algebra/notes/inverse.html#invertible-implies-non-zero-determinant",
    "title": "Inverse and Determinant",
    "section": "Invertible \\(\\implies\\) Non-zero determinant",
    "text": "Invertible \\(\\implies\\) Non-zero determinant\nIf \\(A\\) is invertible, \\(A^{-1}\\) exists. We have:\n\\[\n\\begin{aligned}\nAA^{-1} & =I\\\\\n\\\\\\text{det}(A)\\cdot\\text{det}\\left(A^{-1}\\right) & =1\n\\end{aligned}\n\\]\nSince the product of two real numbers is \\(1\\), both of them have to be non-zero, establishing that \\(\\text{det}(A)\\neq0\\). We can compute the determinant of \\(A^{-1}\\) and it is \\(\\cfrac{1}{\\text{det}(A)}\\).",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "Inverse and Determinant"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/inverse.html#non-zero-determinant-implies-invertible",
    "href": "topics/linear_algebra/notes/inverse.html#non-zero-determinant-implies-invertible",
    "title": "Inverse and Determinant",
    "section": "Non-zero determinant \\(\\implies\\) Invertible",
    "text": "Non-zero determinant \\(\\implies\\) Invertible\nWe use the following identity:\n\\[\nA \\cdot \\text{adj}(A)=\\text{adj}(A)\\cdot A=\\text{det}(A)\\cdot I\n\\]\nwhere \\(\\text{adj}(A)\\) is the adjugate of \\(A\\). If \\(\\text{det}(A)\\neq0\\), we can divide the above identity uniformly by \\(\\text{det}(A)\\) to get:\n\\[\n\\begin{aligned}\nA\\cdot\\left[\\cfrac{1}{\\text{det}(A)}\\text{adj}(A)\\right] & =\\left[\\cfrac{1}{\\text{det}(A)}\\text{adj}(A)\\right]A=I\n\\end{aligned}\n\\]\nWe now have a matrix \\(B\\) such that \\(AB=BA=I\\). It follows that \\(A\\) is invertible. Specifically, the inverse of \\(A\\) can be represented as:\n\\[\nA^{-1}=\\cfrac{1}{\\text{det}(A)}\\text{adj}(A)\n\\]",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "Inverse and Determinant"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/rref.html",
    "href": "topics/linear_algebra/notes/rref.html",
    "title": "RREF is unique",
    "section": "",
    "text": "The reduced row echelon form of a matrix is unique. The proof requires an observation about the row spaces of a matrix and its RREF.",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "RREF is unique"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/rref.html#row-operations-preserve-row-space",
    "href": "topics/linear_algebra/notes/rref.html#row-operations-preserve-row-space",
    "title": "RREF is unique",
    "section": "Row operations preserve row space",
    "text": "Row operations preserve row space\nIf a matrix \\(A\\) has \\(n\\) rows, say \\(r_{1},\\cdots,r_{n}\\), then the span of the rows is the set of all possible linear combinations of the rows and is called the row space of the matrix:\n\\[\n\\text{span}\\{r_{1},\\cdots,r_{n}\\}\n\\]\nSwapping two rows or scaling a row by a non-zero constant clearly preserves this span. Let us now try to add one row to the other, the third type of elementary operation. Without loss of generality, let us say we add the second row to the first. The span becomes:\n\\[\n\\text{span}\\{r_{1}+r_{2},r_{2},\\cdots,r_{n}\\}\n\\]\nWe need to show the following:\n\\[\n\\text{span}\\{r_{1}+r_{2},r_{2},\\cdots,r_{n}\\}=\\text{span}\\{r_{1},\\cdots,r_{n}\\}\n\\]\nWe will denote these two sets \\(R_{1}\\) and \\(R_{2}\\). Let us take any element \\(R_{1}\\):\n\\[\n\\begin{aligned}\na_{1}(r_{1}+r_{2})+a_{2}r_{2}+\\cdots+a_{n}r_{n} & =a_{1}r_{1}+(a_{1}+a_{2})r_{2}+a_{3}r_{3}+\\cdots+a_{n}r_{n}\n\\end{aligned}\n\\]\nWe see that this element is present in \\(R_{2}\\) as well. Therefore:\n\\[\n\\text{span}\\{r_{1}+r_{2},r_{2},\\cdots,r_{n}\\}\\subset\\text{span}\\{r_{1},\\cdots,r_{n}\\}\n\\]\nTo go the other way, take any element in \\(R_{2}\\):\n\\[\n\\begin{aligned}\na_{1}r_{1}+\\cdots+a_{n}r_{n} & =a_{1}(r_{1}+r_{2})+(a_{2}-a_{1})r_{2}+a_{3}r_{3}+\\cdots+a_{n}r_{n}\n\\end{aligned}\n\\]\nWe see that this element is there in \\(R_{1}\\) as well. Therefore:\n\\[\n\\text{span}\\{r_{1},\\cdots,r_{n}\\}\\subset\\text{span}\\{r_{1}+r_{2},r_{2},\\cdots,r_{n}\\}\n\\]\nThus, all elementary operations preserve the span of the rows of the original matrix. An important consequence of this is that the row space of a matrix is equal to that of its RREF.",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "RREF is unique"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/rref.html#non-zero-rows-in-rref-form-a-basis-for-row-space",
    "href": "topics/linear_algebra/notes/rref.html#non-zero-rows-in-rref-form-a-basis-for-row-space",
    "title": "RREF is unique",
    "section": "Non-zero rows in RREF form a basis for row space",
    "text": "Non-zero rows in RREF form a basis for row space\nA concrete example will be used below, but it is quite easy to generalize it:\n\\[\n\\begin{bmatrix}\\boldsymbol{1} &  & 0 &  & 0\\\\\n0 & 0 & \\boldsymbol{1} &  & 0\\\\\n0 & 0 & 0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nAll entries that are not filled are irrelevant to the discussion and could take arbitrary real values. Notice that the non-zero rows are linearly independent. If we call the non-zero rows \\(r_{1},r_{2},r_{3}\\), we have:\n\\[\nar_{1}+br_{2}+cr_{3}=0\\implies a=b=c=0\n\\]\nWe already know that row operations preserve the span of the rows. Hence, the non-zero rows form a basis for the row space.",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "RREF is unique"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/rref.html#proof",
    "href": "topics/linear_algebra/notes/rref.html#proof",
    "title": "RREF is unique",
    "section": "Proof",
    "text": "Proof\nAssume that \\(R_{1}\\) and \\(R_{2}\\) are two RREFs for the matrix \\(A\\). From the observations made so far, \\(R_{1}\\) and \\(R_{2}\\) have the same number of non-zero rows and the span of the non-zero rows of the two matrices are equal, each being equal to the row space of \\(A\\).\nThe pivot columns in \\(R_{1}\\) and \\(R_{2}\\) have to be identical, both in number and position. The number of pivots is equal to the number of non-zero rows, which is the dimension of the row space. So this can’t be different in the RREFs. This leaves us with the possibility of the position of the pivot columns being different. Let us look at a potential contradiction to this:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\\boldsymbol{1} &  & 0 &  & 0\\\\\n0 & 0 & \\boldsymbol{1} &  & 0\\\\\n0 & 0 & 0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix} &  & \\begin{bmatrix}\\boldsymbol{1} & 0 &  &  & 0\\\\\n0 & \\boldsymbol{1} & \\boldsymbol{} &  & 0\\\\\n0 & 0 & 0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{aligned}\n\\]\nAssume that these two matrices are two distinct RREFs for some matrix \\(A\\) with the same number of pivot columns, but at different locations. We know that the row space has dimension \\(3\\), so any set of four vectors in the row space are dependent. If we take the first three rows from the matrix on the left and the second row from the matrix on the right, these four vectors make a linearly independent subset of the row space, which is a contradiction. So the position of the pivot columns have to be the same.\nWe are now back to two RREFs whose general outline looks like this:\n\\[\n\\begin{bmatrix}\\boldsymbol{1} &  & 0 &  & 0\\\\\n0 & 0 & \\boldsymbol{1} &  & 0\\\\\n0 & 0 & 0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nOnce the pivot columns are fixed for both the RREFs consider the last non-zero row, \\(r_{3}^{(2)}\\), from the second RREF. This should be expressible as a linear combination of the non-zero rows of the first RREF:\n\\[\nr_{3}^{(2)}=a_{1}r_{1}^{(1)}+a_{2}r_{2}^{(1)}+a_{3}r_{3}^{(1)}\n\\]\nWe immediately see that \\(a_{1}=a_{2}=0\\). \\(a_{3}\\) is forced to be \\(1\\) as the pivots have to agree. Thus the last non-zero row is identical in both RREFs. We can now repeat the process for the second non-zero row:\n\\[\nr_{2}^{(2)}=a_{1}r_{1}^{(1)}+a_{2}r_{2}^{(1)}+a_{3}r_{3}^{(1)}\n\\]\n\\(a_{1}=0\\), \\(a_{2}=1\\) and \\(a_{3}=0\\). So the last two rows of the two RREFs are the same. A similar argument shows that all non-zero rows in the two RREFs are the same, which means that the two RREFs are one and the same.",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "RREF is unique"
    ]
  },
  {
    "objectID": "topics/linear_algebra/problems/problem-1.html",
    "href": "topics/linear_algebra/problems/problem-1.html",
    "title": "Problem-1",
    "section": "",
    "text": "Let \\(W\\) be a finite dimensional real vector space, and let \\(U\\) and \\(V\\) be two subspaces of \\(W\\). Let \\(U+V\\) be the space\n\\[\nU+V=\\{u+v\\,:\\,u\\in U\\text{ and }v\\in V\\}\n\\]\nShow the following:\n\n\\(U+V\\) is a subspace of \\(W\\).\n\\(\\text{dim}(U+V)=\\text{dim}(U)+\\text{dim}(V)-\\text{dim}(U\\cap V)\\)\n\n\nWe see that \\(0\\in U+V\\). If \\(u_{1}+v_{1}\\in U+V\\) and \\(u_{2}+v_{2}\\in U+V\\), then \\((u_{1}+u_{2})+(v_{1}+v_{2})\\in U+V\\). If \\(u+v\\in U+V\\) and \\(\\lambda\\in\\mathbb{R}\\), then \\(\\lambda(u+v)=\\lambda u+\\lambda v\\in U+V\\). Using these three observations, we conclude that \\(U+V\\) is a subspace of \\(W\\).\nWe know that \\(U\\cap V\\) is a subspace of \\(W\\). Let \\(\\{w_{1},\\cdots,w_{k}\\}\\) be a basis for \\(U\\cap V\\). We can now extend this to a basis for \\(U\\) and a basis for \\(V\\). The two bases are \\(\\{w_{1},\\cdots,w_{k},u_{1},\\cdots,u_{m}\\}\\) for \\(U\\) and \\(\\{w_{1},\\cdots,w_{k},v_{1},\\cdots,v_{n}\\}\\) for \\(V\\). Consider the set \\(\\{w_{1},\\cdots,w_{k},u_{1},\\cdots,u_{m},v_{1},\\cdots,v_{n}\\}.\\) It is quite easy to see that this set spans \\(U+V\\). If we can show that this set is also linearly independent, then we have a basis for \\(U+V\\).\nLet us take a linear combination of this collection and set it to zero:\n\\[\n(a_{1}w_{1}+\\cdots+a_{k}w_{k})+(b_{1}u_{1}+\\cdots+b_{m}u_{m})+(c_{1}v_{1}+\\cdots+c_{n}v_{n})=0\n\\]\nWe can now group the terms in the following way:\n\\[\na_{1}w_{1}+\\cdots+a_{k}w_{k}+b_{1}u_{1}+\\cdots+b_{m}u_{m}=-(c_{1}v_{1}+\\cdots+c_{n}v_{n})\n\\]\nThe LHS is a vector in \\(U\\) and the RHS is a vector in \\(V\\). Since the two are equal, the vector in question is in \\(U\\cap V\\). We can now express this using the basis for \\(U\\cap V\\):\n\\[\n\\begin{aligned}\n-(c_{1}v_{1}+\\cdots+c_{n}v_{n}) & =d_{1}w_{1}+\\cdots+d_{k}w_{k}\\\\\nc_{1}v_{1}+\\cdots+c_{n}v_{n}+d_{1}w_{1}+\\cdots+d_{k}w_{k} & =0\n\\end{aligned}\n\\]\nThis is actually a linear combination of the basis vectors of \\(V\\) set to zero. Hence, \\(c_{1}=\\cdots=c_{n}=d_{1}=\\cdots d_{k}=0\\). Going back to the original equation, we conclude that \\(a_{1}=\\cdots=a_{k}=b_{1}=\\cdots=b_{m}=0\\). This implies, the set \\(\\{w_{1},\\cdots,w_{k},u_{1},\\cdots,u_{m},v_{1},\\cdots,v_{n}\\}\\) is linearly independent. We therefore have shown that this is a basis for \\(U+V\\).\nFrom this, the formula that relates the dimensions of the subspaces \\(U,V,U\\cap V\\) and \\(U+V\\) follows:\n\\[\n\\boxed{\\text{dim}(U+V)=\\text{dim}(U)+\\text{dim}(V)-\\text{dim}(U\\cap V)}\n\\]",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Problems",
      "Problem-1"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA.html",
    "href": "topics/machine_learning/notes/PCA.html",
    "title": "PCA",
    "section": "",
    "text": "Let \\(X\\) be a centered dataset of shape \\(d\\times n\\):\n\\[\nX=\\begin{bmatrix}\\vert &  & \\vert\\\\\nx_{1} & \\cdots & x_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix}\n\\]\nThe \\(i^{th}\\) column of \\(X\\) is the \\(i^{th}\\) data-point. Since \\(X\\) is centered, we have:\n\\[\n\\sum_{i=1}^{n}x_{i}=0\n\\]\nWe are going to look for a subspace of dimension \\(k\\) that is closest to the dataset. More precisely, we want to minimize the reconstruction error after projecting the data-points onto this subspace. We can always find an orthonormal basis for a subspace. Let us call this subspace \\(W\\) and an orthonormal basis for that as \\(\\{w_{1},\\cdots,w_{k}\\}\\):\nThe projection of the \\(i^{th}\\) data-point onto this subspace is:\n\\[\n\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})w_{j}\n\\]\nThe residue in projecting this data-point onto the subspace is:\n\\[\ne_{i}=x_{i}-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})w_{j}\\right]\n\\]\nThe error is \\(||e_{i}||^{2}\\). Let us now expand this error term by using the fact that \\(||e_{i}||^{2}=e_{i}^{T}e_{i}\\).\n\\[\n\\begin{aligned}\ne_{i}^{T}e_{i} & =x_{i}^{T}x_{i}-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\end{aligned}\n\\]\nSince we will be minimizing the error, we can drop \\(x_{i}^{T}x_{i}\\), which is a constant in the context of the minimization problem. We are therefore left with the following quantity for a single data-point:\n\\[\n-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\]\nFor the entire dataset, we sum this error and divide by \\(n\\):\n\\[\n-\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\]\nSetting \\(C=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_{i}x_{i}^{T}\\), interchanging the order of the summation and the order of multiplication, we get:\n\\[\n-\\sum\\limits_{j=1}^{k}w_{j}^{T}\\left[\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_{i}x_{i}^{T}\\right]w_{j}=-\\sum_{j=1}^{k}w_{j}^{T}Cw_{j}\n\\]\nMinimizing the above quantity is the same as maximizing its negation. Therefore, we have the following optimization problem:\n\\[\n\\max\\,\\,\\,\\sum\\limits_{i=1}^{k}w_{i}^{T}Cw_{i}\n\\]\nwhere \\(\\{w_{1},\\cdots,w_{k}\\}\\) is an orthonormal list of vectors and \\(C\\) is the covariance matrix of the dataset.",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA.html#setting-up-the-problem",
    "href": "topics/machine_learning/notes/PCA.html#setting-up-the-problem",
    "title": "PCA",
    "section": "",
    "text": "Let \\(X\\) be a centered dataset of shape \\(d\\times n\\):\n\\[\nX=\\begin{bmatrix}\\vert &  & \\vert\\\\\nx_{1} & \\cdots & x_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix}\n\\]\nThe \\(i^{th}\\) column of \\(X\\) is the \\(i^{th}\\) data-point. Since \\(X\\) is centered, we have:\n\\[\n\\sum_{i=1}^{n}x_{i}=0\n\\]\nWe are going to look for a subspace of dimension \\(k\\) that is closest to the dataset. More precisely, we want to minimize the reconstruction error after projecting the data-points onto this subspace. We can always find an orthonormal basis for a subspace. Let us call this subspace \\(W\\) and an orthonormal basis for that as \\(\\{w_{1},\\cdots,w_{k}\\}\\):\nThe projection of the \\(i^{th}\\) data-point onto this subspace is:\n\\[\n\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})w_{j}\n\\]\nThe residue in projecting this data-point onto the subspace is:\n\\[\ne_{i}=x_{i}-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})w_{j}\\right]\n\\]\nThe error is \\(||e_{i}||^{2}\\). Let us now expand this error term by using the fact that \\(||e_{i}||^{2}=e_{i}^{T}e_{i}\\).\n\\[\n\\begin{aligned}\ne_{i}^{T}e_{i} & =x_{i}^{T}x_{i}-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\end{aligned}\n\\]\nSince we will be minimizing the error, we can drop \\(x_{i}^{T}x_{i}\\), which is a constant in the context of the minimization problem. We are therefore left with the following quantity for a single data-point:\n\\[\n-\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\]\nFor the entire dataset, we sum this error and divide by \\(n\\):\n\\[\n-\\frac{1}{n}\\sum\\limits_{i=1}^{n}\\left[\\sum\\limits_{j=1}^{k}(x_{i}^{T}w_{j})^{2}\\right]\n\\]\nSetting \\(C=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_{i}x_{i}^{T}\\), interchanging the order of the summation and the order of multiplication, we get:\n\\[\n-\\sum\\limits_{j=1}^{k}w_{j}^{T}\\left[\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_{i}x_{i}^{T}\\right]w_{j}=-\\sum_{j=1}^{k}w_{j}^{T}Cw_{j}\n\\]\nMinimizing the above quantity is the same as maximizing its negation. Therefore, we have the following optimization problem:\n\\[\n\\max\\,\\,\\,\\sum\\limits_{i=1}^{k}w_{i}^{T}Cw_{i}\n\\]\nwhere \\(\\{w_{1},\\cdots,w_{k}\\}\\) is an orthonormal list of vectors and \\(C\\) is the covariance matrix of the dataset.",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA.html#solving-the-problem",
    "href": "topics/machine_learning/notes/PCA.html#solving-the-problem",
    "title": "PCA",
    "section": "Solving the problem",
    "text": "Solving the problem\nSince \\(C\\) is a real symmetric matrix, it is diagonalizable. There is an orthonormal basis of eigenvectors of \\(C\\) for \\(\\mathbb{R}^{d}\\). Let us denote it by \\(\\{u,\\cdots,u_{d}\\}\\). We also note that \\(C\\) is positive semi-definite, so it has \\(d\\) non-negative eigenvalues. Let us call them \\(\\lambda_{1}\\geq\\cdots\\geq\\lambda_{d}\\ge0\\). We can represent \\(C\\) as:\n\\[\nC=\\sum\\limits_{i=1}^{d}\\lambda_{i}u_{i}u_{i}^{T}\n\\]\nPlugging this back into the objective function, we have:\n\\[\n\\begin{aligned}\n\\sum\\limits_{i=1}^{k}w_{i}^{T}Cw_{i} & =\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{d}\\lambda_{j}(w_{i}^{T}u_{j})^{2}\n\\end{aligned}\n\\]\nChanging the order of the summation inside, we have:\n\\[\n\\sum_{j=1}^{d}\\lambda_{j}\\sum_{i=1}^{k}(u_{j}^{T}w_{i})^{2}\n\\]\nThe term \\(\\sum\\limits_{i=1}^{k}(u_{j}^{T}w_{i})^{2}\\) is the squared norm of the projection of \\(u_{j}\\) onto the subspace \\(W\\). Since the norm of a vector is always greater than or equal to the norm of its projection, we have:\n\\[\n\\sum_{i=1}^{k}(u_{j}^{T}w_{i})^{2}\\leq||u_{j}||^{2}=1\n\\]\nApplying this inequality across all values of \\(j\\), we end up with:\n\\[\n\\sum\\limits_{i=1}^{k}w_{i}^{T}Cw_{i}\\leq\\sum_{j=1}^{d}\\lambda_{j}\n\\]\nWe have thus arrived at an upper bound for the objective function. This upper bound can actually be achieved when \\(w_{i}=u_{i}\\) for \\(1\\leq i\\leq k\\).",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA.html#summary",
    "href": "topics/machine_learning/notes/PCA.html#summary",
    "title": "PCA",
    "section": "Summary",
    "text": "Summary\nIn summary, the subspace that is closest to the dataset is the one spanned by the top \\(k\\) eigenvectors of the covariance matrix of the centered dataset.",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#data",
    "href": "random/sonnet_test.html#data",
    "title": "Vectors and Matrices",
    "section": "Data",
    "text": "Data\n\nVectors\nMatrices",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vectors",
    "href": "random/sonnet_test.html#vectors",
    "title": "Vectors and Matrices",
    "section": "Vectors",
    "text": "Vectors\n\\[\n(85, 75), (89, 50), (95, 100), (56, 99), (68, 98)\n\\]\n85 and 75 are components of the vector \\((85, 75)\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#matrices",
    "href": "random/sonnet_test.html#matrices",
    "title": "Vectors and Matrices",
    "section": "Matrices",
    "text": "Matrices\n\\[\n\\begin{bmatrix}\n85 & 75 \\\\\n89 & 50 \\\\\n95 & 100 \\\\\n56 & 99 \\\\\n68 & 98\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#column-vector",
    "href": "random/sonnet_test.html#column-vector",
    "title": "Vectors and Matrices",
    "section": "Column Vector",
    "text": "Column Vector\n\\[\n\\begin{pmatrix}\n85 \\\\\n75\n\\end{pmatrix}\n\\quad\n\\begin{bmatrix}\n85 \\\\\n75 \\\\\n89 \\\\\n50 \\\\\n95 \\\\\n100 \\\\\n56 \\\\\n99 \\\\\n68 \\\\\n98\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#row-vector",
    "href": "random/sonnet_test.html#row-vector",
    "title": "Vectors and Matrices",
    "section": "Row Vector",
    "text": "Row Vector\n\\[\n(85, 75)\n\\quad\n\\begin{bmatrix}\n85 & 75 & 89 & 50 & 95 & 100 & 56 & 99 & 68 & 98\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-addition",
    "href": "random/sonnet_test.html#vector-addition",
    "title": "Vectors and Matrices",
    "section": "Vector Addition",
    "text": "Vector Addition\n\\[(1, 2, 3) + (4, 5, 6) = (5, 7, 9)\\]\n\\[(x_1, \\ldots, x_n) + (y_1, \\ldots, y_n) = (x_1 + y_1, \\ldots, x_n + y_n)\\]\nComponents are added",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#scalar-multiplication",
    "href": "random/sonnet_test.html#scalar-multiplication",
    "title": "Vectors and Matrices",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\n\\[3 \\cdot (1, 2, 3) = (3, 6, 9)\\]\n\\[c \\cdot (x_1, \\ldots, x_n) = (cx_1, \\ldots, cx_n)\\]\nComponents are scaled",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#linear-combination",
    "href": "random/sonnet_test.html#linear-combination",
    "title": "Vectors and Matrices",
    "section": "Linear Combination",
    "text": "Linear Combination\n\\[2 \\cdot (1, 2) + 3 \\cdot (-1, 1) = (-1, 7)\\]\n\\[c_1x_1 + \\ldots + c_mx_m\\]\nwhere \\(x_i = (x_{i1}, \\ldots, x_{in})\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#mathbbrn",
    "href": "random/sonnet_test.html#mathbbrn",
    "title": "Vectors and Matrices",
    "section": "\\(\\mathbb{R}^n\\)",
    "text": "\\(\\mathbb{R}^n\\)\n\n\\(\\mathbb{R}\\) : line\n\\(\\mathbb{R}^2 = \\{(x, y) | x, y \\in \\mathbb{R}\\}\\) : plane\n\\(\\mathbb{R}^3 = \\{(x, y, z) | x, y, z \\in \\mathbb{R}\\}\\) : space\n\\(\\mathbb{R}^n = \\{(x_1, \\ldots, x_n) | x_1, \\ldots, x_n \\in \\mathbb{R}\\}\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#m_m-times-nmathbbr",
    "href": "random/sonnet_test.html#m_m-times-nmathbbr",
    "title": "Vectors and Matrices",
    "section": "\\(M_{m \\times n}(\\mathbb{R})\\)",
    "text": "\\(M_{m \\times n}(\\mathbb{R})\\)\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12\n\\end{bmatrix}\n= \\begin{bmatrix}\nA_{11} & A_{12} & A_{13} & A_{14} \\\\\nA_{21} & A_{22} & A_{23} & A_{24} \\\\\nA_{31} & A_{32} & A_{33} & A_{34}\n\\end{bmatrix}\n\\]\n\\(3 \\times 4\\) matrix",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#m_m-times-nmathbbr-1",
    "href": "random/sonnet_test.html#m_m-times-nmathbbr-1",
    "title": "Vectors and Matrices",
    "section": "\\(M_{m \\times n}(\\mathbb{R})\\)",
    "text": "\\(M_{m \\times n}(\\mathbb{R})\\)\n\n\\(M_{3 \\times 4}(\\mathbb{R})\\): set of all real \\(3 \\times 4\\) matrices\n\\(M_{m \\times n}(\\mathbb{R})\\): set of all real \\(m \\times n\\) matrices",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#matrix-vector-multiplication",
    "href": "random/sonnet_test.html#matrix-vector-multiplication",
    "title": "Vectors and Matrices",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\n\\[\n\\begin{bmatrix}\n1 & 3 & 5 \\\\\n2 & 4 & 6\n\\end{bmatrix}\n\\begin{pmatrix}\n3 \\\\\n-1 \\\\\n2\n\\end{pmatrix}\n= 3 \\cdot\n\\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}\n+ (-1) \\cdot\n\\begin{pmatrix}\n3 \\\\\n4\n\\end{pmatrix}\n+ 2 \\cdot\n\\begin{pmatrix}\n5 \\\\\n6\n\\end{pmatrix}\n\\]\nLinear combination of the columns",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#matrix-vector-multiplication-1",
    "href": "random/sonnet_test.html#matrix-vector-multiplication-1",
    "title": "Vectors and Matrices",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\n\\[\n\\begin{vmatrix}\nc_1 & \\ldots & c_n\n\\end{vmatrix}\n\\begin{pmatrix}\nx_1 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n= x_1c_1 + \\ldots + x_nc_n\n\\]\n\\(m \\times n\\) · \\(n \\times 1\\) = \\(m \\times 1\\)\n\\(M_{m \\times n}(\\mathbb{R}) \\cdot \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-matrix-multiplication",
    "href": "random/sonnet_test.html#vector-matrix-multiplication",
    "title": "Vectors and Matrices",
    "section": "Vector-Matrix Multiplication",
    "text": "Vector-Matrix Multiplication\n\\[\n(3, -1)\n\\begin{bmatrix}\n1 & 3 & 5 \\\\\n2 & 4 & 6\n\\end{bmatrix}\n= 3 \\cdot (1, 3, 5) + (-1) \\cdot (2, 4, 6)\n\\]\nLinear combination of the rows",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-matrix-multiplication-1",
    "href": "random/sonnet_test.html#vector-matrix-multiplication-1",
    "title": "Vectors and Matrices",
    "section": "Vector-Matrix Multiplication",
    "text": "Vector-Matrix Multiplication\n\\[\n\\begin{pmatrix}\nx_1 & \\ldots & x_m\n\\end{pmatrix}\n\\begin{vmatrix}\nr_1^T \\\\\n\\vdots \\\\\nr_m^T\n\\end{vmatrix}\n= x_1r_1^T + \\ldots + x_mr_m^T\n\\]\n\\(1 \\times m\\) · \\(m \\times n\\) = \\(1 \\times n\\)\n\\(\\mathbb{R}^m \\cdot M_{m \\times n}(\\mathbb{R}) \\rightarrow \\mathbb{R}^n\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-vector-multiplication-inner-product",
    "href": "random/sonnet_test.html#vector-vector-multiplication-inner-product",
    "title": "Vectors and Matrices",
    "section": "Vector-Vector Multiplication (Inner Product)",
    "text": "Vector-Vector Multiplication (Inner Product)\n\\[\n(1, 0, 2, -1)\n\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n1 \\\\\n3\n\\end{pmatrix}\n= 2 - 0 + 2 - 3\n\\]\nDot product",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-vector-multiplication-inner-product-1",
    "href": "random/sonnet_test.html#vector-vector-multiplication-inner-product-1",
    "title": "Vectors and Matrices",
    "section": "Vector-Vector Multiplication (Inner Product)",
    "text": "Vector-Vector Multiplication (Inner Product)\n\\[\n\\begin{pmatrix}\nx_1 & \\ldots & x_n\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}\n= x_1y_1 + \\ldots + x_ny_n\n\\]\n\\(1 \\times n\\) · \\(n \\times 1\\) = \\(1 \\times 1\\)\n\\(\\mathbb{R}^n \\cdot \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-vector-multiplication-outer-product",
    "href": "random/sonnet_test.html#vector-vector-multiplication-outer-product",
    "title": "Vectors and Matrices",
    "section": "Vector-Vector Multiplication (Outer Product)",
    "text": "Vector-Vector Multiplication (Outer Product)\n\\[\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{pmatrix}\n(5, 6, 7) =\n\\begin{bmatrix}\n5 & 6 & 7 \\\\\n10 & 12 & 14 \\\\\n15 & 18 & 21\n\\end{bmatrix}\n\\]\nOuter Product",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#vector-vector-multiplication-outer-product-1",
    "href": "random/sonnet_test.html#vector-vector-multiplication-outer-product-1",
    "title": "Vectors and Matrices",
    "section": "Vector-Vector Multiplication (Outer Product)",
    "text": "Vector-Vector Multiplication (Outer Product)\n\\[\n\\begin{pmatrix}\nx_1 \\\\\n\\vdots \\\\\nx_m\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 & \\ldots & y_n\n\\end{pmatrix}\n=\n\\begin{bmatrix}\nx_1y_1 & \\ldots & x_1y_n \\\\\n\\vdots & & \\vdots \\\\\nx_my_1 & \\ldots & x_my_n\n\\end{bmatrix}\n\\]\n\\(m \\times 1\\) · \\(1 \\times n\\) = \\(m \\times n\\)\n\\(\\mathbb{R}^m \\cdot \\mathbb{R}^n \\rightarrow M_{m \\times n}(\\mathbb{R})\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#matrix-matrix-multiplication",
    "href": "random/sonnet_test.html#matrix-matrix-multiplication",
    "title": "Vectors and Matrices",
    "section": "Matrix-Matrix Multiplication",
    "text": "Matrix-Matrix Multiplication\n\\(AB = C\\)\n\\(m \\times n\\) · \\(n \\times p\\) = \\(m \\times p\\)\n\nOnly matrices of compatible dimensions can be multiplied\n\ncolumns of \\(A\\) = # rows of \\(B\\)\n\nMatrix multiplication is not commutative\n\nIn general \\(AB \\neq BA\\)\nIf \\(AB = BA\\), we say that \\(A\\) and \\(B\\) commute",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#matrix-matrix-multiplication-1",
    "href": "random/sonnet_test.html#matrix-matrix-multiplication-1",
    "title": "Vectors and Matrices",
    "section": "Matrix-Matrix Multiplication",
    "text": "Matrix-Matrix Multiplication\n\\(AB = C\\)\n\nMatrix-Vector: \\(A \\begin{vmatrix} b_1 & \\ldots & b_p \\end{vmatrix} = \\begin{vmatrix} Ab_1 & \\ldots & Ab_p \\end{vmatrix}\\)\nVector-Matrix: \\(B = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_m^T \\end{vmatrix} \\rightarrow \\begin{vmatrix} a_1^TB \\\\ \\vdots \\\\ a_m^TB \\end{vmatrix}\\)\nVector-Vector (Inner Product): \\(\\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_m^T \\end{vmatrix} \\begin{vmatrix} b_1 & \\ldots & b_p \\end{vmatrix} = \\begin{bmatrix} \\ldots & a_i^Tb_j & \\ldots \\\\ & \\vdots & \\end{bmatrix}\\)\nVector-Vector (Outer Product): \\(\\begin{vmatrix} a_1 & \\ldots & a_n \\end{vmatrix} \\begin{vmatrix} b_1^T \\\\ \\vdots \\\\ b_n^T \\end{vmatrix} = a_1b_1^T + \\ldots + a_nb_n^T\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#special-matrices",
    "href": "random/sonnet_test.html#special-matrices",
    "title": "Vectors and Matrices",
    "section": "Special Matrices",
    "text": "Special Matrices\n\n\nDiagonal: \\[\nD = \\begin{bmatrix}\nd_1 & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & d_n\n\\end{bmatrix}\n\\]\nScalar: \\[\nS = \\begin{bmatrix}\nc & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & c\n\\end{bmatrix} = cI\n\\]\n\nIdentity: \\[\nI = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nSquare matrix: \\[\nA_{n \\times n} \\rightarrow \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#special-matrices-cont.",
    "href": "random/sonnet_test.html#special-matrices-cont.",
    "title": "Vectors and Matrices",
    "section": "Special Matrices (cont.)",
    "text": "Special Matrices (cont.)\n\n\nUpper Triangular: \\[\nU = \\begin{bmatrix}\na_{11} & \\ldots & a_{1n} \\\\\n& \\ddots & \\vdots \\\\\n0 & & a_{nn}\n\\end{bmatrix}\n\\]\n\nLower Triangular: \\[\nL = \\begin{bmatrix}\na_{11} & 0 \\\\\n\\vdots & \\ddots \\\\\na_{n1} & \\ldots & a_{nn}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#transpose",
    "href": "random/sonnet_test.html#transpose",
    "title": "Vectors and Matrices",
    "section": "Transpose",
    "text": "Transpose\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}^T\n=\n\\begin{bmatrix}\n1 & 3 & 5 \\\\\n2 & 4 & 6\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 3 & 5 \\\\\n2 & 4 & 6\n\\end{bmatrix}^T\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#transpose-properties",
    "href": "random/sonnet_test.html#transpose-properties",
    "title": "Vectors and Matrices",
    "section": "Transpose Properties",
    "text": "Transpose Properties\n\n\\(A_{m \\times n} \\rightarrow A^T_{n \\times m}\\)\n\\((A^T)^T = A\\)\n\\(A_{ij} = A^T_{ji}\\)\n\\((AB)^T = B^TA^T\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#symmetric-and-skew-symmetric",
    "href": "random/sonnet_test.html#symmetric-and-skew-symmetric",
    "title": "Vectors and Matrices",
    "section": "Symmetric and Skew-symmetric",
    "text": "Symmetric and Skew-symmetric\n\n\nSymmetric: \\[\nA = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 0 & 4 \\\\\n3 & 4 & 1\n\\end{bmatrix}\n\\] \\[\nA^T = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 0 & 4 \\\\\n3 & 4 & 1\n\\end{bmatrix}\n\\] \\(A = A^T\\)\n\nSkew-Symmetric: \\[\nA = \\begin{bmatrix}\n0 & 2 & 3 \\\\\n-2 & 0 & 4 \\\\\n-3 & -4 & 0\n\\end{bmatrix}\n\\] \\[\nA^T = \\begin{bmatrix}\n0 & -2 & -3 \\\\\n2 & 0 & -4 \\\\\n3 & 4 & 0\n\\end{bmatrix}\n\\] \\(A = -A^T\\)",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#symmetric-and-skew-symmetric-cont.",
    "href": "random/sonnet_test.html#symmetric-and-skew-symmetric-cont.",
    "title": "Vectors and Matrices",
    "section": "Symmetric and Skew-symmetric (cont.)",
    "text": "Symmetric and Skew-symmetric (cont.)\nFor any square matrix \\(A\\):\n\\[\nA = \\underbrace{\\frac{A + A^T}{2}}_{\\text{symmetric}} + \\underbrace{\\frac{A - A^T}{2}}_{\\text{skew-symmetric}}\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/sonnet_test.html#inverse",
    "href": "random/sonnet_test.html#inverse",
    "title": "Vectors and Matrices",
    "section": "Inverse",
    "text": "Inverse\n\\(A_{n \\times n} \\rightarrow B_{n \\times n}\\)\n\\(AB = BA = I \\implies B = A^{-1}\\) and \\(A = B^{-1}\\)\nFor a 2x2 matrix: \\[\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\quad \\text{if } ad-bc \\neq 0\n\\]",
    "crumbs": [
      "Random",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "random/index.html",
    "href": "random/index.html",
    "title": "Random",
    "section": "",
    "text": "This section has a collection of notes on miscellaneous topics. Some are technical in nature, some are not so technical. But most of them are concerned with mathematics, computing and pedagogy.",
    "crumbs": [
      "Random"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Linear Transformations.html",
    "href": "courses/MLF/notes/Part-2/Linear Transformations.html",
    "title": "Linear Transformations",
    "section": "",
    "text": "Question\n\n\n\nWhat does a linear transformation do to vectors?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Linear Transformations"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Linear Transformations.html#algebraic-view",
    "href": "courses/MLF/notes/Part-2/Linear Transformations.html#algebraic-view",
    "title": "Linear Transformations",
    "section": "Algebraic view",
    "text": "Algebraic view\nRecall that a linear transformation is a linear mapping between two vector spaces \\(V\\) and \\(W\\):\n\\[\n\\begin{equation*}\nT:V\\rightarrow W\n\\end{equation*}\n\\]\nWhat makes \\(T\\) linear are these two properties:\n\n\\(T(\\mathbf{v}_1 + \\mathbf{v}_2) = T(\\mathbf{v}_1) + T(\\mathbf{v}_2)\\), for all \\(\\mathbf{v}_1, \\mathbf{v}_2 \\in V\\)\n\\(T(c \\cdot \\mathbf{v}) = c \\cdot T(\\mathbf{v})\\), for all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in V\\)\n\nIn this course, we will be dealing with \\(\\mathbb{R}^{n}\\). So, our transformations will be of the form: \\[\n\\begin{equation*}\nT:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\n\\end{equation*}\n\\]\nTo get a better idea about what a linear transformation does, let us restrict our attention to a map from \\(\\mathbb{R}^{2}\\) to itself. If \\(\\mathbf{u}\\) is a vector in \\(\\mathbb{R}^{2}\\), then the transformation returns another vector in the same space. We can associate a matrix with every linear transformation. Assuming that we use the standard ordered basis \\(\\beta =\\{\\mathbf{e}_{1} ,\\mathbf{e}_{2} \\}\\) for \\(\\mathbb{R}^{2}\\) and calling the matrix \\(\\displaystyle \\mathbf{T}\\), we have:\n\\[\n\\begin{equation*}\n\\mathbf{T} :=[T]_{\\beta }^{\\beta } =\\begin{bmatrix}\n| & |\\\\\nT(\\mathbf{e}_{1} ) & T(\\mathbf{e}_{2} )\\\\\n| & |\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe basis vectors \\(\\mathbf{e}_{1}\\) and \\(\\mathbf{e}_{2}\\) are mapped to \\(T(\\mathbf{e}_{1} )\\) and \\(T(\\mathbf{e}_{2} )\\) respectively. The action of the linear transformation on the basis vectors gives us complete information on what happens to any vector in \\(\\mathbb{R}^{2}\\). To see why this is true, consider any vector \\(\\displaystyle \\mathbf{u} =\\alpha _{1}\\mathbf{e}_{1} +\\alpha _{2}\\mathbf{e}_{2}\\), then:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nT(\\mathbf{u}) & =T( \\alpha _{1}\\mathbf{e}_{1} +\\alpha _{2}\\mathbf{e}_{2})\\\\\n& \\\\\n& =\\alpha _{1} T(\\mathbf{e}_{1}) +\\alpha _{2} T(\\mathbf{e}_{2})\\\\\n& \\\\\n& =\\mathbf{T}\\begin{bmatrix}\n\\alpha _{1}\\\\\n\\alpha _{2}\n\\end{bmatrix}\n\\end{aligned}\n\\end{equation*}\n\\]\nAny vector \\(\\displaystyle \\mathbf{u}\\) in \\(\\displaystyle \\mathbb{R}^{2}\\) is mapped to some other vector in the same space as a linear combination of the columns of the matrix corresponding to the linear transformation. Since every linear transformation corresponds to a matrix and since every matrix can be mapped to a linear transformation, we will use the two terms interchangeably from now.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Linear Transformations"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Linear Transformations.html#geometric-view",
    "href": "courses/MLF/notes/Part-2/Linear Transformations.html#geometric-view",
    "title": "Linear Transformations",
    "section": "Geometric view",
    "text": "Geometric view\nGeometrically, what does all this mean? Let us begin with a simple example:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThis is the identity transformation. It doesn’t disturb the vectors and leaves them as they are. That is, for any vector \\(\\displaystyle \\mathbf{u}\\) in \\(\\displaystyle \\mathbb{R}^{2}\\), we have:\n\\[\n\\begin{equation*}\n\\mathbf{Tu} =\\mathbf{u}\n\\end{equation*}\n\\]\nVisually:\n\nThis is not all that interesting. Next:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n2 & 0\\\\\n0 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nLet us see what this does to the basis vectors:\n\nNotice the effect it has. Each vector is scaled. In this case, it is stretched. It becomes twice as long as the input. To see why this is true algebraically, consider an arbitrary vector \\(x=\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}^{T}\\):\n\\[\n\\begin{equation*}\n\\mathbf{Tx} =\\begin{bmatrix}\n2 & 0\\\\\n0 & 2\n\\end{bmatrix}\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\n\\end{bmatrix} =2\\cdot \\begin{bmatrix}\nx_{1}\\\\\nx_{2}\n\\end{bmatrix} =2\\mathbf{x}\n\\end{equation*}\n\\]\nLet us now consider another matrix:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n0 & -1\\\\\n1 & 0\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe effect it has on the basis vectors is:\n\nThis is a rotation matrix. That is, it rotates the input vector without changing its magnitude. Moving on, let us take up another matrix. This time, let us compose the two linear transformations that we have seen. Composition of linear transformations is equivalent to matrix multiplication:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n0 & -1\\\\\n1 & 0\n\\end{bmatrix}\\begin{bmatrix}\n2 & 0\\\\\n0 & 2\n\\end{bmatrix} =\\begin{bmatrix}\n0 & -2\\\\\n2 & 0\n\\end{bmatrix}\n\\end{equation*}\n\\]\nWhat do you expect this matrix to do?\n\nIt stretches the vectors and rotates them by \\(90^{\\circ }\\). Note that the two matrices involved in the product are commutative. That is, \\(\\mathbf{T}_{1}\\mathbf{T}_{2} =\\mathbf{T}_{2}\\mathbf{T}_{1}\\). Intuitively, we can see why this is true. We could either stretch a vector and then rotate it (OR) we could rotate it and then stretch it. However, this (commutativity) is not true of any two arbitrary linear transformations. Let us now move to a more complex linear transformation:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n2 & 1\\\\\n1 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe effect on basis vectors is:\n\nThis is not simple rotation or stretching. It is an example of a shear transformation. Now that we have a good idea of what linear transformations do, we are ready to explore the idea of eigenvalues and eigenvectors.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Linear Transformations"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Linear Transformations.html#summary",
    "href": "courses/MLF/notes/Part-2/Linear Transformations.html#summary",
    "title": "Linear Transformations",
    "section": "Summary",
    "text": "Summary\nA linear transformation \\(T\\) is a linear mapping between vector spaces. The action of a linear transformation on the basis vectors of the domain provides a complete description of what it does to all the vectors in the domain. Every linear transformation corresponds to a matrix. The action of a linear transformation on a vector is equivalent to pre-multiplying the vector with the matrix.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Linear Transformations"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html",
    "title": "Characteristic Polynomial",
    "section": "",
    "text": "Question\n\n\n\nHow do we find the eigenvalues of a matrix?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#necessary-condition",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#necessary-condition",
    "title": "Characteristic Polynomial",
    "section": "Necessary condition",
    "text": "Necessary condition\nIf \\(\\mathbf{x}\\) is an eigenvector of a matrix \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\), then we have:\n\\[\n\\begin{equation*}\n\\mathbf{Ax} =\\lambda \\mathbf{x}\n\\end{equation*}\n\\]\nLet us rearrange this a bit. Observe that if \\(\\mathbf{I}\\) is the identity matrix, then:\n\\[\n\\begin{equation*}\n\\mathbf{Ix} =\\mathbf{x}\n\\end{equation*}\n\\]\nUsing this fact, we can express the eigenvalue equation as:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\mathbf{Ax} & =\\lambda \\mathbf{Ix}\\\\\n& \\\\\n\\mathbf{Ax} -\\lambda \\mathbf{Ix} & =\\mathbf{0}\\\\\n& \\\\\n(\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} & =0\n\\end{aligned}\n\\end{equation*}\n\\]\nIf \\((\\lambda ,\\mathbf{x} )\\) is an eigenpair, then \\((\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} =0\\). Another way of expressing this is:\n\n\n\n\n\n\nNecessary condition\n\n\n\n\\((\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} =0\\) is a necessary condition for \\(\\mathbf{x}\\) to be an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#sufficient-condition",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#sufficient-condition",
    "title": "Characteristic Polynomial",
    "section": "Sufficient condition",
    "text": "Sufficient condition\nLet us try the other direction. What if \\((\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} =\\mathbf{0}\\) for \\(\\mathbf{x} \\neq \\mathbf{0}\\)? We have:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n(\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} & =\\mathbf{0}\\\\\n& \\\\\n\\mathbf{Ax} -\\lambda \\mathbf{Ix} & =\\mathbf{0}\\\\\n& \\\\\n\\mathbf{Ax} & =\\lambda \\mathbf{x}\n\\end{aligned}\n\\end{equation*}\n\\]\nFrom the above result and with \\(\\mathbf{x} \\neq \\mathbf{0}\\), we see that \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\).\n\n\n\n\n\n\nSufficient condition\n\n\n\n\\((\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} =\\mathbf{0}\\) and \\(\\mathbf{x} \\neq \\mathbf{0}\\) is a sufficient condition for \\(\\mathbf{x}\\) to be an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#nullspace-argument",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#nullspace-argument",
    "title": "Characteristic Polynomial",
    "section": "Nullspace argument",
    "text": "Nullspace argument\nThus we see that for a non-zero vector \\(\\mathbf{x}\\), \\((\\lambda ,\\mathbf{x} )\\) is an eigenpair of \\(\\mathbf{A}\\) if and only if \\((\\mathbf{A} -\\lambda \\mathbf{I} )\\mathbf{x} =\\mathbf{0}\\). From this, we deduce that \\(\\mathbf{x}\\) has to be in the nullspace of \\(\\mathbf{A} -\\lambda \\mathbf{I}\\). Thus the nullspace of \\(\\mathbf{A} -\\lambda \\mathbf{I}\\) is non-trivial. If the nullspace is non-trivial, we can conclude that the matrix is not invertible. To see why this is true, think about what happens if the matrix is invertible. If the matrix is invertible, then \\(\\mathbf{0}\\) would be the only element in the nullspace.\nIf the matrix is not invertible, then its determinant is zero. So, we have the following result:\n\n\n\n\n\n\nNote\n\n\n\n\\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\(|\\mathbf{A} -\\lambda \\mathbf{I} |=0\\)\n\n\nNotice how we end up with a result that doesn’t involve the eigenvector and only the matrix and its eigenvalues. We can now characterize the collection of all eigenvectors along with the zero vector, the eigenspace, as follows:\n\n\n\n\n\n\nNote\n\n\n\nThe eigenspace corresponding to an eigenvalue \\(\\lambda\\) is the nullspace of \\(\\mathbf{A} -\\lambda \\mathbf{I}\\). If we denote the eigenspace corresponding to the eigenvalue \\(\\lambda\\) as \\(E(\\lambda)\\), we have: \\[\nE(\\lambda) = \\mathcal{N}(\\mathbf{A} - \\lambda \\mathbf{I})\n\\]",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#the-polynomial",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#the-polynomial",
    "title": "Characteristic Polynomial",
    "section": "The Polynomial",
    "text": "The Polynomial\nAt this stage, let us take up an example:\n\\[\n\\begin{equation*}\n\\mathbf{A} =\\begin{bmatrix}\n3 & 1\\\\\n0 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nFor \\(\\lambda\\) to be an eigenvalue of the matrix \\(\\mathbf{A}\\), \\(|\\mathbf{A} -\\lambda \\mathbf{I} |=0\\). This translates to:\n\\[\n\\begin{equation*}\n\\begin{vmatrix}\n3-\\lambda  & 1\\\\\n0 & 2-\\lambda\n\\end{vmatrix} =0\n\\end{equation*}\n\\]\nExpanding the determinant, we get:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n(3-\\lambda )(2-\\lambda )=0\n\\end{aligned}\n\\end{equation*}\n\\]\nThe expression on the LHS is a polynomial of degree \\(2\\) and is called the characteristic polynomial of \\(\\mathbf{A}\\). We see that \\(\\lambda =2,3\\) are the eigenvalues of the matrix \\(\\mathbf{A}\\).\n\n\n\n\n\n\nDefinition\n\n\n\nFor a \\(n \\times n\\) matrix \\(\\mathbf{A}\\), the determinant \\(|\\mathbf{A} - \\lambda \\mathbf{I}|\\) is a polynomial of degree \\(n\\) and is termed the characteristic polynomial. The roots or zeros of this polynomial are the eigenvalues of \\(\\mathbf{A}\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#properties",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#properties",
    "title": "Characteristic Polynomial",
    "section": "Properties",
    "text": "Properties\nThere are two properties of the polynomial that are worth knowing:\n\nThe sum of the eigenvalues of the matrix \\(\\mathbf{A}\\) is equal to its trace.\nThe product of the eigenvalues of the matrix \\(\\mathbf{A}\\) is equal to its determinant.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#summary",
    "href": "courses/MLF/notes/Part-2/Characteristic Polynomial.html#summary",
    "title": "Characteristic Polynomial",
    "section": "Summary",
    "text": "Summary\nTo find the eigenvalues of a matrix \\(\\mathbf{A}\\), we first compute the characteristic polynomial \\(|\\mathbf{A} -\\lambda \\mathbf{I} |\\). The roots of this polynomial are the eigenvalues of the matrix \\(\\mathbf{A}\\). There are two important properties:\n\nsum of the eigenvalues is equal to the trace of the matrix\nproduct of the eigenvalues is equal to the determinant of the matrix",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Characteristic Polynomial"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Question\n\n\n\nWhat is a linear regression model?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html#motivation",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html#motivation",
    "title": "Linear Regression",
    "section": "Motivation",
    "text": "Motivation\nIn this part, we will try to solve the regression problem that we introduced in the first part. Let us formally define the problem:\n\n\n\n\n\n\nProblem\n\n\n\nPredict the selling price of a house given the data of \\(\\displaystyle n\\) houses, where each house is represented by \\(\\displaystyle d\\) features.\n\n\nConsider two houses, one which has an area of \\(1000\\) square feet and the other which has an area of \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows:\n\\[\n\\begin{equation*}\n\\text{Selling-price} =2\\times \\text{Area} -0.2\\times \\text{Distance} +\\text{Constant}\n\\end{equation*}\n\\]\nThis is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the coefficients or weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature is negatively correlated with the selling-price, but it is not as important as the area.\n\n\n\n\n\n\nRemark\n\n\n\nWe might be wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: &gt; All models are wrong, but some are useful.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html#linear-model",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html#linear-model",
    "title": "Linear Regression",
    "section": "Linear model",
    "text": "Linear model\nGeneralizing this, let us say that we have a feature vector \\(\\mathbf{x}\\) and a weight vector \\(\\mathbf{w}\\). Recall that the housing data has six features:\n\\[\n\\begin{equation*}\n\\mathbf{x} =\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix} ,\\ \\mathbf{w} =\\begin{bmatrix}\nw_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\\\\\nw_{5}\\\\\nw_{6}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe function or model that maps a data-point to the label \\(y\\) (selling-price) is:\n\\[\n\\begin{equation*}\ny=x_{1} w_{1} +x_{2} w_{2} +x_{3} w_{3} +x_{4} w_{4} +x_{5} w_{5} +x_{6} w_{6} +\\text{constant}\n\\end{equation*}\n\\]\nWe can rewrite the constant as one more weight, say \\(w_{0}\\):\n\\[\n\\begin{equation*}\ny=x_{1} w_{1} +x_{2} w_{2} +x_{3} w_{3} +x_{4} w_{4} +x_{5} w_{5} +x_{6} w_{6} +w_{0}\n\\end{equation*}\n\\]\nGoing back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(w_{0}\\) to the weights:\n\\[\n\\begin{equation*}\n\\mathbf{x} =\\begin{bmatrix}\n1\\\\\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix} ,\\mathbf{w} =\\begin{bmatrix}\nw_{0}\\\\\nw_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\\\\\nw_{5}\\\\\nw_{6}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nIf we now look at the expression for \\(y\\), it is nothing but the dot-product of the two vectors:\n\\[\n\\begin{equation*}\n\\begin{aligned}\ny & =1\\cdot w_{0} +x_{1} w_{1} +x_{2} w_{2} +x_{3} w_{3} +x_{4} w_{4} +x_{5} w_{5} +x_{6} w_{6}\\\\\n& \\\\\n& =\\mathbf{w}^{T}\\mathbf{x}\n\\end{aligned}\n\\end{equation*}\n\\]\n\\(\\mathbf{w}^{T}\\mathbf{x}\\) is the dot product between the two vectors \\(\\displaystyle \\mathbf{w}\\) and \\(\\mathbf{x}\\). This is the notation that we will be using for the dot product from now on. This is the same as the matrix-product of a row-vector and a column-vector:\n\\[\n\\begin{equation*}\ny=\\mathbf{w}^{T}\\mathbf{x} =\\begin{bmatrix}\nw_{0} & w_{1} & w_{2} & w_{3} & w_{4} & w_{5} & w_{6}\n\\end{bmatrix}\\begin{bmatrix}\n1\\\\\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe linear model is therefore expressed as:\n\\[\n\\begin{equation*}\nf(\\mathbf{x}) =\\mathbf{w}^{T}\\mathbf{x}\n\\end{equation*}\n\\]\nIf \\(\\mathbf{x}\\) is the feature vector of a house, then \\(f(\\mathbf{x} )\\) will be its predicted selling price. We call \\(\\displaystyle \\mathbf{w}\\) the weight vector of the model. The elements of \\(\\displaystyle \\mathbf{w}\\) are called the parameters of the model.\n\n\n\n\n\n\nRemark\n\n\n\nAll vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(\\mathbf{x}^{T}\\), where \\(\\mathbf{x}\\) is some column-vector.}",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html#learning-problem",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html#learning-problem",
    "title": "Linear Regression",
    "section": "Learning problem",
    "text": "Learning problem\nSo much for one house. But we have several houses. If we assume that the relationship between the labels and features is perfectly linear, we can write down \\(\\displaystyle n\\) linear equations, one for each house, in terms of \\(\\displaystyle 7\\) unknowns. For the sake of brevity, we will just list down the equation for the \\(\\displaystyle i^{th}\\) house. Since the unknown values are the \\(\\displaystyle w\\)s, we will make that the LHS and the \\(\\displaystyle y\\)s the RHS:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nw_{0} +x_{i1} \\cdot w_{1} +x_{i2} \\cdot w_{2} +x_{i3} \\cdot w_{3} +x_{i4} \\cdot w_{4} +x_{i5} \\cdot w_{5} +x_{i6} \\cdot w_{6} & =y_{i}\n\\end{aligned}\n\\end{equation*}\n\\]\nThese \\(\\displaystyle n\\) equations can be given a neat matrix representation:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} & x_{1,5} & x_{1,6}\\\\\n\\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & x_{n,3} & x_{n,4} & x_{n,5} & x_{n,6}\n\\end{bmatrix}\\begin{bmatrix}\nw_{0}\\\\\nw_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\\\\\nw_{5}\\\\\nw_{6}\n\\end{bmatrix} =\\begin{bmatrix}\ny_{1}\\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nIf we call the data-matrix \\(\\mathbf{X}\\), the label vector \\(\\mathbf{y}\\) and the weight vector \\(\\mathbf{w}\\), this is the equation we have:\n\\[\n\\begin{equation*}\n\\mathbf{Xw} =\\mathbf{y}\n\\end{equation*}\n\\]\nWe are given both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). This is nothing but our labeled dataset. We have to learn our weight vector \\(\\mathbf{w}\\). This leaves us with two questions:\n\nDoes the equation \\(\\mathbf{Xw} =\\mathbf{y}\\) have a solution?\nIf it doesn’t have a solution, how do we estimate \\(\\mathbf{w}\\)?\n\nThe answer to the first question is that, the equation \\(\\mathbf{Xw} =\\mathbf{y}\\) may not have a solution in practice. This could be because of the following reasons:\n\nCase-1: The relationship between \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) is not linear.\nCase-2: Even if the relationship is linear, the labels may be corrupted by some noise \\(\\epsilon\\).\n\nRecall the quote with which we began this unit: “all models are wrong, but some are useful.” In the first case, we can’t do much. We would have to abandon the simple linear model and go for more complex models. In the second case, we still have hope. This situation is generally expressed as follows:\n\\[\n\\begin{equation*}\n\\mathbf{y} =\\mathbf{Xw} +\\mathbf{\\epsilon }\n\\end{equation*}\n\\]\nHere, \\(\\mathbf{\\epsilon }\\) is some small error term. The error-term for each house is assumed to be a small value that is equal to zero on average. The general approach in such situations is to find a set of weights that minimizes the error in prediction. Here is what we mean by this:\nConsider the \\(\\displaystyle i^{th}\\) house. The actual selling price for it is \\(\\displaystyle y_{i}\\). The predicted selling price is \\(\\displaystyle f(\\mathbf{x}_{i})\\). Therefore, the error in prediction for this house is \\(\\displaystyle f(\\mathbf{x}_{i}) -y_{i}\\). Since we are only interested in the absolute value of the error, it is a good idea to square this quantity: \\(\\displaystyle [ f(\\mathbf{x}_{i}) -y_{i}]^{2}\\). Summing this term across all \\(\\displaystyle n\\) houses gives us a measure of how good the function \\(\\displaystyle f\\) is in predicting the selling prices:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nL(\\mathbf{w} ) & =\\sum\\limits _{i=1}^{n}[ f( x_{i}) -y_{i}]^{2}\\\\\n& \\\\\n& =\\sum\\limits _{i=1}^{n}\\left[\\mathbf{w}^{T}\\mathbf{x}_{i} -y_{i}\\right]^{2}\n\\end{aligned}\n\\end{equation*}\n\\]\nThis is called the sum of squared errors or SSE. The quantity \\(\\displaystyle L(\\mathbf{w})\\) is called the error function or the loss function. Lower the value of the loss function, better our model. The learning problem in linear regression can be formulated as finding a vector of weights — \\(\\mathbf{w}\\) — that will minimize the loss \\(L(\\mathbf{w} )\\):\n\\[\n\\begin{equation*}\n\\underset{\\mathbf{w}}{\\min} \\ \\ L(\\mathbf{w})\n\\end{equation*}\n\\]\nLet us now recast the loss function using the data-matrix \\(\\displaystyle \\mathbf{X}\\) and label vector \\(\\displaystyle \\mathbf{y}\\). Consider the vector \\(\\displaystyle \\mathbf{Xw}\\). This is a \\(\\displaystyle n\\times 1\\) vector that contains the predicted labels for all \\(\\displaystyle n\\) houses. So, the loss function can also be written as:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nL(\\mathbf{w}) & =\\sum\\limits _{i=1}^{n}[(\\mathbf{Xw})_{i} -y_{i}]^{2}\\\\\n& \\\\\n& =(\\mathbf{Xw} -\\mathbf{y} )^{T} (\\mathbf{Xw} -\\mathbf{y} )\n\\end{aligned}\n\\end{equation*}\n\\]",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html#mathematical-viewpoint",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html#mathematical-viewpoint",
    "title": "Linear Regression",
    "section": "Mathematical Viewpoint",
    "text": "Mathematical Viewpoint\nAnother way of looking at this problem is as follows. Since we cannot solve \\(\\displaystyle \\mathbf{Xw} =\\mathbf{y}\\), can we at least try to find an approximate solution such that \\(\\displaystyle \\mathbf{Xw} \\approx \\mathbf{y}\\)? It turns out that the loss function \\(\\displaystyle L(\\mathbf{w})\\) defined in the previous section is just such a function that makes the notion of approximation precise. Smaller the value of \\(\\displaystyle L(\\mathbf{w})\\), closer is \\(\\displaystyle \\mathbf{Xw}\\) to \\(\\displaystyle \\mathbf{y}\\), and hence better the approximation.\nLet us revisit the system of equations we started with: \\(\\displaystyle \\mathbf{Xw} =\\mathbf{y}\\). As stated earlier, we will encounter this situation only if our label is a linear function of the features. But it might still be a good idea to understand how to solve the equality case before we try to find an approximate solution. We will therefore try to solve the following sequence of equations:\n\n\\(\\mathbf{Xw} =\\mathbf{0}\\)\n\\(\\mathbf{Xw} =\\mathbf{y}\\)\n\\(\\displaystyle \\mathbf{Xw} \\approx \\mathbf{y}\\)\n\nThe equation \\(\\displaystyle \\mathbf{Xw} =\\mathbf{0}\\) serves as a good starting point before jumping into the more general case of \\(\\displaystyle \\mathbf{Xw} =\\mathbf{y}\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Linear Regression.html#summary",
    "href": "courses/MLF/notes/Part-1/Linear Regression.html#summary",
    "title": "Linear Regression",
    "section": "Summary",
    "text": "Summary\nA linear regression model assumes a linear relationship between inputs and outputs, where the model is expressed as \\(f(\\mathbf{x} )=\\mathbf{w}^{T}\\mathbf{x}\\). If the labeled dataset is \\((\\mathbf{X} ,\\mathbf{y} )\\), our task is to learn the parameters \\(\\mathbf{w}\\) from the data by minimizing a loss function defined as:\n\\[\n\\begin{equation*}\nL(\\mathbf{w} )=(\\mathbf{Xw} -\\mathbf{y} )^{T} (\\mathbf{Xw} -\\mathbf{y} )\n\\end{equation*}\n\\]",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Linear Regression"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html",
    "href": "courses/MLF/notes/Part-1/Datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Question\n\n\n\nWhat is a dataset and why is it important?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html#datasets",
    "href": "courses/MLF/notes/Part-1/Datasets.html#datasets",
    "title": "Datasets",
    "section": "Datasets",
    "text": "Datasets\nThere are different kinds of datasets. The housing dataset that we saw right at the beginning is a tabular dataset. Data comes in the form of a table. Each column of this table is called an attribute or a feature and each row represents one record or observation. Recall that we also use the term data-point to refer to each row of the table. By far, tabular datasets are the most common form in which data is represented. Tabular data can be neatly packed into comma-separated files or CSVs. Few other forms of data:\n\nimage\ntext\nspeech\n\nImage, text and speech data cannot be packed into simple CSVs and are often called unstructured data.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html#whence-comes-data",
    "href": "courses/MLF/notes/Part-1/Datasets.html#whence-comes-data",
    "title": "Datasets",
    "section": "Whence comes data?",
    "text": "Whence comes data?\nHow do we obtain data? Where does data come from? This seems like a simple question but it doesn’t have a simple answer. Here are some scenarios that are arranged in increasing order of complexity:\n\n\n\n\n\n\nScenario-1\n\n\n\nAn FMCG company has given you some historical data concerning its sales over the last three years. It wants you to predict the average sales in the coming quarter.\n\n\nHere we are lucky. Someone comes to our doorstep and gives us the data. It might be the case that the company has neatly arranged the data in a tabular format. In addition, we also have a very precise definition of the problem statement. We have to predict a real number by looking at the data. It is a regression problem.\n\n\n\n\n\n\nScenario-2\n\n\n\nTwitter is developing an algorithm to detect tweets that contain offensive content. As a data scientist, you are given a dump of one million tweets and asked to develop an algorithm to solve the problem.\n\n\nThis is a more challenging problem compared to scenario-1. First, this is an instance of what is called a binary classification problem. Instead of predicting a real number, we have to predict one of two (binary) outcomes for each tweet:\n\noffensive\nnot-offensive\n\nIn order to train a computer to distinguish between the two kinds of tweets, we need to give it examples of tweets of both kinds. Unfortunately, we don’t have that information. If that information is absent, how can we teach the computer to differentiate between the two? So, the first task here is to get the dataset labeled. That is, for each tweet, mark it as “offensive” or “not-offensive”. This process is time consuming and requires considerable manpower, especially if the dataset is large.\n\n\n\n\n\n\nScenario-3\n\n\n\nYou are a research scientist at a manufacturing company. You want to set up a facility that automates the segregation of defective products from non-defective ones. Come up with an end-end ML solution.\n\n\nThis is by far the most challenging scenario. We don’t have access to the data. We need to gather data in the first place. Once we have the data, we need to label it or annotate it. Only then can we start thinking about training ML models on top of the data.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html#supervision",
    "href": "courses/MLF/notes/Part-1/Datasets.html#supervision",
    "title": "Datasets",
    "section": "Supervision",
    "text": "Supervision\nLabeling a dataset is an important part of the data preparation process. However, there may be situations where labeling is not practically feasible. In such cases, we have to settle with unlabeled data. Therefore, datasets in ML can be classified into two categories:\n\nlabeled dataset\nunlabeled dataset\n\nTechniques that work with labeled data fall under the category of supervised learning. Those that work with unlabeled data come under unsupervised learning. What is so special about the term “supervised”?\nCambridge dictionary defines the verb supervise as follows: to watch a person or activity to make certain that everything is done correctly, safely. By a slight extension of this definition, we could say that a supervisor is a teacher who tells us whether we are right or wrong. In this sense, the label performs the role of a supervisor for the machine as it is learning. With unlabeled data, there is no supervision available.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html#partitioning-the-dataset",
    "href": "courses/MLF/notes/Part-1/Datasets.html#partitioning-the-dataset",
    "title": "Datasets",
    "section": "Partitioning the dataset",
    "text": "Partitioning the dataset\nAs humans, how do we know if someone has learnt a skill or not? Tests or exams are the way to go. Exams are so ubiquitous that we often conflate learning with scoring well in exams. However, for a machine, getting a good score in an exam is a good enough proxy for learning. For almost every skill that we can think of, there is some test or exam to evaluate our competency in that skill. Take the analogy that we have been working with: three-digit addition. To know if kids have learnt addition, teachers conduct tests that have problems on three digit addition.\nAn important feature of testing is to make sure that it is challenging. If we ask the same questions that are there in the textbook, kids might score high marks. But chances are that a lot of them would have memorized the answers. Therefore, whenever we have a dataset, we always partition it into two parts:\n\ntrain-dataset\ntest-dataset\n\nWe train the model on the train-dataset and evaluate its performance on the test-dataset. But often, we don’t stop with two partitions, we go for three partitions:\n\ntrain-dataset\nvalidation-dataset\ntest-dataset\n\nThink about the validation-dataset as additional problems for practice or a mock exam that helps the machine learn a good model. The test-dataset is not shown to the model during the learning stage. The learning algorithm has access to only the train-dataset and the validation-dataset. Once the learning process is complete, the model is evaluated on the test-dataset. The test-dataset is sacred in any ML problem. It should be kept hidden and used only at the end. This is analogous to the effort taken by the administration of colleges and universities to seal exam papers and keep them secure until the day of the examination. If the exam paper somehow gets leaked, the exam can no longer be conducted in a fair manner!",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Datasets.html#summary",
    "href": "courses/MLF/notes/Part-1/Datasets.html#summary",
    "title": "Datasets",
    "section": "Summary",
    "text": "Summary\nDatasets come in different types: tabular data, image, text, speech data and so on. The source of data varies from situation to situation. Sometimes the data could be given to us in a well formatted and usable condition. At other times, we would have to expend effort in gathering data and making it suitable for further processing. Datasets could either be labeled or unlabeled. ML algorithms that deal with labeled data are called supervised learning methods. To evaluate the performance of any ML model, it is important to partition the data into two parts: train, test; the model is trained on the training data and evaluated on the test data.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Datasets"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html",
    "href": "courses/MLF/notes/Part-1/Xw=0.html",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "",
    "text": "Question\n\n\n\nHow do we solve for \\(\\mathbf{w}\\) in the equation \\(\\mathbf{Xw}=\\mathbf{0}\\)?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html#setting",
    "href": "courses/MLF/notes/Part-1/Xw=0.html#setting",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "Setting",
    "text": "Setting\nBefore we tackle the general problem of \\(\\mathbf{Xw}=\\mathbf{y}\\), let us first see if we can solve the system when \\(\\mathbf{y}\\) is the zero vector. In all the discussions that follow, this will be our setting:\n\n\\(\\mathbf{X}\\) is a matrix of dimensions \\(n\\times d\\)\n\\(\\mathbf{w}\\in\\mathbb{R}^{d}\\)\n\\(\\mathbf{Xw}\\in\\mathbb{R}^{n}\\)\n\nThe equation that we have taken up is:\n\\[\n\\mathbf{Xw}=\\mathbf{0}\n\\]",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html#nullspace",
    "href": "courses/MLF/notes/Part-1/Xw=0.html#nullspace",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "Nullspace",
    "text": "Nullspace\nWe can immediately see that \\(\\mathbf{w}=\\mathbf{0}\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(\\mathbf{w}_{1}\\). Then, we can see that \\(k\\mathbf{w}_{1}\\) is also a solution. This is because: \\[\n\\mathbf{X}(k\\mathbf{w}_{1})=k\\cdot\\mathbf{Xw}_{1}=\\mathbf{0}\n\\] Also, if \\(\\mathbf{w}_{1}\\) and \\(\\mathbf{w}_{2}\\) are two solutions to the equation, then \\(\\mathbf{w}_{1}+\\mathbf{w}_{2}\\) is also a solution, as: \\[\n\\mathbf{X}(\\mathbf{w}_{1}+\\mathbf{w}_{2})=\\mathbf{Xw}_{1}+\\mathbf{Xw}_{2}=0\n\\] From these two observations, we see that the set of all solutions to the equation \\(\\mathbf{Xw}=\\mathbf{0}\\) is a subspace of \\(\\mathbb{R}^{d}\\). We denote this by \\(\\mathcal{N}(\\mathbf{X})\\) and we call it the nullspace of \\(\\mathbf{X}\\). The dimension of the nullspace is called nullity.\nAll this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B=\\{\\mathbf{w}_{1},\\cdots,\\mathbf{w}_{k}\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(\\mathcal{N}(\\mathbf{X})=\\text{span}(B)\\). To get to the basis, we first need to revisit Gaussian elimination.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html#row-echelon-form",
    "href": "courses/MLF/notes/Part-1/Xw=0.html#row-echelon-form",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "Row-Echelon form",
    "text": "Row-Echelon form\nThe central idea in Gaussian elimination is to transform a matrix into its row-echelon form. Let us take up an example and work with that:\n\\[\n\\mathbf{X}=\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n2 & 1 & 0 & 1\\\\\n3 & 1 & -1 & 1\n\\end{bmatrix}\n\\]\nRecall that we can apply a sequence of any of these three row operations on a matrix:\n\nswap two rows\nscale a row by a non-zero constant\nadd a scalar multiple of a row to another row\n\nStep-1 \\[\n\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n2 & 1 & 0 & 1\\\\\n3 & 1 & -1 & 1\n\\end{bmatrix}\\ \\xrightarrow{R_{2}\\rightarrow R_{2}-2R_{1}}\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n3 & 1 & -1 & 1\n\\end{bmatrix}\n\\]\nStep-2 \\[\n\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n3 & 1 & -1 & 1\n\\end{bmatrix}\\xrightarrow{R_{3}\\rightarrow R_{3}-3R_{1}}\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n0 & 1 & 2 & 1\n\\end{bmatrix}\n\\]\nStep-3 \\[\n\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n0 & 1 & 2 & 1\n\\end{bmatrix}\\xrightarrow{R_{3}\\rightarrow R_{3}-R_{2}}\\begin{bmatrix}1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nThe final matrix that we have is in row-echelon form. Here is a quick reminder of when a matrix is in row-echelon matrix is:\n\nAll rows that have only zeros are at the bottom.\nThe first nonzero entry in a row is always to the right of the first nonzero entry in the row above it.\n\nThe first nonzero entry in a row is called a pivot and columns that contain pivots are called pivot columns. The row echelon form a matrix is not unique. However, we can reduce a matrix to its reduced row echelon form (RREF), which is unique. A matrix is in reduced row echelon form if:\n\nIt is in row echelon form.\nThe only nonzero entry in a pivot column is the pivot.\nAll pivots are equal to \\(1\\).\n\nThe matrix that we have obtained in this example is actually in RREF. Let us call the RREF of \\(\\mathbf{X}\\) as \\(\\mathbf{R}\\). We state the following result without proof.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(\\mathbf{R}\\) is the reduced row echelon form of \\(\\mathbf{X}\\), then \\(\\mathbf{Xw}=\\mathbf{0}\\) if and only if \\(\\mathbf{Rw}=\\mathbf{0}\\)\n\n\nThus, the nullspace of a matrix and the nullspace of its RREF are the same. This lets us forget \\(\\mathbf{X}\\) and instead deal with its row-echelon form directly.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html#recipe-for-a-basis",
    "href": "courses/MLF/notes/Part-1/Xw=0.html#recipe-for-a-basis",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "Recipe for a Basis",
    "text": "Recipe for a Basis\nNow we have to solve the following equation:\n\\[\n\\begin{bmatrix}\n\\textbf{1} & 0 & -1 & 0\\\\\n0 & \\textbf{1} & 2 & 1\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\\begin{bmatrix}w_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\n\\end{bmatrix}=\\begin{bmatrix}0\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\]\nColumns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called pivot variables, while the others are called free variables. We can now state the algorithm for finding a basis for the nullspace of \\(\\mathbf{\\boldsymbol{X}}\\):\n\nAlgorithm\n\n\n\n\n\n\nAlgorithm: Find a basis of \\({\\displaystyle \\mathcal{N}(\\mathbf{X})}\\)\n\n\n\n\n\\(B=\\{\\}\\)\nFor each free variable \\(w_{i}\\):\n\nSet \\(w_{i}=1\\) and \\(w_{j}=0\\) for all free variables, \\(j\\neq i\\)\nSolve for the pivot variables\nAdd \\(\\mathbf{w}\\) to \\(B\\)\n\nReturn \\({\\displaystyle B}\\)​\n\n\n\n\\(B\\) is the required basis. Let us try it out here. \\(w_{1}\\) and \\(w_{2}\\) are the pivot variables. \\(w_{3}\\) and \\(w_{4}\\) are the free variables. First, let us set \\(w_{3}=1,w_{4}=0\\). This gives us \\(w_{1}=1,w_{2}=-2\\). The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^{T}\\). Next, we set \\(w_{3}=0,w_{4}=1\\). This gives us \\(w_{1}=0,w_{2}=-1\\). The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^{T}\\). Thus, a basis for \\(\\mathcal{N}(\\mathbf{X})\\) is:\n\\[\nB=\\left\\{ \\begin{bmatrix}1\\\\\n-2\\\\\n1\\\\\n0\n\\end{bmatrix},\\begin{bmatrix}0\\\\\n-1\\\\\n0\\\\\n1\n\\end{bmatrix}\\right\\}\n\\]\nThe set of all solutions for the equation \\(\\mathbf{Xw}=\\mathbf{0}\\) is \\(\\text{span}(B)\\).\n\n\nProof\nIf you are wondering why this algorithm works, note that the rank of the matrix, \\(r\\), is equal to the number of non-zero rows in the row-echelon form of \\({\\displaystyle \\mathbf{X}}\\). If we look at the columns, then we have \\({\\displaystyle r}\\) pivot columns. These \\({\\displaystyle r}\\) columns are linearly independent by construction. Therefore, each of the remaining \\({\\displaystyle d-r}\\) columns can be expressed as a unique linear combination of these \\({\\displaystyle r}\\) pivot columns. The coefficients of this linear combination are going to come from our \\({\\displaystyle d}\\) variables, \\({\\displaystyle w_{1},\\cdots,w_{d}}\\).\nRecall that the matrix \\({\\displaystyle \\mathbf{X}}\\) is \\({\\displaystyle n\\times d}\\). From the rank-nullity theorem, we know that the nullity is going to be \\(d-r\\). Thus a basis of \\(\\mathcal{N}(\\mathbf{X})\\) will have \\(d-r\\) linearly independent vectors. We have to hunt for these \\(d-r\\) vectors. To get there, we divide the \\(d\\) variables into two parts:\n\n\\(r\\) pivot variables: variables corresponding to the pivot columns\n\\(d-r\\) free variables: all other variables\n\nTo get a basis vector, we set one of the \\(d-r\\) free variables to \\(1\\) and the rest to \\(0\\). Basically, here we are trying to express a non-pivot column as a linear combination of the \\({\\displaystyle r}\\) pivot columns. Then we determine the \\(r\\) pivot variables (coefficients of the combination) by solving the \\(r\\) equations corresponding to them. The resulting \\({\\displaystyle \\mathbf{w}}\\) vector is one element in the basis. By repeating this process with each of the \\(d-r\\) free variables, we are guaranteed to have \\(d-r\\) linearly independent vectors.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=0.html#summary",
    "href": "courses/MLF/notes/Part-1/Xw=0.html#summary",
    "title": "\\(\\mathbf{X} \\mathbf{w}=\\mathbf{0}\\)",
    "section": "Summary",
    "text": "Summary\nIn order to solve the equation \\(\\mathbf{Xw}=\\mathbf{0}\\), we first reduce the matrix \\(\\mathbf{X}\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the nullspace of \\(\\mathbf{X}\\). The span of the basis is the set of all solutions to this equation.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{X} \\mathbf{w}=\\mathbf{0}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=y.html",
    "href": "courses/MLF/notes/Part-1/Xw=y.html",
    "title": "\\(\\mathbf{Xw} = \\mathbf{y}\\)",
    "section": "",
    "text": "Question\n\n\n\nHow do we solve for \\(\\mathbf{w}\\) in the equation \\(\\mathbf{Xw} =\\mathbf{y}\\)?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} = \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=y.html#column-space",
    "href": "courses/MLF/notes/Part-1/Xw=y.html#column-space",
    "title": "\\(\\mathbf{Xw} = \\mathbf{y}\\)",
    "section": "Column space",
    "text": "Column space\nNow we come to the general form of the equation, \\(\\mathbf{Xw} =\\mathbf{y}\\). First, we need to know if the equation admits any solution at all. For this, we have to take a closer look at this equation and see what it means:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n| &  & |\\\\\n\\mathbf{c}_{1} & \\cdots  & \\mathbf{c}_{d}\\\\\n| &  & |\n\\end{bmatrix}\\begin{bmatrix}\nw_{1}\\\\\n\\vdots \\\\\nw_{d}\n\\end{bmatrix} =\\begin{bmatrix}\ny_{1}\\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nHere, \\(\\mathbf{c}_{1} ,\\cdots ,\\mathbf{c}_{d}\\) are the columns of \\(\\mathbf{X}\\). Recall that the product of a matrix and a vector can be interpreted as a linear combination of the columns of the matrix:\n\\[\n\\begin{equation*}\nw_{1}\\mathbf{c}_{1} +\\cdots +w_{d}\\mathbf{c}_{d} =\\mathbf{y}\n\\end{equation*}\n\\]\n\\(\\mathbf{Xw} =\\mathbf{y}\\) has a solution if and only if \\(\\mathbf{y}\\) can be expressed as a linear combination of the columns of \\(\\mathbf{X}\\). Since the set of all linear combination of \\(\\mathbf{X}\\) is given by the \\(\\text{span} (\\{\\mathbf{c}_{1} ,\\cdots ,\\mathbf{c}_{d} \\})\\), the equation is solvable if and only if \\(y\\in \\text{span} (\\{\\mathbf{c}_{1} ,\\cdots ,\\mathbf{c}_{n} \\})\\)\nThe span of the columns of \\(\\mathbf{X}\\) is a subspace of \\(\\displaystyle \\mathbb{R}^{n}\\) and is called the column space of the matrix \\(\\mathbf{X}\\) and we denote it by \\(\\mathcal{R}(\\mathbf{X})\\): \\[\n\\mathcal{R} (\\mathbf{X} )=\\text{span} (\\{\\mathbf{c}_{1} ,\\cdots ,\\mathbf{c}_{d} \\})\n\\] We have now answered the question of when \\(\\mathbf{Xw} =\\mathbf{y}\\) is solvable. Since this is important, let us highlight it:\n\n\n\n\n\n\nImportant\n\n\n\n\\(\\mathbf{Xw}=\\mathbf{y}\\) has a solution if and only if \\(\\mathbf{y}\\) is in the column space of \\(\\mathbf{X}\\)",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} = \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=y.html#conditions-for-solution",
    "href": "courses/MLF/notes/Part-1/Xw=y.html#conditions-for-solution",
    "title": "\\(\\mathbf{Xw} = \\mathbf{y}\\)",
    "section": "Conditions for Solution",
    "text": "Conditions for Solution\nLet us reuse the example from the previous unit:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n1 & 0 & -1 & 0\\\\\n2 & 1 & 0 & 1\\\\\n3 & 1 & -1 & 1\n\\end{bmatrix}\\begin{bmatrix}\nw_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\n\\end{bmatrix} =\\begin{bmatrix}\ny_{1}\\\\\ny_{2}\\\\\ny_{3}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nWe are back to the row-echelon form, but with the augmented matrix:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n1 & 0 & -1 & 0 & y_{1}\\\\\n2 & 1 & 0 & 1 & y_{2}\\\\\n3 & 1 & -1 & 1 & y_{3}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nIf we apply the same sequence of row operations as in the previous case, we get:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n1 & 0 & -1 & 0 & y_{1}\\\\\n0 & 1 & 2 & 1 & y_{2} -2y_{1}\\\\\n0 & 0 & 0 & 0 & y_{3} -y_{1} -y_{2}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nWe can immediately see that the system has a solution if and only if the following condition is met:\n\\[\n\\begin{equation*}\ny_{3} -y_{1} -y_{2} =0\n\\end{equation*}\n\\]\nNow that we have the row-echelon matrix, let us rephrase the equation as follows:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n1 & 0 & -1 & 0\\\\\n0 & 1 & 2 & 1\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\\begin{bmatrix}\nw_{1}\\\\\nw_{2}\\\\\nw_{3}\\\\\nw_{4}\n\\end{bmatrix} =\\begin{bmatrix}\ny_{1}\\\\\ny_{2} -2y_{1}\\\\\ny_{3} -y_{1} -y_{2}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nGoing back to a general system, let us call this system after row reduction \\(\\mathbf{Rw} =\\mathbf{z}\\). We state the following result without proof:\n\n\n\n\n\n\nRemark\n\n\n\n\\(\\mathbf{w}\\) is a solution to \\(\\mathbf{Xw} =\\mathbf{y}\\) if and only if \\(\\mathbf{w}\\) is a solution to \\(\\mathbf{Rw} =\\mathbf{z}\\), where \\(\\mathbf{R} \\ |\\mathbf{\\ z}\\) is the augmented matrix after Gaussian elimination.\n\n\nThis allows us to work with the reduced system while temporarily forgetting the original system.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} = \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=y.html#general-solution",
    "href": "courses/MLF/notes/Part-1/Xw=y.html#general-solution",
    "title": "\\(\\mathbf{Xw} = \\mathbf{y}\\)",
    "section": "General Solution",
    "text": "General Solution\n\nDescription\nIf a solution exists, how do we find it? And what about all possible solutions? Let us now turn our attention to the system after row reduction: \\[\n\\mathbf{R} \\mathbf{x} = \\mathbf{z}\n\\] First note that the set of pivot columns are linearly independent and form a basis for the column space of \\(\\mathbf{R}\\). In the example we are working with, this is quite clear: \\[\n\\begin{equation*}\n\\mathcal{R} (\\mathbf{R} )=\\text{span}\\left(\\left\\{\\begin{bmatrix}\n1\\\\\n0\\\\\n0\n\\end{bmatrix} ,\\begin{bmatrix}\n0\\\\\n1\\\\\n0\n\\end{bmatrix}\\right\\}\\right)\n\\end{equation*}\n\\]\nSo, \\(\\mathbf{y}\\) can be uniquely expressed as a linear combination of the columns of this basis. Let us call this the particular solution \\(\\mathbf{w}_{p}\\). If \\(\\mathbf{w}_{n}\\) is some vector in the nullspace of \\(\\mathbf{X}\\), then every general solution \\(\\mathbf{w}_{g}\\) to the equation can be expressed in this form:\n\\[\n\\begin{equation*}\n\\mathbf{w}_{g} =\\mathbf{w}_{p} +\\mathbf{w}_{n}\n\\end{equation*}\n\\]\nIt may still not be clear why every solution should be of this form and we will prove this shortly. Before that, let us express the set of all solutions to a consistent system more formally: \\[\n\\mathbf{w}_p + \\mathcal{N}(\\mathbf{X})\n\\] This is an affine space. Geometrically, an affine space is a subspace that is translated by some vector. In the simple setting of \\(\\mathbb{R}^{3}\\), if the nullspace is a plane passing through the origin, then an affine space would be a plane parallel to it. Coming back to our context, the set of all solutions to \\(\\mathbf{Xw} = \\mathbf{y}\\) can be obtained by translating the nullspace of \\(\\mathbf{X}\\) by a particular solution \\(\\mathbf{x}_p\\)​​.\n\n\nProof\nWe will show that this affine space is indeed the solution space. Let us call the set of all solutions \\(S\\). Now, pick up any element in the affine space: \\[\n\\mathbf{w} = \\mathbf{w}_p + \\mathbf{w}_n\n\\] where \\(\\mathbf{w}_n \\in \\mathcal{N}(\\mathbf{X})\\). We see that \\(\\mathbf{w}\\) is a solution because: \\[\n\\begin{equation*}\n\\mathbf{Xw} =\\mathbf{X} (\\mathbf{w}_{p} +\\mathbf{w}_{n} )=\\mathbf{Xw}_{p} +\\mathbf{Xw}_{n} =\\mathbf{y}\n\\end{equation*}\n\\] We have shown that any arbitrary element of the affine space is also present in \\(S\\). In other words: \\[\n\\mathbf{w}_p + \\mathcal{N}(\\mathbf{X}) \\subset S\n\\] Now we go the other way. Let \\(\\mathbf{w}\\) be any element in \\(S\\). We can rewrite \\(\\mathbf{w}\\) as: \\[\n\\mathbf{w} = \\mathbf{w}_p + (\\mathbf{w} - \\mathbf{w}_p)\n\\] Now, note that \\(\\mathbf{w} - \\mathbf{w}_p \\in \\mathcal{N}(\\mathbf{X})\\). This is because: \\[\n\\mathbf{X}(\\mathbf{w} - \\mathbf{w}_p) = \\mathbf{Xw} - \\mathbf{Xw}_p = \\mathbf{y} - \\mathbf{y} = \\mathbf{0}\n\\] This shows that \\(S \\subset \\mathbf{w}_p + \\mathcal{N}(\\mathbf{X})\\). Thus we have the following beautiful result:\n\n\n\n\n\n\nImportant\n\n\n\nThe set of all solutions to a consistent system \\(\\mathbf{Xw} = \\mathbf{y}\\) is the affine space \\(\\mathbf{w}_p + \\mathcal{N}(\\mathbf{X})\\), where \\(\\mathbf{w}_{p}\\) is a solution to the system.\n\n\nComing back to the example we are working with, how do we find the particular solution \\(\\mathbf{w}_{p}\\)? We set all free variables to zero and solve for the pivot variables:\n\\[\n\\begin{equation*}\n\\mathbf{w}_{p} =\\begin{bmatrix}\ny_{1}\\\\\ny_{2} -2y_{1}\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\end{equation*}\n\\]\nWe already know how to get \\(\\mathbf{w}_{n}\\). Refer to the previous unit on computing a basis for the nullspace of the matrix.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} = \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Xw=y.html#summary",
    "href": "courses/MLF/notes/Part-1/Xw=y.html#summary",
    "title": "\\(\\mathbf{Xw} = \\mathbf{y}\\)",
    "section": "Summary",
    "text": "Summary\n\\(\\mathbf{Xw} =\\mathbf{y}\\) is solvable if and only if \\(\\mathbf{y}\\) is an element of the column space of \\(\\mathbf{X}\\). A system that admits a solution is said to be consistent. Every solution to a consistent system of equations can be expressed as \\(\\mathbf{w}_{p} +\\mathbf{w}_{n}\\), where \\(\\mathbf{w}_{p}\\) is a particular solution and \\(\\mathbf{w}_{n}\\) is some vector in the nullspace of \\(\\mathbf{X}\\). By varying \\(\\mathbf{w}_n\\), we can get all possible solutions to the system.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} = \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLT/runs/T2-2024/week-3.html#step-1",
    "href": "courses/MLT/runs/T2-2024/week-3.html#step-1",
    "title": "Week-3",
    "section": "Step-1",
    "text": "Step-1\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n0 & 1 & 0 & 1 & 3 & 3 & 5 & 5\\\\\n0 & 0 & 1 & 1 & 1 & 2 & 1 & 2\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{z} = \\begin{bmatrix}\n1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\n\\end{bmatrix}\n\\]\n$$\n$$",
    "crumbs": [
      "Courses",
      "MLT",
      "Runs",
      "T2 2024",
      "Week-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-8.html",
    "href": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Let \\(A\\) be a real \\(3\\times3\\) matrix:\n\\[\nA=\\begin{bmatrix}C_{1} & C_{2} & C_{3}\\end{bmatrix}\n\\]\nFind the determinant of all the following matrices in terms of \\(|A|\\):\n\\[\n\\begin{aligned}\nA_{1} & =\\begin{bmatrix}C_{1} & C_{2}+5C_{3} & C_{3}\\end{bmatrix}\\\\\nA_{2} & =\\begin{bmatrix}C_{1}+C_{2}+C_{3} & C_{2} & C_{3}\\end{bmatrix}\\\\\nA_{3} & =\\begin{bmatrix}C_{1} & C_{2}+5C_{3} & 0\\end{bmatrix}\\\\\nA_{4} & =\\begin{bmatrix}C_{1}+C_{2} & C_{2}+C_{3} & C_{3}+C_{1}\\end{bmatrix}\n\\end{aligned}\n\\]\n\nSince \\(A_{3}\\) has a zero column, \\(|A_{3}|=0\\).\n\nTo get \\(A_{1}\\), we perform \\(C_{2}\\rightarrow C_{2}+5C_{3}\\).\nTo get \\(A_{2}\\), we perform \\(C_{1}\\rightarrow C_{1}+C_{2}+C_{3}\\)\n\nBoth these operations do not change the determinant. So \\(|A_{1}|=|A_{2}|=|A|\\). \\(A_{4}\\) is slightly tricky:\n\\[\n\\begin{aligned}\n\\begin{vmatrix}C_{1}+C_{2} & C_{2}+C_{3} & C_{3}+C_{1}\\end{vmatrix} & =\\begin{vmatrix}2(C_{1}+C_{2}+C_{3}) & C_{2}+C_{3} & C_{3}+C_{1}\\end{vmatrix}\\\\\n& =2\\begin{vmatrix}C_{1}+C_{2}+C_{3} & C_{2}+C_{3} & C_{3}+C_{1}\\end{vmatrix}\\\\\n& =2\\begin{vmatrix}C_{1} & C_{2}+C_{3} & C_{3}+C_{1}\\end{vmatrix}\\\\\n& =2\\begin{vmatrix}C_{1} & C_{2}+C_{3} & C_{3}\\end{vmatrix}\\\\\n& =2\\begin{vmatrix}C_{1} & C_{2} & C_{3}\\end{vmatrix}\\\\\n& =2|A|\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.1",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-4.html",
    "href": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-4.html",
    "title": "Question-4",
    "section": "",
    "text": "Consider the matrix:\n\\[\n\\begin{bmatrix}a & b & c\\\\\nb & c & a\\\\\nc & a & b\n\\end{bmatrix}\n\\]\nIf \\(a+b+c\\) is divisible by \\(6\\), is \\(\\text{det}(A)\\) also divisible by \\(6\\)?\n\nWe can compute the determinant as follows:\n\\[\n\\begin{aligned}\n\\begin{vmatrix}a & b & c\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix} & =\\begin{vmatrix}a+b+c & a+b+c & a+b+c\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix}\\\\\n& =(a+b+c)\\begin{vmatrix}1 & 1 & 1\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix}\\\\\n& =(a+b+c)\\begin{vmatrix}1 & 0 & 0\\\\\nb & c-b & a-b\\\\\nc & a-c & b-c\n\\end{vmatrix}\\\\\n& =(a+b+c)(ab+bc+ca-a^{2}-b^{2}-c^{2})\n\\end{aligned}\n\\]\nThe determinant of the matrix is divisible by \\(a+b+c\\). Therefore, it is also divisible by \\(6\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.1",
      "Question-4"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html",
    "href": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Comment on the truth value of the following statements:",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.5",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html#row-echelon-form",
    "href": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html#row-echelon-form",
    "title": "Question-2",
    "section": "Row echelon form",
    "text": "Row echelon form\nThe row echelon form of a matrix is not unique. As an example, consider the following matrix:\n\\[\nA_{1}=\\begin{bmatrix}1 & 1\\\\\n0 & 2\n\\end{bmatrix}\n\\]\nDividing the second row by \\(2\\) turns \\(A\\) into a matrix in row echelon form:\n\\[\nA_{2}=\\begin{bmatrix}1 & 1\\\\\n0 & 1\n\\end{bmatrix}\n\\]\nBut this is not the only one possible. For instance, we could do a perfectly valid but useless transformation of adding the second row to the first row to give:\n\\[\nA_{3}=\\begin{bmatrix}1 & 2\\\\\n0 & 1\n\\end{bmatrix}\n\\]\n\\(A_{3}\\) is still in row echelon form. We could turn around and say that both \\(A_{3}\\) and \\(A_{1}\\) can be reduced to \\(A_{2}\\). Thus, two unequal matrices could share a common row echelon form.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.5",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html#reduced-row-echelon-form",
    "href": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-2.html#reduced-row-echelon-form",
    "title": "Question-2",
    "section": "Reduced row echelon form",
    "text": "Reduced row echelon form\nThe reduced row echelon form of a matrix is unique. The proof is slightly involved and requires some concepts that are yet to be introduced. The interested reader can take a look at Topics/Linear Algebra/RREF in the sidebar.\n\nFor a diagonal matrix with non-zero diagonal entries, we can divide each row by the corresponding entry and reduce it to the identity matrix. For a non-zero scalar matrix, we can divide each row by the non-zero scalar associated with it to get the identity matrix. We see that the RREF of both matrices is the identity.\nNote that both matrices discussed here are invertible. In fact, it can be shown that the RREF of any invertible matrix is the identity matrix.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.5",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-6.html",
    "href": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-6.html",
    "title": "Question-6",
    "section": "",
    "text": "Let the reduced row echelon form of a matrix \\(A\\) be:\n\\[\n\\begin{bmatrix}1 & 0 & -1\\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\]\nThe first and the third columns of \\(A\\) are \\(\\begin{bmatrix}-1\\\\\n1\n\\end{bmatrix}\\) and \\(\\begin{bmatrix}2\\\\\n-1\n\\end{bmatrix}\\) respectively. Find the second column of \\(A\\) and thereby the complete matrix.\n\nLet the matrix \\(A\\) be:\n\\[\n\\begin{bmatrix}-1 & a & 2\\\\\n1 & b & -1\n\\end{bmatrix}\n\\]\nWe can start row reduction:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}-1 & a & 2\\\\\n1 & b & -1\n\\end{bmatrix} & \\rightarrow\\begin{bmatrix}-1 & a & 2\\\\\n0 & a+b & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\begin{bmatrix}-1 & a & 2\\\\\n0 & a+b & 1\n\\end{bmatrix} & \\rightarrow\\begin{bmatrix}1 & -a & -2\\\\\n0 & a+b & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\nAt this stage, we have to decide if we can divide the second row by \\(a+b\\). This can be done if \\(a+b\\neq0\\). This is the case as \\(a+b=0\\) would mean that the second column is no longer a pivot column.\n\\[\n\\begin{bmatrix}1 & -a & -2\\\\\n0 & a+b & 1\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & -a & -2\\\\\n0 & 1 & \\cfrac{1}{a+b}\n\\end{bmatrix}\n\\]\nIf \\(a=0\\), the matrix becomes:\n\\[\n\\begin{bmatrix}1 & 0 & -2\\\\\n0 & 1 & \\cfrac{1}{b}\n\\end{bmatrix}\n\\]\nThis is in RREF, but no matter what value we choose for \\(b\\), we can never make it equal to the RREF given in the question. So \\(a\\neq0\\) and we can proceed with elimination:\n\\[\n\\begin{bmatrix}1 & -a & -2\\\\\n0 & 1 & \\cfrac{1}{a+b}\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & 0 & \\cfrac{-a-2b}{a+b}\\\\\n\\\\0 & 1 & \\cfrac{1}{a+b}\n\\end{bmatrix}\n\\]\nWe can now do a direct comparison:\n\\[\n\\begin{aligned}\na+b & =1\\\\\n-a-2b & =-1\n\\end{aligned}\n\\]\nFrom this, we get \\(a=1,b=0\\). Thus the matrix \\(A\\) is:\n\\[\n\\begin{bmatrix}1 & 1 & 2\\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.5",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-8.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Let \\(Ax=b\\) be a system of linear equations.\n\\[\nA=\\begin{bmatrix}1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix},\\,\\,\\,x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\n\\end{bmatrix},\\,\\,\\,\\,b=\\begin{bmatrix}b_{1}\\\\\nb_{2}\\\\\nb_{3}\\\\\nb_{4}\n\\end{bmatrix}\n\\]\n\nFind the dependent and independent variables.\nWhen is the system consistent?\nFind out all solutions to this system whenever it is consistent.\n\n\nFirst, note that \\(A\\) is in rref. We first identify the pivots and the pivot columns:\n\\[\n\\begin{bmatrix}\\boldsymbol{1} & 0 & 0 & 0\\\\\n0 & 0 & \\boldsymbol{1} & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nThe first and third columns are pivot columns. Hence, \\(x_{1},x_{3}\\) are dependent and \\(x_{2},x_{4}\\) are independent variables. For the system to be consistent \\(b_{3}=b_{4}=0\\). Let us now work with this special case:\n\\[\nA=\\begin{bmatrix}1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix},\\,\\,\\,x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\n\\end{bmatrix},\\,\\,\\,\\,b=\\begin{bmatrix}b_{1}\\\\\nb_{2}\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\]\nWe can give arbitrary values to \\(x_{2}\\) and \\(x_{4}\\) and then solve for \\(x_{1}\\) and \\(x_{3}\\):\n\\[\n\\begin{aligned}\nx_{2} & =t_{2}\\\\\nx_{4} & =t_{4}\\\\\nx_{3} & =b_{2}-t_{4}\\\\\nx_{1} & =b_{1}\n\\end{aligned}\n\\]\nThe set of all solutions to the system is given by:\n\\[\nS=\\left\\{ \\left(b_{1},t_{2},b_{2}-t_{4},t_{4}\\right):t_{2},t_{4}\\in\\mathbb{R}\\right\\}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-7.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-7.html",
    "title": "Question-7",
    "section": "",
    "text": "Solve the following system using Gaussian elimination.\n\\[\n\\begin{aligned}\n2x_{1}+x_{2} & =3\\\\\nx_{1}+3x_{2} & =4\n\\end{aligned}\n\\]\n\nLet us form the augmented matrix:\n\\[\n\\begin{bmatrix}2 & 1 &  & 3\\\\\n1 & 3 &  & 4\n\\end{bmatrix}\n\\]\nWe now start row reduction:\n\\(R_{1}\\leftrightarrow R_{2}\\)\n\\[\n\\begin{bmatrix}1 & 3 &  & 4\\\\\n2 & 1 &  & 3\n\\end{bmatrix}\n\\]\n\\(R_{2}\\rightarrow R_{2}-2R_{1}\\)\n\\[\n\\begin{bmatrix}1 & 3 &  & 4\\\\\n0 & -5 &  & -5\n\\end{bmatrix}\n\\]\n\\(R_{2}\\rightarrow\\cfrac{-1}{5}R_{2}\\)\n\\[\n\\begin{bmatrix}1 & 3 &  & 4\\\\\n0 & 1 &  & 1\n\\end{bmatrix}\n\\]\nThe matrix is now in REF. We now proceed to get the RREF.\n\\(R_{1}\\rightarrow R_{1}-3R_{2}\\)\n\\[\n\\begin{bmatrix}\\boldsymbol{1} & 0 &  & 1\\\\\n0 & \\boldsymbol{1} &  & 1\n\\end{bmatrix}\n\\]\nWe have transformed \\(Ax=b\\) into \\(Rx=c\\), where \\(R\\) is in RREF. All that remains is to read off the solution here:\n\\[\n\\begin{aligned}\nx_{2} & =1\\\\\nx_{1} & =1\n\\end{aligned}\n\\]\nSince both \\(x_{1}\\) and \\(x_{2}\\) are dependent variables, the solution is unique.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-5.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-5.html",
    "title": "Question-5",
    "section": "",
    "text": "Consider the following system of equations:\n\\[\n\\begin{aligned}\n0x_{1}+x_{2}+0x_{3}+0x_{4} & =1\\\\\n0x_{1}+0x_{2}+x_{3}+0x_{4} & =1\n\\end{aligned}\n\\]\nComment on the dependent and independent variables for the above system.\n\nWe can form the matrix corresponding to this system:\n\\[\n\\begin{bmatrix}0 & \\boldsymbol{1} & 0 & 0\\\\\n0 & 0 & \\boldsymbol{1} & 0\n\\end{bmatrix}\n\\]\nThe second and third columns are pivot columns. Hence \\(x_{2}\\) and \\(x_{3}\\) are dependent variables. \\(x_{1}\\) and \\(x_{4}\\) are independent variables.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-5"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Find the inverse of the following matrix:\n\\[\nA=\\begin{bmatrix}2 & -1 & 0\\\\\n1 & 0 & -1\\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html#method-1",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html#method-1",
    "title": "Question-2",
    "section": "Method-1",
    "text": "Method-1\nWe reduce \\(A\\) to its RREF while simultaneously applying these operations to the identity matrix on the right:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}2 & -1 & 0\\\\\n1 & 0 & -1\\\\\n0 & 1 & -1\n\\end{bmatrix} &  & \\begin{bmatrix}1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}0 & -1 & 2\\\\\n1 & 0 & -1\\\\\n0 & 1 & -1\n\\end{bmatrix} &  & \\begin{bmatrix}1 & -2 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}1 & 0 & -1\\\\\n0 & -1 & 2\\\\\n0 & 1 & -1\n\\end{bmatrix} &  & \\begin{bmatrix}0 & 1 & 0\\\\\n1 & -2 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}1 & 0 & -1\\\\\n0 & -1 & 2\\\\\n0 & 0 & 1\n\\end{bmatrix} &  & \\begin{bmatrix}0 & 1 & 0\\\\\n1 & -2 & 0\\\\\n1 & -2 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}1 & 0 & -1\\\\\n0 & 1 & -2\\\\\n0 & 0 & 1\n\\end{bmatrix} &  & \\begin{bmatrix}0 & 1 & 0\\\\\n-1 & 2 & 0\\\\\n1 & -2 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}1 & 0 & -1\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix} &  & \\begin{bmatrix}0 & 1 & 0\\\\\n1 & -2 & 2\\\\\n1 & -2 & 1\n\\end{bmatrix}\\\\\n\\\\\\begin{bmatrix}1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix} &  & \\begin{bmatrix}1 & -1 & 1\\\\\n1 & -2 & 2\\\\\n1 & -2 & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe have:\n\\[\nA^{-1}=\\begin{bmatrix}1 & -1 & 1\\\\\n1 & -2 & 2\\\\\n1 & -2 & 1\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html#method-2",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-2.html#method-2",
    "title": "Question-2",
    "section": "Method-2",
    "text": "Method-2\nAlternatively, we have the cofactor matrix. For the sake of convenience, let us write down \\(A\\) again:\n\\[\nA=\\begin{bmatrix}2 & -1 & 0\\\\\n1 & 0 & -1\\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\nFirst for the nine minors:\n\\[\n\\begin{aligned}\nM_{11} & =\\begin{vmatrix}0 & -1\\\\\n1 & -1\n\\end{vmatrix} & M_{12} & =\\begin{vmatrix}1 & -1\\\\\n0 & -1\n\\end{vmatrix} & M_{13} & =\\begin{vmatrix}1 & 0\\\\\n0 & 1\n\\end{vmatrix}\\\\\n& =1 &  & =-1 &  & =1\\\\\n\\\\M_{21} & =\\begin{vmatrix}-1 & 0\\\\\n1 & -1\n\\end{vmatrix} & M_{22} & =\\begin{vmatrix}2 & 0\\\\\n0 & -1\n\\end{vmatrix} & M_{23} & =\\begin{vmatrix}2 & -1\\\\\n0 & 1\n\\end{vmatrix}\\\\\n& =1 &  & =-2 &  & =2\\\\\n\\\\M_{31} & =\\begin{vmatrix}-1 & 0\\\\\n0 & -1\n\\end{vmatrix} & M_{32} & =\\begin{vmatrix}2 & 0\\\\\n1 & -1\n\\end{vmatrix} & M_{33} & =\\begin{vmatrix}2 & -1\\\\\n1 & 0\n\\end{vmatrix}\\\\\n& =1 &  & =-2 &  & =1\n\\end{aligned}\n\\]\nAnd now the cofactor matrix:\n\\[\nC=\\begin{bmatrix}1 & 1 & 1\\\\\n-1 & -2 & -2\\\\\n1 & 2 & 1\n\\end{bmatrix}\n\\]\nThe determinant of \\(A\\) is \\(1\\). Finally, the inverse is:\n\\[\n\\begin{aligned}\nA^{-1} & =\\cfrac{1}{\\text{det}(A)}\\text{adj}(A)\\\\\n& =\\cfrac{1}{\\text{det}(A)}\\cdot C^{T}\\\\\n& =\\begin{bmatrix}1 & -1 & 1\\\\\n1 & -2 & 2\\\\\n1 & -2 & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-9.html",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-9.html",
    "title": "Question-9",
    "section": "",
    "text": "What are the consequences of a square matrix being equal to its inverse?\n\n\nWe have \\(A=A^{-1}\\). Multiplying both sides by \\(A\\), we get \\(A^{2}=I\\). This is the first observation.\nSince \\(\\text{det}(A^{-1})=\\frac{1}{\\text{det}(A)}\\), we have:\n\n\\[\n\\text{det}(A)^{2}=1\\implies\\text{det}(A)=\\pm1\n\\]\n\n\\(A=\\pm I\\) are two obvious solutions to \\(A^{2}=I\\). But these two are not the only matrices. Let us quickly construct a non-trivial solution:\n\n\\[\n\\begin{aligned}\nA & =\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\\\\nA^{2} & =\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\\\\n& =\\begin{bmatrix}a^{2}+bc & b(a+d)\\\\\nc(a+d) & d^{2}+bc\n\\end{bmatrix}\\\\\n& =\\begin{bmatrix}1 & 0\\\\\n0 & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe can set \\(a=1,b=c=0,d=-1\\). Thus the matrix \\(A=\\begin{bmatrix}1 & 0\\\\\n0 & -1\n\\end{bmatrix}\\) satisfies \\(A^{2}=I\\). Besides, note that it is its own inverse and its determinant is \\(-1\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-9"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-7.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-7.html",
    "title": "Question-7",
    "section": "",
    "text": "Let \\(v_{1}=(1,0,0),v_{2}=(0,1,0)\\) and \\(v_{3}=(0,0,1)\\) be three vectors and let \\(a,b,c\\) be three real numbers. Express the following vectors in terms of \\(v_{1},v_{2}\\) and \\(v_{3}\\).\n\n\\((a,b,c)\\)\n\\((a,0,c)\\)\n\n\nNotice the following:\n\\[\n\\begin{aligned}\na\\cdot v_{1} & =a\\cdot(1,0,0)=(a,0,0)\\\\\nb\\cdot v_{2} & =b\\cdot(0,1,0)=(0,b,0)\\\\\nc\\cdot v_{3} & =c\\cdot(0,0,1)=(0,0,c)\n\\end{aligned}\n\\]\nTherefore, we have:\n\\[\n\\begin{aligned}\n(a,b,c) & =av_{1}+bv_{2}+cv_{3}\\\\\n(a,0,c) & =av_{1}+cv_{3}\n\\end{aligned}\n\\]\nWe have expressed the vectors \\((a,b,c)\\) and \\((a,0,c)\\) as a linear combination of the vectors \\(v_{1},v_{2}\\) and \\(v_{3}\\). This is a term that you will see throughout the course.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-2.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Let \\(v_{1}=(1,1),v_{2}=(1,0)\\) and \\(v_{3}=(0,1)\\) be three vectors. Compute the following:\n\n\\(2v_{1}+0v_{2}+v_{3}\\)\n\\(0v_{1}+2v_{2}+3v_{3}\\)\n\\(2v_{1}+v_{2}+0v_{3}\\)\n\\(0v_{1}+3v_{2}+2v_{3}\\)\n\n\nWe use the fact that \\(0\\cdot(a,b)=(0,0)\\) for all \\(a,b\\in\\mathbb{R}\\). Now we can go ahead and compute these quantities:\n\\[\n\\begin{aligned}\n2v_{1}+v_{3} & =2(1,1)+(0,1)=(2,3)\\\\\n2v_{2}+3v_{3} & =2(1,0)+3(0,1)=(2,3)\\\\\n2v_{1}+v_{2} & =2(1,1)+(1,0)=(3,2)\\\\\n3v_{2}+2v_{3} & =3(1,0)+2(0,1)=(3,2)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-9.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-9.html",
    "title": "Question-9",
    "section": "",
    "text": "Let \\(u=(1,1,1)\\) and \\(v=(2,-1,4)\\) be two vectors. If \\(cu+3v=(4,j,k)\\), where \\(c,j,k\\) are real numbers, find the value of \\(c\\), \\(j\\) and \\(k\\).\n\n\\[\n\\begin{aligned}\ncu+3v & =(4,j,k)\\\\\nc(1,1,1)+3(2,-1,4) & =(4,j,k)\\\\\n(c+6,c-3,c+12) & =(4,j,k)\\\\\n\\implies c & =-2\\\\\n\\implies j & =-5\\\\\n\\implies k & =10\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-9"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-6.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-6.html",
    "title": "Question-6",
    "section": "",
    "text": "Let \\(a\\) and \\(b\\) be two vectors. Compute the following:\n\n\\(3(a+b)+[(a+b)-(a-b)]\\)\n\\(5(a+b)-[(a+b)-(a-b)]\\)\n\\(3(a+b)+[(a+b)+(a-b)]\\)\n\\(5(a+b)-[(a+b)+(a-b)]\\)\n\n\nFirst:\n\\[\n\\begin{aligned}\n3(a+b)+[(a+b)-(a-b)] & =3a+3b+(a+b-a+b)\\\\\n& =3a+3b+2b\\\\\n& =3a+5b\n\\end{aligned}\n\\]\nSecond:\n\\[\n\\begin{aligned}\n5(a+b)-[(a+b)-(a-b)] & =5a+5b-(a+b-a+b)\\\\\n& =5a+5b-2b\\\\\n& =5a+3b\n\\end{aligned}\n\\]\nThird:\n\\[\n\\begin{aligned}\n3(a+b)+[(a+b)+(a-b)] & =3a+3b+2a\\\\\n& =5a+3b\n\\end{aligned}\n\\]\nFourth:\n\\[\n\\begin{aligned}\n5(a+b)-[(a+b)+(a-b)] & =5a+5b-2a\\\\\n& =3a+5b\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-2.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-2.html",
    "title": "Question-2",
    "section": "",
    "text": "What is the relationship between the following matrices?\n\ndiagonal matrix\nscalar matrix\nidentity matrix\n\n\n\nDiagonal, scalar and identity matrices are all square matrices.\nAn identity matrix is also a scalar matrix.\nA scalar matrix is also a diagonal matrix.\n\nIf \\(S\\) is a scalar matrix, then it can be expressed as \\(S=cI\\) for some scalar \\(c\\), where \\(I\\) is the identity matrix of the same order as the scalar matrix.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Do there exist square matrices \\(A\\) and \\(B\\) of order two such that \\(AB=BA\\)?\nDoes there exist a square matrix \\(A\\) of order two such that \\(A^{2}=A\\)?\nDoes there exist a square matrix \\(A\\) of order two such that \\(A^{2}+A+I=0\\)?",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#abba",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#abba",
    "title": "Question-8",
    "section": "\\(AB=BA\\)",
    "text": "\\(AB=BA\\)\nIn general, matrix multiplication is not commutative. That is, if \\(A\\) and \\(B\\) are two matrices, then \\(AB\\neq BA\\). However, not all pairs of matrices are like this.\n\nA simple but trivial example is the case of \\(A=B\\).\nAnother example is the case of \\(B=I\\), since \\(AI=IA=A\\).\nTwo diagonal matrices always commute. This is because the product of two diagonal matrices is another diagonal matrix whose entries are the product of the corresponding diagonal entries:\n\n\\[\n\\begin{aligned}\nD_{1} & =\\begin{bmatrix}2 & 0\\\\\n0 & 3\n\\end{bmatrix}\\\\\nD_{2} & =\\begin{bmatrix}3 & 0\\\\\n0 & 5\n\\end{bmatrix}\\\\\nD_{1}D_{2} & =D_{2}D_{1}=\\begin{bmatrix}6 & 0\\\\\n0 & 15\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#a2a",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#a2a",
    "title": "Question-8",
    "section": "\\(A^{2}=A\\)",
    "text": "\\(A^{2}=A\\)\nIn the case of \\(A^{2}=A\\), we can see that \\(A=I\\) would satisfy \\(A^{2}=A\\). Interestingly, \\(A=-I\\) would not satisfy \\(A^{2}=A\\). Compare this to the single variable equation \\(a^{2}=a\\), in which \\(a=-1\\) is also a solution. To construct a non-trivial example, we can take any \\(2\\times2\\) matrix \\(A=\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\)\n\\[\n\\begin{aligned}\n\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix} & =\\begin{bmatrix}a^{2}+bc & ab+bd\\\\\nac+cd & bc+d^{2}\n\\end{bmatrix}\\\\\n& =\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe now have:\n\\[\n\\begin{aligned}\na^{2}+bc & =a\\\\\nb(a+d) & =b\\\\\nc(a+d) & =c\\\\\nd^{2}+bc & =d\n\\end{aligned}\n\\]\nOne solution can be \\(a=1,b=2,c=0,d=0\\) which gives us the matrix \\(\\begin{bmatrix}1 & 2\\\\\n0 & 0\n\\end{bmatrix}\\). You can verify that \\(A^{2}=A\\) for this matrix.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#a2ai0",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-8.html#a2ai0",
    "title": "Question-8",
    "section": "\\(A^{2}+A+I=0\\)",
    "text": "\\(A^{2}+A+I=0\\)\nFirst consider the corresponding equation in one variable:\n\\[\na^{2}+a+1=0\n\\]\nThis equation does not have any real solutions. Can we expect something similar for the matrix equation? Taking a general matrix, we get:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}a^{2}+bc & ab+bd\\\\\nac+cd & bc+d^{2}\n\\end{bmatrix}+\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}+\\begin{bmatrix}1 & 0\\\\\n0 & 1\n\\end{bmatrix} & =\\begin{bmatrix}a^{2}+bc+a+1 & ab+bd+b\\\\\nac+cd+c & bc+d^{2}+d+1\n\\end{bmatrix}\n\\end{aligned}\n\\]\nSetting this to the zero matrix:\n\\[\n\\begin{aligned}\na^{2}+a+1+bc & =0\\\\\nb(a+d+1) & =0\\\\\nc(a+d+1) & =0\\\\\nd^{2}+d+1+bc & =0\n\\end{aligned}\n\\]\nIf \\(a+d+1\\neq0\\), then \\(b=c=0\\). This would imply that \\(a^{2}+a+1=d^{2}+d+1=0\\), which is impossible for real numbers \\(a,d\\). Therefore, \\(a+d+1=0\\). Equating the first and last equations, we get:\n\\[\na^{2}+a+1=d^{2}+d+1\\implies(a-d)(a+d+1)=0.\n\\]\nWe see that \\(a+d+1=0\\) will take care of the second and third equations. We now take up the first equation:\n\\[\n\\begin{aligned}\na^{2}+a+1+bc & =0\\\\\n\\left(a+\\frac{1}{2}\\right)^{2}+\\frac{3}{4}+bc & =0\\\\\n\\left(a+\\frac{1}{2}\\right)^{2} & =-\\left(\\frac{3}{4}+bc\\right)\n\\end{aligned}\n\\]\nWe can use this to get one set of values:\n\\[\na=-\\frac{1}{2},b=3,c=-\\frac{1}{4},d=-\\frac{1}{2}\n\\]\nThe resulting matrix is:\n\\[\n\\begin{bmatrix}-\\frac{1}{2} & 3\\\\\n\\\\\\frac{-1}{4} & -\\frac{1}{2}\n\\end{bmatrix}\n\\]\nWe can characterize all possible solutions here. This is left as an exercise to the reader.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-10.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-10.html",
    "title": "Question-10",
    "section": "",
    "text": "Let \\(A=\\begin{bmatrix}-1 & 2\\\\\n-4 & 7\n\\end{bmatrix}\\) and \\(A^{2}-\\alpha A+I=0\\) for some \\(\\alpha\\in\\mathbb{R}\\). Find the value of \\(\\alpha\\).\n\nWe have:\n\\[\nA^{2}=\\begin{bmatrix}-1 & 2\\\\\n-4 & 7\n\\end{bmatrix}\\begin{bmatrix}-1 & 2\\\\\n-4 & 7\n\\end{bmatrix}=\\begin{bmatrix}-7 & 12\\\\\n-24 & 41\n\\end{bmatrix}\n\\]\nNow:\n\\[\n\\begin{aligned}\nA^{2}-\\alpha A+I & =\\begin{bmatrix}-7 & 12\\\\\n-24 & 41\n\\end{bmatrix}-\\alpha\\begin{bmatrix}-1 & 2\\\\\n-4 & 7\n\\end{bmatrix}+\\begin{bmatrix}1 & 0\\\\\n0 & 1\n\\end{bmatrix}\\\\\n\\\\ & =\\begin{bmatrix}-7+\\alpha+1 & 12-2\\alpha\\\\\n-24+4\\alpha & 41-7\\alpha+1\n\\end{bmatrix}\\\\\n\\\\ & =\\begin{bmatrix}0 & 0\\\\\n0 & 0\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe can now see that \\(\\boxed{\\alpha=6}\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-10"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-7.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-7.html",
    "title": "Question-7",
    "section": "",
    "text": "Let \\(A=[\\delta_{ij}]\\) be a matrix of order \\(3\\) such that \\(\\delta_{ij}=\\begin{cases}\n0 & \\text{if }i&gt;j\\\\\nj & \\text{if }i\\leq j\n\\end{cases}\\). Is \\(A\\) upper triangular or lower triangular? Find \\(\\text{det}(A)\\).\n\n\\[\nA=\\begin{bmatrix}1 & 2 & 3\\\\\n0 & 2 & 3\\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\]\n\\(A\\) is upper triangular as all entries below the main diagonal are zero. \\(\\text{det}(A)=1\\times2\\times3=6\\), the product of the diagonal entries since \\(A\\) is upper triangular.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-10.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-10.html",
    "title": "Question-10",
    "section": "",
    "text": "Suppose \\(A\\) and \\(B\\) are two \\(3\\times3\\) matrices such that \\(\\text{det}(A)=4\\) and \\(B=3A\\), find the value of \\(\\sqrt[3]{\\text{det}(A^{2}B)}\\).\n\nFirst:\n\\[\n\\begin{aligned}\n\\text{det}(B) & =\\text{det}(3A)\\\\\n& =3^{3}\\text{det}(A)\\\\\n& =27\\times4\\\\\n& =108\n\\end{aligned}\n\\]\nNext:\n\\[\n\\begin{aligned}\n\\text{det}(A^{2}B) & =\\text{det}(A)^{2}\\text{det}(B)\\\\\n& =16\\times108\\\\\n& 1728\n\\end{aligned}\n\\]\nFinally, \\(\\sqrt[3]{1728}=12\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-10"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-3.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-3.html",
    "title": "Question-3",
    "section": "",
    "text": "If all the elements of a \\(n\\times n\\) matrix \\(A\\) are the same, find the determinant of \\(A\\) and \\(A+A^{T}\\).\n\nWe can perform the row operation \\(R_{1}\\rightarrow R_{1}-R_{2}\\). This would leave the determinant unchanged and will result in a zero row. Thus \\(|A|=0\\). Since \\(A+A^{T}=2A\\) for this matrix, \\(|A+A^{T}|=0\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-2.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Consider the two matrices:\n\\[\nA=\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n-1 & 0 & 2\n\\end{bmatrix},\\,\\,\\,\\,B=\\begin{bmatrix}1 & 3 & -2\\\\\n0 & 1 & -1\\\\\n3 & 4 & 2\n\\end{bmatrix}\n\\]\nFind \\(\\text{det}(A),\\text{det}(B),\\text{det}(AB),\\text{det}(BA)\\).\n\nThe following solution to find the determinant is unnecessarily long. We can perform a sequence of row operations on \\(A\\):\n\\(R_{3}\\rightarrow R_{3}+R_{2}\\)\n\\[\n\\begin{aligned}\n\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n-1 & 0 & 2\n\\end{bmatrix} & \\rightarrow\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n0 & -3 & 6\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThis operation doesn’t change the determinant.\n\\(R_{3}\\rightarrow\\frac{-1}{3}R_{3}\\)\n\\[\n\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n0 & -3 & 6\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n0 & 1 & -2\n\\end{bmatrix}\n\\]\nThis operation scales the determinant by \\(\\frac{-1}{3}\\).\n\\(R_{1}\\rightarrow R_{1}-2R_{2}\\)\n\\[\n\\begin{bmatrix}2 & -1 & 1\\\\\n1 & -3 & 4\\\\\n0 & 1 & -2\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}0 & 5 & -7\\\\\n1 & -3 & 4\\\\\n0 & 1 & -2\n\\end{bmatrix}\n\\]\nThis operation doesn’t change the determinant.\n\\(R_{1}\\leftrightarrow R_{2}\\)\n\\[\n\\begin{bmatrix}0 & 5 & -7\\\\\n1 & -3 & 4\\\\\n0 & 1 & -2\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 5 & -7\\\\\n0 & 1 & -2\n\\end{bmatrix}\n\\]\nThis operation scales the determinant by \\(-1\\).\n\\(R_{2}\\rightarrow R_{2}-5R_{3}\\)\n\\[\n\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 5 & -7\\\\\n0 & 1 & -2\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 0 & 3\\\\\n0 & 1 & -2\n\\end{bmatrix}\n\\]\nThis operation doesn’t change the determinant.\n\\(R_{2}\\rightarrow\\frac{1}{3}R_{2}\\)\n\\[\n\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 0 & 3\\\\\n0 & 1 & -2\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 0 & 1\\\\\n0 & 1 & -2\n\\end{bmatrix}\n\\]\nThis operation scales the determinant by \\(\\frac{1}{3}\\).\n\\(R_{2}\\leftrightarrow R_{3}\\)\n\\[\n\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 0 & 1\\\\\n0 & 1 & -2\n\\end{bmatrix}\\rightarrow\\begin{bmatrix}1 & -3 & 4\\\\\n0 & 1 & -2\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nThis operation scales the determinant by \\(-1\\).\nThe final matrix that we have is an upper triangular matrix. The determinant of an upper triangular matrix is the product of its diagonal entries, which is \\(1\\) in this case. So we have:\n\\[\n\\begin{aligned}\n1 & =\\text{det}(A)\\times\\frac{-1}{3}\\times(-1)\\times\\frac{1}{3}\\times(-1)\\\\\n\\text{det}(A) & =-9\n\\end{aligned}\n\\]\nFor \\(\\text{det}(B)\\), we will expand the determinant along the first column:\n\\[\n\\begin{vmatrix}1 & 3 & -2\\\\\n0 & 1 & -1\\\\\n3 & 4 & 2\n\\end{vmatrix}=1(2+4)+3(-3+2)=3\n\\]\nFor \\(\\text{det}(AB)\\), we just use the property that \\(\\text{det}(AB)=\\text{det}(A)\\cdot\\text{det}(B)\\). Thus, we get \\(\\text{det}(AB)=-27\\). \\(\\text{det}(BA)\\) is going to be the same.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-1.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-1.html",
    "title": "Question-1",
    "section": "",
    "text": "Let \\(A\\) be a \\(3\\times3\\) matrix with non-zero determinant. If \\(\\text{det}(2A)=k\\text{det}(A)\\), find the value of \\(k\\).\n\nIf a row is scaled by a constant, the determinant is scaled by the same constant. When a matrix is scaled by a constant, each row of the determinant is scaled by the same constant. With this, for a \\(n\\times n\\) matrix \\(A\\):\n\\[\n\\begin{aligned}\n\\text{det}(cA) & =c^{n}\\text{det}(A)\n\\end{aligned}\n\\]\nIn this question \\(n=3\\) and \\(c=2\\):\n\\[\n\\text{det}(2A)=8\\text{\\ensuremath{\\cdot}det}(A)\n\\]\nTherefore, \\(k=8\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-1"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-4.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-4.html",
    "title": "Question-4",
    "section": "",
    "text": "Let \\(A=\\begin{bmatrix}a_{11} & a_{12} & a_{13}\\\\\nta_{11}-sa_{31} & ta_{12}-sa_{32} & ta_{13}-sa_{33}\\\\\nra_{31} & ra_{32} & ra_{33}\n\\end{bmatrix}\\) be a matrix and \\(r,s,t\\neq0\\). Find \\(\\text{det}(A)\\).\n\nConsider the matrix \\(B\\):\n\\[\nB=\\begin{bmatrix}a_{11} & a_{12} & a_{13}\\\\\n0 & 0 & 0\\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\nWe can now do the following row operations on \\(B\\):\n\\[\n\\begin{aligned}\nR_{2} & \\rightarrow R_{2}+tR_{1}-sR_{3}\\\\\nR_{3} & \\rightarrow rR_{3}\n\\end{aligned}\n\\]\nThese two row operations will give us \\(A\\). The determinants are related as follows:\n\\[\n\\text{det}(A)=r\\cdot\\text{det}(B)\n\\]\nSince \\(B\\) has a zero row, its determinant is zero. Hence \\(\\text{det}(A)=0\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-4"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-1.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-1.html",
    "title": "Question-1",
    "section": "",
    "text": "Consider the following system of linear equations:\n\\[\n\\begin{aligned}\n-2x_{1}+3x_{2}+x_{3} & =1\\\\\n-x_{1}+x_{3} & =0\\\\\n2x_{2} & =5\n\\end{aligned}\n\\]\n\nConvert the system into the form \\(Ax=b\\).\nSolve the system.\n\n\nWe have:\n\\[\nA=\\begin{bmatrix}-2 & 3 & 1\\\\\n-1 & 0 & 1\\\\\n0 & 2 & 0\n\\end{bmatrix},\\,\\,\\,x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\\\\\nx_{3}\n\\end{bmatrix},\\,\\,\\,b=\\begin{bmatrix}1\\\\\n0\\\\\n5\n\\end{bmatrix}\n\\]\nFrom the last equation, we see that \\(x_{2}=\\frac{5}{2}\\). The second equation shows that \\(x_{1}=x_{3}\\). Using these two facts in equation-(1), we get \\(x_{1}=x_{3}=3x_{2}-1=\\frac{13}{2}\\). The solution is unique and is given by \\(\\left(\\frac{13}{2},\\frac{5}{2},\\frac{13}{2}\\right)\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-1"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-8.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Let \\(v\\) be a solution of the systems \\(A_{1}x=b\\) and \\(A_{2}x=b\\). Construct two systems that have \\(v\\) as a solution.\n\nAs before, we can add these two systems to begin with. Since \\(v\\) is a solution to both systems:\n\\[\n\\begin{aligned}\nA_{1}v & =b\\\\\nA_{2}v & =b\\\\\n\\implies A_{1}v+A_{2}v & =2b\\\\\n\\implies(A_{1}+A_{2})v & =2b\n\\end{aligned}\n\\]\nThus, \\(v\\) is a solution to the system \\((A_{1}+A_{2})x=2b\\). Next, subtracting the two systems:\n\\[\n\\begin{aligned}\nA_{1}v & =b\\\\\nA_{2}v & =b\\\\\n\\implies A_{1}v-A_{2}v & =0\\\\\n\\implies(A_{1}-A_{2})v & =0\n\\end{aligned}\n\\]\nThus, \\(v\\) is a solution to the system \\((A_{1}-A_{2})x=0\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-6.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-6.html",
    "title": "Question-6",
    "section": "",
    "text": "Consider a system of linear equations:\n\\[\n\\begin{aligned}\n2x_{1}+3x_{2} & =6\\\\\n-2x_{1}+kx_{2} & =d\\\\\n4x_{1}+6x_{2} & =12\n\\end{aligned}\n\\]\nCome up with all possible values that \\(k\\) and \\(d\\) can take so that the system has:\n\na unique solution\ninfinitely many solutions\nno solution\n\n\nLet us take one more look at the equations. Dividing the third equation by \\(2\\), we get:\n\\[\n\\begin{aligned}\n2x_{1}+3x_{2} & =6\\\\\n-2x_{1}+kx_{2} & =d\\\\\n2x_{1}+3x_{2} & =6\n\\end{aligned}\n\\]\nThe first and last equations are essentially the same. Therefore we only have two equations:\n\\[\n\\begin{aligned}\n2x_{1}+3x_{2} & =6\\\\\n-2x_{1}+kx_{2} & =d\n\\end{aligned}\n\\]\nIt is now convenient to think about this geometrically. For the system to have no solution, these two equations should correspond to two distinct parallel lines. They have the same slope but different intercepts. This happens when:\n\\[\n\\cfrac{2}{-2}=\\cfrac{3}{k}\\neq\\cfrac{6}{d}\n\\]\nThis gives us \\(k=-3\\) and \\(d\\neq-6\\). For the system to have infinitely many solutions, the lines should be identical, which happens when:\n\\[\n\\cfrac{2}{-2}=\\cfrac{3}{k}=\\cfrac{6}{d}\n\\]\nThis gives \\(k=-3\\) and \\(d=-6\\). For the system to have a unique solution, the slopes should be different:\n\\[\n\\cfrac{2}{-2}\\neq\\cfrac{3}{k}\\implies k\\neq-3\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/orientation.html",
    "href": "courses/Maths-2/runs/T2-2024/orientation.html",
    "title": "Support",
    "section": "",
    "text": "Support\n\nInstructors\nTeaching Assistants (TAs)\n\nPranjal\nGiri\n\n\n\n\nLive Sessions\n\nFour instructor sessions\n\n3 to 4 PM\nTue, Wed, Thu\n8 to 10 PM, Sunday\nAll of them will be streamed\n\nAt least two TA sessions\n\n\n\nSupplementary Content\n\nMaster Folder\n\nPA solutions\nGA solutions\nBook (Linear Algebra)\nLive Session links\nNotes\n\n\n\n\nSyllabus\n\nLinear Algebra\n\nweeks 1 to 8\n\nMultivariable calculus\n\nweeks 9 to 11\n\n\n\n\nPre-requisites\n \nMaths-1\n\nSet theory\n\nintersection, union, subset, superset\n\nFunctions\n\ndefinition\none-one, onto, one-one and onto function\ninverse\n\nCalculus\n\nLimits\nContinuity\nDifferentiable\n\n\n \n\n\nLeads to\n\nMLF (diploma level, DS)\nMLT (diploma level, DS)\nDeep Learning (degree level)\nLLM -&gt; large language models (degree level)\n\n\n\nNature of Course\n\nAbstract mathemaitcs\n\n\\(S = \\left\\{ \\sqrt{2}, \\pi\\right\\}\\)\n\nComputational mathematics\n\nFormula -&gt; physical quantities\nAlgorithms -&gt; Sequence of steps\n\n\nComputing the zeros/roots of a polynomial\n\\[\nx^2 - 5 x + 6 = 0 \\Rightarrow x^2 - 3 x - 2 x + 6 = 0 \\Rightarrow x (x - 3)\n   - 2 (x - 3) = 0 \\Rightarrow x = 2, 3\n\\]\n\\(\\mathbf{A} x = b\\)",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024",
      "Support"
    ]
  },
  {
    "objectID": "courses/Maths-2/slides/demo.html#testing-chalkboard",
    "href": "courses/Maths-2/slides/demo.html#testing-chalkboard",
    "title": "Vector Spaces",
    "section": "Testing chalkboard",
    "text": "Testing chalkboard\nSolve for \\(x\\):\n\\[\nx^2 - 5x + 6 = 0\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Slides",
      "Vector Spaces"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "My name is Karthik Thiagarajan, a teaching fellow at the BS program in Data Science and Application, IIT Madras. I have been associated with the BS program since its inception in the year 2021. I have a BTech degree in mechanical engineering from IIT Madras. I have been a part of the following courses in varying capacities:\n\nCT\nPython\nMaths-2\nMLF\nMLT\nRL\n\nI maintain the content that I create as a part of these courses here. My Discourse handle is Karthik_POD. You can navigate through the content by using the sidebar."
  },
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "Slides for workshops.",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#announcements",
    "href": "workshops/intelligent_machines/index.html#announcements",
    "title": "Intelligent Machines",
    "section": "Announcements",
    "text": "Announcements\n\nIIT Madras is a plastic free zone.\nDo not engage with the wildlife.\nDo not have your lunch inside the classroom.",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#human-intelligence",
    "href": "workshops/intelligent_machines/index.html#human-intelligence",
    "title": "Intelligent Machines",
    "section": "Human Intelligence",
    "text": "Human Intelligence\n\n\n\n\nHoward Gardner\n\n\n\n\nTheory of multiple intelligences, Frames of mind, 1983\n\n\nLogical-mathematical\nBodily-Kinesthetic\nLinguistic\nMusical\nVisual-Spatial intelligence\nInterpersonal\nIntrapersonal",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#human-intelligence-1",
    "href": "workshops/intelligent_machines/index.html#human-intelligence-1",
    "title": "Intelligent Machines",
    "section": "Human Intelligence",
    "text": "Human Intelligence",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#evolution",
    "href": "workshops/intelligent_machines/index.html#evolution",
    "title": "Intelligent Machines",
    "section": "Evolution",
    "text": "Evolution",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#human-vision",
    "href": "workshops/intelligent_machines/index.html#human-vision",
    "title": "Intelligent Machines",
    "section": "Human Vision",
    "text": "Human Vision",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#what-is-a-machine",
    "href": "workshops/intelligent_machines/index.html#what-is-a-machine",
    "title": "Intelligent Machines",
    "section": "What is a machine?",
    "text": "What is a machine?\n\n\nAnything that reduces human effort.\n\n\n\n– Prof Dubey",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#evolution-1",
    "href": "workshops/intelligent_machines/index.html#evolution-1",
    "title": "Intelligent Machines",
    "section": "Evolution",
    "text": "Evolution",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#kasparov-vs-deep-blue",
    "href": "workshops/intelligent_machines/index.html#kasparov-vs-deep-blue",
    "title": "Intelligent Machines",
    "section": "Kasparov vs Deep Blue",
    "text": "Kasparov vs Deep Blue",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#atari",
    "href": "workshops/intelligent_machines/index.html#atari",
    "title": "Intelligent Machines",
    "section": "Atari",
    "text": "Atari",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#go",
    "href": "workshops/intelligent_machines/index.html#go",
    "title": "Intelligent Machines",
    "section": "Go",
    "text": "Go",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#alphago-vs-lee-sedol",
    "href": "workshops/intelligent_machines/index.html#alphago-vs-lee-sedol",
    "title": "Intelligent Machines",
    "section": "AlphaGo vs Lee Sedol",
    "text": "AlphaGo vs Lee Sedol",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#alphastar",
    "href": "workshops/intelligent_machines/index.html#alphastar",
    "title": "Intelligent Machines",
    "section": "AlphaStar",
    "text": "AlphaStar",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#generative-ai",
    "href": "workshops/intelligent_machines/index.html#generative-ai",
    "title": "Intelligent Machines",
    "section": "Generative AI",
    "text": "Generative AI\n\nChat-GPT\nGemini\nSonnet-3.5\nPerplexity\nImagen\nDALL \\(\\cdot\\) E3\nImagen Video\nVEO\nV2A",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#enablers",
    "href": "workshops/intelligent_machines/index.html#enablers",
    "title": "Intelligent Machines",
    "section": "Enablers",
    "text": "Enablers",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#data",
    "href": "workshops/intelligent_machines/index.html#data",
    "title": "Intelligent Machines",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#hardware",
    "href": "workshops/intelligent_machines/index.html#hardware",
    "title": "Intelligent Machines",
    "section": "Hardware",
    "text": "Hardware\n\n\n\n\n\nNVIDIA V100 Tensor Core GPU\nEquivalent to \\(32\\) CPUs\n\\(7\\) teraflops",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#pattern-recognition",
    "href": "workshops/intelligent_machines/index.html#pattern-recognition",
    "title": "Intelligent Machines",
    "section": "Pattern Recognition",
    "text": "Pattern Recognition",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#decision-making",
    "href": "workshops/intelligent_machines/index.html#decision-making",
    "title": "Intelligent Machines",
    "section": "Decision Making",
    "text": "Decision Making",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#dataset",
    "href": "workshops/intelligent_machines/index.html#dataset",
    "title": "Intelligent Machines",
    "section": "Dataset",
    "text": "Dataset",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#learning-from-data-1",
    "href": "workshops/intelligent_machines/index.html#learning-from-data-1",
    "title": "Intelligent Machines",
    "section": "Learning from Data",
    "text": "Learning from Data",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#data-rightarrow-decisions",
    "href": "workshops/intelligent_machines/index.html#data-rightarrow-decisions",
    "title": "Intelligent Machines",
    "section": "Data \\(\\rightarrow\\) Decisions",
    "text": "Data \\(\\rightarrow\\) Decisions",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#neuron",
    "href": "workshops/intelligent_machines/index.html#neuron",
    "title": "Intelligent Machines",
    "section": "Neuron",
    "text": "Neuron",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#neural-network",
    "href": "workshops/intelligent_machines/index.html#neural-network",
    "title": "Intelligent Machines",
    "section": "Neural Network",
    "text": "Neural Network",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#partition-the-data",
    "href": "workshops/intelligent_machines/index.html#partition-the-data",
    "title": "Intelligent Machines",
    "section": "Partition the Data",
    "text": "Partition the Data",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#industry",
    "href": "workshops/intelligent_machines/index.html#industry",
    "title": "Intelligent Machines",
    "section": "Industry",
    "text": "Industry",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#academia",
    "href": "workshops/intelligent_machines/index.html#academia",
    "title": "Intelligent Machines",
    "section": "Academia",
    "text": "Academia",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#people",
    "href": "workshops/intelligent_machines/index.html#people",
    "title": "Intelligent Machines",
    "section": "People",
    "text": "People\n\n\n\n\nGeoffrey Hinton\n\n\n\n\n\n\n\nYoshua Bengio\n\n\n\n\n\n\n\nYann LeCun",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#optimus-tesla",
    "href": "workshops/intelligent_machines/index.html#optimus-tesla",
    "title": "Intelligent Machines",
    "section": "Optimus | Tesla",
    "text": "Optimus | Tesla",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#optimus-tesla-1",
    "href": "workshops/intelligent_machines/index.html#optimus-tesla-1",
    "title": "Intelligent Machines",
    "section": "Optimus | Tesla",
    "text": "Optimus | Tesla",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#digit-agility-robotics",
    "href": "workshops/intelligent_machines/index.html#digit-agility-robotics",
    "title": "Intelligent Machines",
    "section": "Digit | Agility Robotics",
    "text": "Digit | Agility Robotics",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "workshops/intelligent_machines/index.html#sophia-hanson-robotics",
    "href": "workshops/intelligent_machines/index.html#sophia-hanson-robotics",
    "title": "Intelligent Machines",
    "section": "Sophia | Hanson Robotics",
    "text": "Sophia | Hanson Robotics",
    "crumbs": [
      "Workshops",
      "Intelligent Machines"
    ]
  },
  {
    "objectID": "courses/Maths-2/index.html",
    "href": "courses/Maths-2/index.html",
    "title": "Maths-2",
    "section": "",
    "text": "Some of the resources you will find here:\n\nSolutions to activity questions\nSolutions to practice assignment questions\nNotes on some topics\nLive session notes",
    "crumbs": [
      "Courses",
      "Maths-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/index.html#overview",
    "href": "courses/Maths-2/runs/T2-2024/index.html#overview",
    "title": "T2-2024",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/index.html#schedule",
    "href": "courses/Maths-2/runs/T2-2024/index.html#schedule",
    "title": "T2-2024",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/index.html#lectures",
    "href": "courses/Maths-2/runs/T2-2024/index.html#lectures",
    "title": "T2-2024",
    "section": "Lectures",
    "text": "Lectures\n\n\n\nSession\nNotes",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/index.html#aq-and-pa",
    "href": "courses/Maths-2/runs/T2-2024/index.html#aq-and-pa",
    "title": "T2-2024",
    "section": "AQ and PA",
    "text": "AQ and PA",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024"
    ]
  },
  {
    "objectID": "courses/Maths-2/runs/T2-2024/index.html#quizzes",
    "href": "courses/Maths-2/runs/T2-2024/index.html#quizzes",
    "title": "T2-2024",
    "section": "Quizzes",
    "text": "Quizzes",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Runs",
      "T2-2024"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/index.html",
    "href": "courses/Maths-2/AQ/index.html",
    "title": "Activity Questions",
    "section": "",
    "text": "Some of the questions may be presented in a different form compared to what you observe on the portal.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html",
    "title": "Question-3",
    "section": "",
    "text": "Comment on the truth value of the following statements:",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-1",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-1",
    "title": "Question-3",
    "section": "Statement-1",
    "text": "Statement-1\nGeometrically, it is quite clear that these are the only three possibilities. Algebraically, let us try to understand the case of infinite solutions a little better. Why can’t a particular system have only two solutions, for instance? Let \\(x_{1}\\) and \\(x_{2}\\) be two solutions, then \\((c+1)x_{1}-cx_{2}\\) is also a solution. This is because:\n\\[\n\\begin{aligned}\nA[(c+1)x_{1}-cx_{2}] & =(c+1)Ax_{1}-cAx_{2}\\\\\n& =(c+1)b-cb\\\\\n& =b\n\\end{aligned}\n\\]\nSince \\(c\\) is an arbitrary parameter, we see that there are infinitely many solutions.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-2",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-2",
    "title": "Question-3",
    "section": "Statement-2",
    "text": "Statement-2\nMultiplying each equation by \\(c\\) corresponds to scaling each row of the matrix \\(A\\) by \\(c\\) and scaling each component of the vector \\(b\\) by \\(c\\). Multiplying each row of \\(A\\) by \\(c\\) is the same as multiplying the entire matrix by \\(c\\). Scaling each component of \\(b\\) by \\(c\\) is the same as scaling the vector \\(b\\) by \\(c\\). If \\(x^{*}\\) is a solution of the system \\(Ax=b\\), then:\n\\[\n\\begin{aligned}\nAx^{*} & =b\\\\\ncAx^{*} & =cb\\\\\n(cA)x^{*} & =cb\n\\end{aligned}\n\\]\nNote that the second step is valid only if \\(c\\) is a non-zero constant. We see that the new system is \\((cA)x^{*}=cb\\). The solution of this system is still \\(x^{*}\\). This shows that multiplying all equations by a non-zero constant doesn’t change the solution to the system.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-3",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-3",
    "title": "Question-3",
    "section": "Statement-3",
    "text": "Statement-3\nLet the solution to \\(Ax=b\\) be \\(x^{*}\\). Then, \\(Ax^{*}=b\\).\n\\[\n\\begin{aligned}\nAx^{*} & =b\\\\\ncAx^{*} & =cb\\\\\ncA\\left(\\frac{x^{*}}{c}\\right) & =b\n\\end{aligned}\n\\]\nWe see that \\(\\frac{x^{*}}{c}\\) is a solution to the system \\(cAx=b\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-4",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-3.html#statement-4",
    "title": "Question-3",
    "section": "Statement-4",
    "text": "Statement-4\nThis is just a special case of the previous statement.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-9.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-9.html",
    "title": "Question-9",
    "section": "",
    "text": "Consider a system of equations:\n\\[\n\\begin{aligned}\nx_{1}-3x_{2} & =4\\\\\n3x_{1}+kx_{2} & =-12\n\\end{aligned}\n\\]\nwhere \\(k\\in\\mathbb{R}\\). For what value of \\(k\\) does this system have:\n\na unique solution\ninfinitely many solutions\nno solution\n\n\nFor this system to have a unique solution, the slopes of the two equations should be different:\n\\[\n\\cfrac{1}{3}\\neq\\cfrac{-3}{k}\\implies k\\neq-9\n\\]\nFor the system to have infinitely many solutions, the two lines should be identical:\n\\[\n\\cfrac{1}{3}=\\cfrac{-3}{k}=\\cfrac{-1}{3}\n\\]\nThis is not possible, so the system will never have infinitely solutions. For no solution, we need the slopes to be identical and the intercepts to be different:\n\\[\n\\cfrac{1}{3}=\\cfrac{-3}{k}\\neq\\cfrac{-1}{3}\n\\]\nWe have \\(k=-9\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-9"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-7.html",
    "href": "courses/Maths-2/AQ/Week-1/1.3/AQ-1.3-7.html",
    "title": "Question-7",
    "section": "",
    "text": "Let \\(x_{1}\\) and \\(x_{2}\\) be solutions of the system \\(Ax=b\\). Consider the vectors \\(x_{1}+x_{2}\\) and \\(x_{1}-x_{2}\\). These two vectors are the solutions to which system?\n\nSince \\(x_{1}\\) and \\(x_{2}\\) are solutions to \\(Ax=b\\), \\(Ax_{1}=Ax_{2}=b\\). We can now perform two operations. The first is adding:\n\\[\n\\begin{aligned}\nAx_{1}+Ax_{2} & =2b\\\\\nA(x_{1}+x_{2}) & =2b\n\\end{aligned}\n\\]\nThus, \\(x_{1}+x_{2}\\) is a solution to the system \\(Ax=2b\\). Next, subtracting:\n\\[\n\\begin{aligned}\nAx_{1}-Ax_{2} & =0\\\\\nA(x_{1}-x_{2}) & =0\n\\end{aligned}\n\\]\nThus, \\(x_{1}-x_{2}\\) is a solution to the system \\(Ax=0\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.3",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-3.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-3.html",
    "title": "Question-3",
    "section": "",
    "text": "Let \\(A\\) be a \\(2\\times2\\) matrix, which is given as \\(\\begin{bmatrix}a_{11} & a_{12}\\\\\na_{21} & a_{22}\n\\end{bmatrix}\\). Consider the following matrices:\n\\[\n\\begin{aligned}\nB & =\\begin{bmatrix}a_{11}-a_{21} & a_{12}-a_{22}\\\\\na_{21} & a_{22}\n\\end{bmatrix},C=\\begin{bmatrix}a_{11}-a_{12} & a_{12}\\\\\na_{21}-a_{22} & a_{22}\n\\end{bmatrix}\\\\\n\\\\D & =\\begin{bmatrix}a_{11}+a_{21} & a_{12}-a_{22}\\\\\na_{21} & a_{22}\n\\end{bmatrix},E=\\begin{bmatrix}a_{11}-a_{21} & a_{12}+a_{22}\\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\end{aligned}\n\\]\nSelect all matrices that have the same determinant as that of matrix \\(A\\).\n\n\n\\(B\\) is obtained by performing the operation \\(R_{1}\\rightarrow R_{1}-R_{2}\\) on \\(A\\).\n\\(C\\) is obtained by performing the operation \\(C_{1}\\rightarrow C_{1}-C_{2}\\) on \\(A\\).\n\nWe therefore have \\(|B|=|C|=|A|\\). There is no relationship between \\(|D|\\), \\(|E|\\) and \\(|A|\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-8.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Find out the determinant of the following matrix:\n\\[\n\\begin{bmatrix}1 & a & bc\\\\\n1 & b & ca\\\\\n1 & c & ab\n\\end{bmatrix}\n\\]\n\n\\(R_{2}\\rightarrow R_{2}-R_{1}\\)\n\\[\n\\begin{bmatrix}1 & a & bc\\\\\n0 & b-a & c(a-b)\\\\\n1 & c & ab\n\\end{bmatrix}\n\\]\n\\(R_{3}\\rightarrow R_{3}-R_{1}\\)\n\\[\n\\begin{bmatrix}1 & a & bc\\\\\n0 & b-a & c(a-b)\\\\\n0 & c-a & b(a-c)\n\\end{bmatrix}\n\\]\nBoth these operations leave the determinant unchanged. We can now expand the determinant along the first column:\n\\[\n\\begin{aligned}\n\\begin{vmatrix}1 & a & bc\\\\\n1 & b & ca\\\\\n1 & c & ab\n\\end{vmatrix} & =b(b-a)(a-c)-c(c-a)(a-b)\\\\\n\\\\ & =(a-b)(c-a)(b-c)\\\\\n\\\\ & =(a-b)(b-c)(c-a)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-10.html",
    "href": "courses/Maths-2/AQ/Week-1/1.4/AQ-1.4-10.html",
    "title": "Question-10",
    "section": "",
    "text": "Find the determinant of the matrix given below:\n\\[\n\\begin{bmatrix}a & b & c\\\\\nb & c & a\\\\\nc & a & b\n\\end{bmatrix}\n\\]\n\n\\[\n\\begin{aligned}\n\\begin{vmatrix}a & b & c\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix} & =\\begin{vmatrix}a+b+c & a+b+c & a+b+c\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix}\\\\\n\\\\ & =(a+b+c)\\begin{vmatrix}1 & 1 & 1\\\\\nb & c & a\\\\\nc & a & b\n\\end{vmatrix}\\\\\n\\\\ & =(a+b+c)\\begin{vmatrix}1 & 0 & 0\\\\\nb & c-b & a-b\\\\\nc & a-c & b-c\n\\end{vmatrix}\\\\\n\\\\ & =(a+b+c)[-(b-c)^{2}-(a-b)(a-c)]\\\\\n\\\\ & =(a+b+c)[-b^{2}-c^{2}+2bc-a^{2}+ac+ab-bc]\\\\\n\\\\ & =(a+b+c)[ab+bc+ca-a^{2}-b^{2}-c^{2}]\n\\end{aligned}\n\\]\nThese are the operations:\n\nFirst we start with \\(R_{1}\\rightarrow R_{1}+R_{2}+R_{3}\\).\nThis produces a common factor \\((a+b+c)\\) in the first row which we are moving out.\nWe now move to the columns. We perform \\(C_{2}\\rightarrow C_{2}-C_{1}\\) followed by \\(C_{3}\\rightarrow C_{3}-C_{1}\\).\nWe now have enough number of zeros and we expand the determinant along the first row.\nThe rest is just basic algebra.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.4",
      "Question-10"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-6.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-6.html",
    "title": "Question-6",
    "section": "",
    "text": "Let \\(A,B,C\\) be three matrices of order \\(3\\). Comment on the truth value of the following statements:\n\n\\(\\text{det}(ABC)=\\text{det}(A)\\text{det}(B)\\text{det}(C)\\)\n\\(\\text{det}\\left(A^{3}\\right)=\\left(\\text{det}(A)\\right)^{3}\\)\n\\(\\text{det}(A+B+C)=\\text{det}(A)+\\text{det}(B)+\\text{det}(C)\\)\n\\(\\text{det}\\left(AB^{T}\\right)=\\text{det}(A)\\text{det}(B)\\)\n\n\n(1) We have:\n\\[\n\\begin{aligned}\n\\text{det}(ABC) & =\\text{det}((AB)C)\\\\\n& =\\text{det}(AB)\\text{det}(C)\\\\\n& =\\text{det}(A)\\text{det}(B)\\text{det}(C)\n\\end{aligned}\n\\]\n(2) Using the previous result and setting \\(A=B=C\\) shows that \\(\\text{det}(A^{3})=\\text{det}(A)^{3}\\).\n(3) This is not true. Here is a counter-example:\n\\[\n\\begin{aligned}\nA=B=C= & I\\\\\n\\implies A+B+C & =3I\\\\\n\\implies\\text{det}(A+B+C) & =27\\\\\n\\implies\\text{det}(A)+\\text{det}(B)+\\text{det}(C) & =3\n\\end{aligned}\n\\]\n(4) This result is true.\n\\[\n\\begin{aligned}\n\\text{det}(AB^{T}) & =\\text{det}(A)\\text{det}(B^{T})=\\text{det}(A)\\text{det}(B)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-9.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-9.html",
    "title": "Question-9",
    "section": "",
    "text": "Suppose \\(A=\\begin{bmatrix}4 & -1 & 3\\\\\n2 & 0 & 1\\\\\n3 & -2 & 0\n\\end{bmatrix}\\). Find the determinant of the cofactor matrix of \\(A\\).\n\nWe have the following relation:\n\\[\nA \\cdot \\text{adj}(A)=\\text{adj}(A)\\cdot A=\\text{det}(A)\\cdot I\n\\]\nWe also know that \\(\\text{adj}(A)=C^{T}\\), where \\(C\\) is the cofactor matrix. Therefore, \\(|\\text{adj}(A)|=\\text{det}|C^{T}|=\\text{det}|C|\\). It is enough if we compute the determinant of the adjugate:\n\\[\n\\begin{aligned}\nA\\cdot\\text{adj}(A) & =\\text{det}(A)\\cdot I\\\\\n\\implies\\text{det}(A)\\cdot\\text{det}(\\text{adj}(A)) & =\\text{det}(A)^{3}\\\\\n\\implies\\text{det}(\\text{adj}(A)) & =\\text{det}(A)^{2} & \\text{det}(A)\\neq0\n\\end{aligned}\n\\]\nLet us now compute \\(\\text{det}(A)\\):\n\\[\n\\begin{aligned}\n\\begin{vmatrix}4 & -1 & 3\\\\\n2 & 0 & 1\\\\\n3 & -2 & 0\n\\end{vmatrix} & =4\\times(0+2)+1(0-3)+3(-4-0)\\\\\n& =8-3-12\\\\\n& =-7\n\\end{aligned}\n\\]\nSo the determinant of the cofactor matrix is \\(49\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-9"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-5.html",
    "href": "courses/Maths-2/AQ/Week-1/1.5/AQ-1.5-5.html",
    "title": "Question-5",
    "section": "",
    "text": "Find the determinant of the following matrix:\n\\[\n\\begin{bmatrix}1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1\\\\\n1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nWe can expand the determinant along the last row. The determinant of the entire matrix becomes the determinant of the following sub-matrix:\n\\[\n\\begin{bmatrix}1 & 0 & 1\\\\\n0 & 1 & 0\\\\\n1 & 0 & 0\n\\end{bmatrix}\n\\]\nWe can expand this determinant again along the last row, which gives the result \\(-1\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.5",
      "Question-5"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-3.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-3.html",
    "title": "Question-3",
    "section": "",
    "text": "Convert the following system of equations into matrix-vector form:\n\\[\n\\begin{aligned}\n7x_{1}+10x_{2}+12x_{3} & =36\\\\\n8x_{1}+4x_{2}-9x_{3} & =11\\\\\n4x_{1}-x_{2}+3x_{3} & =10\n\\end{aligned}\n\\]\n\nThis can be represented as the system \\(Ax=b\\), where:\n\\[\nA=\\begin{bmatrix}7 & 10 & 12\\\\\n8 & 4 & -9\\\\\n4 & -1 & 3\n\\end{bmatrix},\\,\\,\\,,x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\\\\\nx_{3}\n\\end{bmatrix},\\,\\,\\,b=\\begin{bmatrix}36\\\\\n11\\\\\n10\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-7.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-7.html",
    "title": "Question-7",
    "section": "",
    "text": "\\(A\\) is a square matrix whose columns are \\(C_{1}\\) and \\(C_{2}\\) and \\(B=\\begin{bmatrix}b_{11} & b_{12}\\\\\nb_{21} & b_{22}\n\\end{bmatrix}\\). What are the first and second columns of \\(AB\\)?\n\nWe have \\(A=\\begin{bmatrix}\\vert & \\vert\\\\\nC_{1} & C_{2}\\\\\n\\vert & \\vert\n\\end{bmatrix}\\). Let us try to understand what happens when we multiply \\(A\\) with a vector \\(x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\n\\end{bmatrix}\\). We have:\n\\[\nAx=x_{1}C_{1}+x_{2}C_{2}\n\\]\nWe see that \\(Ax\\) is a linear combination of the columns of the matrix \\(A\\), with the coefficients coming from the vector.\nThe first column of \\(AB\\) is equal to the product of \\(A\\) and the first column of \\(B\\):\n\\[\nb_{11}C_{1}+b_{21}C_{2}\n\\]\nThe second column of \\(AB\\) is equal to the product of \\(A\\) and the second column of \\(B\\):\n\\[\nb_{12}C_{1}+b_{22}C_{2}\n\\]\nRemark: The product of a matrix and a vector is a linear combination of the columns. This is a very important result and will make an appearance throughout the course.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html",
    "title": "Question-6",
    "section": "",
    "text": "\\(A\\) is a square matrix of order \\(2\\).\nLet \\(A=\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\). We can now compute \\(A^{2}\\):\n\\[\n\\begin{aligned}\nA^{2} & =\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}a & b\\\\\nc & d\n\\end{bmatrix}\\\\\n& =\\begin{bmatrix}a^{2}+bc & ab+bd\\\\\nac+cd & bc+d^{2}\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#a20",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#a20",
    "title": "Question-6",
    "section": "\\(A^{2}=0\\)",
    "text": "\\(A^{2}=0\\)\nIf \\(A^{2}=0\\), then:\n\\[\n\\begin{aligned}\na^{2}+bc & =0\\\\\nb(a+d) & =0\\\\\nc(a+d) & =0\\\\\nd^{2}+bc & =0\n\\end{aligned}\n\\]\nOne solution is to have \\(a=b=c=d=0\\). But this is not the only one. The presence of \\(a+d\\) in two equations suggests that it might be useful to set \\(a+d=0\\). We can set \\(a=1,d=-1\\). Since \\(bc\\) appears in two equations, we can set \\(b=1,c=-1\\). We have \\(A=\\begin{bmatrix}1 & 1\\\\\n-1 & -1\n\\end{bmatrix}.\\) Verify that \\(A^{2}=0\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#a2i",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#a2i",
    "title": "Question-6",
    "section": "\\(A^{2}=I\\)",
    "text": "\\(A^{2}=I\\)\nMoving to \\(A^{2}=I\\), we have to solve the following system:\n\\[\n\\begin{aligned}\na^{2}+bc & =1\\\\\nb(a+d) & =0\\\\\nc(a+d) & =0\\\\\nd^{2}+bc & =1\n\\end{aligned}\n\\]\nWe can set \\(a=1,d=-1\\) and \\(b=d=0\\). This results in \\(A=\\begin{bmatrix}1 & 0\\\\\n0 & -1\n\\end{bmatrix}\\). Verify that \\(A^{2}=I\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#insights",
    "href": "courses/Maths-2/AQ/Week-1/1.2/AQ-1.2-6.html#insights",
    "title": "Question-6",
    "section": "Insights",
    "text": "Insights\nWe can therefore conclude that \\(A^{2}=0\\) admits solutions other than \\(A=0\\) and \\(A^{2}=I\\) admits solutions other than \\(A=\\pm I\\). This question is insightful for the following reason. Consider the corresponding equations in \\(\\mathbb{R}\\):\n\\[\nx^{2}=0\\text{ and }x^{2}=1\n\\]\n\\(x=0\\) is the only solution to the first equation and \\(x=\\pm1\\) are the only two solutions to the second. This shows the difference between simple algebraic equations in one variable and their matrix counterparts.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.2",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-5.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-5.html",
    "title": "Question-5",
    "section": "",
    "text": "The marks obtained by four students in quiz-1, quiz-2 and the end-term exam are given below:\n\n\n\nNames\nQuiz-1\nQuiz-2\nEnd-term\n\n\n\n\nA\n50\n60\n60\n\n\nB\n70\n80\n90\n\n\nC\n80\n90\n100\n\n\nD\n75\n80\n85\n\n\n\nAfter quiz-2, the students are given bonus marks. A, B, C and D are given the bonus marks \\(5,6,1\\) and \\(4\\) respectively.\n\nRepresent the bonus marks as a vector. What kind of a vector would this be?\nCompute the final marks of the students after adding the bonus marks. What is the mathematical operation being performed here?\n\n\nThe current marks of students in quiz-2 is represented by the column vector \\(\\begin{bmatrix}60\\\\\n80\\\\\n90\\\\\n80\n\\end{bmatrix}\\). The bonus marks can also be represented as this column vector: \\(\\begin{bmatrix}5\\\\\n6\\\\\n1\\\\\n4\n\\end{bmatrix}\\). Adding these two vectors will give us the final quiz-2 marks:\n\\[\n\\begin{bmatrix}60\\\\\n80\\\\\n90\\\\\n80\n\\end{bmatrix}+\\begin{bmatrix}5\\\\\n6\\\\\n1\\\\\n4\n\\end{bmatrix}=\\begin{bmatrix}65\\\\\n86\\\\\n91\\\\\n84\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-5"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-1.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-1.html",
    "title": "Question-1",
    "section": "",
    "text": "If \\(x=(1,2)\\) and \\(y=(2,3)\\) are two vectors, compute the following vectors\n\n\\(x+y\\)\n\\(x-y\\)\n\\(2x\\)\n\\(3y\\)\n\n\nAddition of vectors happens component wise. We have:\n\\[\n\\begin{aligned}\nx+y & =(1,2)+(2,3)=(3,5)\\\\\nx-y & =(1,2)-(2,3)=(-1,-1)\n\\end{aligned}\n\\]\nMultiplying a vector by a scalar is done by multiplying each component of the vector by the scalar.\n\\[\n\\begin{aligned}\n2x & =2(1,2)=(2,4)\\\\\n3y & =3(2,3)=(6,9)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-1"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-3.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-3.html",
    "title": "Question-3",
    "section": "",
    "text": "The marks obtained by four students in quiz-1, quiz-2 and the end-term exam are given below:\n\n\n\nNames\nQuiz-1\nQuiz-2\nEnd-term\n\n\n\n\nA\n50\n60\n60\n\n\nB\n70\n80\n90\n\n\nC\n80\n90\n100\n\n\nD\n75\n80\n85\n\n\n\nWhat is the mathematical quantity associated with each of the following practical quantities:\n\nThe quiz-1 marks of all students\nAll the marks associated with student B.\n\n\nThe quiz-1 marks of all students represents a column vector. This is usually denoted as:\n\\[\n\\begin{bmatrix}50\\\\\n70\\\\\n80\\\\\n75\n\\end{bmatrix}\n\\]\nThis is the second column in the table. Visually, a column vector is represented vertically. This column vector has \\(4\\) components. Each component corresponds to the quiz-1 marks of one of the students.\nThe marks associated with student B represents a row vector. This is usually denoted as:\n\\[\n\\begin{bmatrix}70 & 80 & 90\\end{bmatrix}\n\\]\nThis is the third column in the table. Visually, a row vector is represented horizontally. This row vector has \\(3\\) components. Each component corresponds to the marks scored by B in an exam.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-4.html",
    "href": "courses/Maths-2/AQ/Week-1/1.1/AQ-1.1-4.html",
    "title": "Question-4",
    "section": "",
    "text": "The marks obtained by four students in quiz-1, quiz-2 and the end-term exam are given below:\n\n\n\nNames\nQuiz-1\nQuiz-2\nEnd-term\n\n\n\n\nA\n50\n60\n60\n\n\nB\n70\n80\n90\n\n\nC\n80\n90\n100\n\n\nD\n75\n80\n85\n\n\n\nThe marks of student A are halved for each exam. Which mathematical operation does this correspond to on the columns and rows of the above table?\n\nThe marks of student A represents the row vector \\(\\begin{bmatrix}50 & 60 & 60\\end{bmatrix}\\). Halving the marks of A in each exam corresponds to scalar multiplication of the row vector by \\(0.5\\):\n\\[\n0.5\\cdot\\begin{bmatrix}50 & 60 & 60\\end{bmatrix}=\\begin{bmatrix}25 & 30 & 30\\end{bmatrix}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 1",
      "1.1",
      "Question-4"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-8.html",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-8.html",
    "title": "Question-8",
    "section": "",
    "text": "Find the adjugate of a general \\(3\\times3\\) diagonal matrix, say \\(D\\). If \\(D=\\text{adj}(D)\\), what can you say about \\(D\\)?\n\nLet \\(D=\\begin{bmatrix}a & 0 & 0\\\\\n0 & b & 0\\\\\n0 & 0 & c\n\\end{bmatrix}\\). The cofactor matrix is given by:\n\\[\n\\begin{aligned}\nC & =\\begin{bmatrix}bc & 0 & 0\\\\\n0 & ca & 0\\\\\n0 & 0 & ab\n\\end{bmatrix}\n\\end{aligned}\n\\]\nWe see that the cofactor matrix is also diagonal. The adjugate is equal to the transpose of the cofactor matrix. Since \\(C\\) is diagonal, \\(\\text{adj}(D)=C\\).\nIf \\(D=\\text{adj}(D)\\), we have:\n\\[\n\\begin{aligned}\na & =bc\\\\\nb & =ca\\\\\nc & =ab\n\\end{aligned}\n\\]\nIf any one of \\(a,b,c\\) is zero, then all three are zero. So we can safely assume that \\(a,b,c\\neq0\\). Plugging the first equation into the second:\n\\[\n\\begin{aligned}\nb & =ca\\\\\n& =bc^{2}\\\\\nb(c^{2}-1) & =0\\\\\nc & =\\pm1\n\\end{aligned}\n\\]\nBy a similar argument, we get \\(a=\\pm1\\) and \\(b=\\pm1\\). We get the following solutions for \\((a,b,c)\\):\n\\[\n\\begin{aligned}\n(1,1,1)\\\\\n(1,-1,-1),(-1,1,-1),(-1,-1,1)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-8"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-4.html",
    "href": "courses/Maths-2/AQ/Week-2/2.3/AQ-2.3-4.html",
    "title": "Question-4",
    "section": "",
    "text": "If \\(A\\) is invertible, does the system \\(\\text{adj}(A)x=b\\) have a solution? If yes, find the solution. Is it unique?\n\nWe have:\n\\[\nA\\cdot\\text{adj}(A)=\\text{det}(A)\\cdot I\n\\]\nSince \\(A\\) is invertible, \\(A^{-1}\\) exists. We can pre-multiply both sides of the above equation by \\(A^{-1}\\) to get:\n\\[\n\\begin{aligned}\nA^{-1}A\\cdot\\text{adj}(A) & =\\text{det}(A)A^{-1}\\\\\n\\implies\\text{adj}(A) & =\\text{det}(A)A^{-1}\n\\end{aligned}\n\\]\nNotice that \\(\\text{adj}(A)\\) is just a non-zero multiple of \\(A^{-1}\\). So \\(\\text{adj}(A)\\) is also invertible. Its inverse will be \\(\\cfrac{A}{\\text{det}(A)}\\). Now:\n\\[\n\\begin{aligned}\n\\text{adj}(A)x & =b\\\\\n\\text{det}(A)A^{-1}x & =b\\\\\nx & =\\cfrac{Ab}{\\text{det}(A)}\n\\end{aligned}\n\\]\nTherefore, \\(\\cfrac{Ab}{\\text{det}(A)}\\) is the unique solution to the system \\(\\text{adj}(A)x=b\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.3",
      "Question-4"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Identify which of the following are in row echelon form and which are in reduced row echelon form. Also identify the pivots along the way.\n\\[\n\\begin{aligned}\nA & =\\begin{bmatrix}0 & 1 & 0\\\\\n1 & 0 & 1\\\\\n0 & 1 & 0\n\\end{bmatrix} & B & =\\begin{bmatrix}-1 & 2 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 0\n\\end{bmatrix} & C & =\\begin{bmatrix}1 & 2 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 0\n\\end{bmatrix}\\\\\n\\\\D & =\\begin{bmatrix}0 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 0\n\\end{bmatrix} & E & =\\begin{bmatrix}0 & 1 & 3\\\\\n0 & 1 & 1\\\\\n0 & 0 & 1\n\\end{bmatrix} & F & =\\begin{bmatrix}1 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\\\G & =\\begin{bmatrix}0 & 1 & 3 & -1\\\\\n0 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix} & H & =\\begin{bmatrix}0 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\n\\end{bmatrix} & I & =\\begin{bmatrix}1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\\\J & =\\begin{bmatrix}0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 0\\\\\n1 & 0 & 0\n\\end{bmatrix} & K & =\\begin{bmatrix}1 & 0 & 0 & -1\\\\\n0 & 1 & 1 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix} & L & =\\begin{bmatrix}1 & 0 & -1\\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#pivot",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#pivot",
    "title": "Question-2",
    "section": "Pivot",
    "text": "Pivot\nThe pivot is the first non-zero entry in a row. As an example, the numbers marked in bold are the pivots:\n\\[\n\\begin{bmatrix}\\boldsymbol{1} & 0 & -1 & 2\\\\\n0 & \\boldsymbol{2} & 1 & 0\\\\\n0 & 0 & 0 & \\boldsymbol{-1}\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nEach non-zero row has a pivot. A zero row doesn’t have a pivot. The pivot is also called the leading entry.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#ref",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#ref",
    "title": "Question-2",
    "section": "REF",
    "text": "REF\nFor a matrix to be in row echelon form, the following conditions have to be satisfied:\n\nThe pivot in any row should be to the right of the pivot in the previous row.\nThe pivot is \\(1\\). [this condition is not binding according to some authors, but it is binding for this course]\nAll zero rows should come at the end.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#rref",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#rref",
    "title": "Question-2",
    "section": "RREF",
    "text": "RREF\nFor a matrix to be in reduced row echelon form, the following conditions have to be satisfied. A column that contains a pivot is called a pivot column:\n\nThe matrix should be in row echelon form.\nThe pivot should be the only non-zero entry in a pivot column.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#solutions",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-2.html#solutions",
    "title": "Question-2",
    "section": "Solutions",
    "text": "Solutions\n\\[\nA=\\begin{bmatrix}0 & \\boldsymbol{1} & 0\\\\\n\\boldsymbol{1} & 0 & 1\\\\\n0 & \\boldsymbol{1} & 0\n\\end{bmatrix}\n\\]\n\\(A\\) is not in REF. The pivot in the second row is to the left of the pivot in the first row.\n\\[\nB=\\begin{bmatrix}\\boldsymbol{-1} & 2 & 0\\\\\n0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\n\\(B\\) is not in REF. The pivot in the first row is \\(-1\\).\n\\[\nC=\\begin{bmatrix}\\boldsymbol{1} & 2 & 0\\\\\n0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0\n\\end{bmatrix}\\] \\(C\\) is in RREF. The first and last columns are pivot columns.\n\\[\nD=\\begin{bmatrix}0 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\n\\(D\\) is in RREF. It doesn’t have any pivots though.\n\\[\nE=\\begin{bmatrix}0 & \\boldsymbol{1} & 3\\\\\n0 & \\boldsymbol{1} & 1\\\\\n0 & 0 & \\boldsymbol{1}\n\\end{bmatrix}\\] \\(E\\) is not in REF. The pivot in the second row is below the pivot in the first row.\n\\[\nF=\\begin{bmatrix}\\boldsymbol{1} & 1 & 0\\\\\n0 & \\boldsymbol{1} & 0\\\\\n0 & 0 & \\boldsymbol{1}\n\\end{bmatrix}\n\\]\n\\(F\\) is in REF. It is not in RREF as the second pivot column has two non-zero entries.\n\\[\nG=\\begin{bmatrix}0 & \\boldsymbol{1} & 3 & -1\\\\\n0 & 0 & \\boldsymbol{1} & 1\\\\\n0 & 0 & 0 & \\boldsymbol{1}\n\\end{bmatrix}\n\\]\n\\(G\\) is in REF. It is not in RREF since the third and fourth pivot columns have other non-zero entries.\n\\[\nH=\\begin{bmatrix}0 & 0 & 0\\\\\n\\boldsymbol{1} & 0 & 0\\\\\n0 & \\boldsymbol{1} & 0\\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\n\\(H\\) is not in REF. The first row is a zero row and for a matrix to be in REF, all zero rows should come at the end.\n\\[\nI=\\begin{bmatrix}\\boldsymbol{1} & 0 & 0\\\\\n0 & \\boldsymbol{1} & 0\\\\\n0 & 0 & \\boldsymbol{1}\n\\end{bmatrix}\\] \\(I\\) is in RREF.\n\\[\nJ=\\begin{bmatrix}0 & \\boldsymbol{1} & 0\\\\\n0 & 0 & \\boldsymbol{1}\\\\\n0 & 0 & 0\\\\\n\\boldsymbol{1} & 0 & 0\n\\end{bmatrix}\n\\]\n\\(J\\) is not in REF. There is a zero row above a non-zero row.\n\\[\nK=\\begin{bmatrix}\\boldsymbol{1} & 0 & 0 & -1\\\\\n0 & \\boldsymbol{1} & 1 & 0\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\\(K\\) is in RREF. Do not be misled by the \\(-1\\) that appears at the end of the first row or the \\(1\\) that appears after the pivot in the second row.\n\\[\nL=\\begin{bmatrix}\\boldsymbol{1} & 0 & -1\\\\\n0 & \\boldsymbol{1} & 1\n\\end{bmatrix}\n\\]\n\\(L\\) is in RREF.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-6.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-6.html",
    "title": "Question-6",
    "section": "",
    "text": "Suppose a system of linear equations consists of only one equation and four variables:\n\\[\nx_{1}+x_{2}+x_{3}+x_{4}=a\n\\]\nwhere \\(a\\) is a constant. Find out the number of independent variables and find all possible solutions to this system.\n\nWe can form the matrix corresponding to this:\n\\[\n\\begin{bmatrix}\\boldsymbol{1} & 1 & 1 & 1\\end{bmatrix}\n\\]\nThe first column is a pivot column. Hence, \\(x_{1}\\) is a dependent variable. \\(x_{2},x_{3},x_{4}\\) are independent variables. So this equation has three independent variables. To solve the system, we give arbitrary values to the independent variables and then solve for the dependent variable:\n\\[\n\\begin{aligned}\nx_{2} & =t_{2}\\\\\nx_{3} & =t_{3}\\\\\nx_{4} & =t_{4}\\\\\n\\implies x_{1} & =a-(t_{2}+t_{3}+t_{4})\n\\end{aligned}\n\\]\nTherefore, the set of all solutions to this system can be represented by this set:\n\\[\nS=\\left\\{ \\left(a-(t_{2}+t_{3}+t_{4}),t_{2},t_{3},t_{4}\\right)\\,:\\,t_{2},t_{3},t_{4}\\in\\mathbb{R}\\right\\}\n\\]",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-6"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-4.html",
    "href": "courses/Maths-2/AQ/Week-2/2.4/AQ-2.4-4.html",
    "title": "Question-4",
    "section": "",
    "text": "Express the following system in matrix-vector form and solve it:\n\\[\n\\begin{aligned}\n0x_{1}+x_{2}+0x_{3}+0x_{4} & =1\\\\\n0x_{1}+0x_{2}+0x_{3}+0x_{4} & =1\\\\\nx_{1}+x_{2}+0x_{3}+0x_{4} & =1\\\\\n0x_{1}+0x_{2}+x_{3}+x_{4} & =1\n\\end{aligned}\n\\]\n\nWe have:\n\\[\nA=\\begin{bmatrix}0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 1\n\\end{bmatrix},\\,\\,\\,\\,x=\\begin{bmatrix}x_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\n\\end{bmatrix},\\,\\,\\,\\,b=\\begin{bmatrix}1\\\\\n1\\\\\n1\\\\\n1\n\\end{bmatrix}\n\\]\nWe note that the second row is a zero row in \\(A\\), but the corresponding component in the vector \\(b\\) is non-zero. Hence, this system doesn’t have any solution.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.4",
      "Question-4"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-7.html",
    "href": "courses/Maths-2/AQ/Week-2/2.5/AQ-2.5-7.html",
    "title": "Question-7",
    "section": "",
    "text": "Consider the three elementary row operations:\nType-1: Interchanging two rows.\nType-2: Multiplying a row by a non-zero constant.\nType-3: Adding a scalar multiple of a row to another row.\nFor each statement below, prove it if it is correct and provide a counterexample if it isn’t.\n\nIf a matrix \\(A\\) can be obtained from \\(B\\) by a finite number of row operations, then \\(B\\) can be obtained from \\(A\\) by a finite number of row operations.\nThe reduced row echelon form of a matrix cannot be the identity matrix.\nAn upper triangular matrix with all diagonal elements equal to \\(1\\) is in row echelon form.\nThe identity matrix is in reduced row echelon form.\nThe reduced row echelon form of a scalar matrix (other than identity) can be obtained by applying only operations of type 1.\nThe reduced row echelon form of a diagonal matrix (other than identity) can be obtained by applying only operations of type-2.\n\n\n(1) All row operations are reversible. Let us call the matrix \\(A_{1}\\) before the operation and let \\(A_{2}\\) be the matrix after performing the operation.\nType-1: If \\(R_{1}\\) and \\(R_{2}\\) are interchanged in \\(A_{1}\\), we can perform this operation on \\(A_{2}\\) to get back the original matrix \\(A_{1}\\).\nType-2: If \\(R_{1}\\) of \\(A_{1}\\) scaled by a non-zero constant \\(c\\), we can scale \\(R_{1}\\) of \\(A_{2}\\) by \\(\\frac{1}{c}\\) to get back the original matrix \\(A_{1}\\).\nType-3: If \\(R_{1}\\) of \\(A_{1}\\) is replaced by \\(R_{1}+R_{2}\\), we can replace \\(R_{1}\\) of \\(A_{2}\\) by \\(R_{1}-R_{2}\\) to get back the original matrix \\(A_{1}\\).\n(2) This is incorrect. The RREF of a matrix can be the identity matrix. The identity matrix is itself a trivial example. Every invertible matrix has the identity matrix as its RREF.\n(3) This is true. As a concrete example, consider a \\(3\\times3\\) matrix. All the entries not filled below can take arbitrary values:\n\\[\n\\begin{bmatrix}1\\\\\n0 & 1\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n(4) Yes, the identity matrix is indeed in reduced row echelon form.\n(5) A non-zero scalar matrix is of the form given below (\\(c\\neq0\\)):\n\\[\ncI=\\begin{bmatrix}c & 0 & 0\\\\\n0 & c & 0\\\\\n0 & 0 & c\n\\end{bmatrix}\n\\]\nTo get its RREF, we need type-2 operation of scaling each row by the constant \\(\\frac{1}{c}\\). Type-1 operation is going to be of no use here.\n(6) To reduce a diagonal matrix to its RREF, we may need both type-1 and type-2 operations. This is especially the case if there are any zero entries on the diagonal. For instance:\n\\[\n\\begin{bmatrix}1 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nWe need to perform \\(R_{2}\\leftrightarrow R_{3}\\) to convert the above matrix into its RREF.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.5",
      "Question-7"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-3.html",
    "href": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-3.html",
    "title": "Question-3",
    "section": "",
    "text": "Show that \\(\\text{det}(AB)\\) is zero if and only if at least one of \\(\\text{det}(A)\\) and \\(\\text{det}(B)\\) is zero.\n\nIf \\(\\text{det}(AB)=0\\), then \\(\\text{det}(A)\\text{det}(B)=0\\). If the product of two numbers is zero, at least one of them has to be zero. Going the other way, if \\(\\text{det}(A)\\) or \\(\\text{det}(B)\\) is zero, \\(\\text{det}(AB)\\) is zero.",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.1",
      "Question-3"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-2.html",
    "href": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-2.html",
    "title": "Question-2",
    "section": "",
    "text": "Suppose there exist three square matrices \\(A,D,P\\) of order \\(3\\) such that \\(D=PAP^{-1}\\) and \\(D\\) is diagonal. Find a relationship between \\(|D|\\) and \\(|A|\\). If \\(D=I\\), is it necessary that \\(A\\) is also equal to \\(I\\)?\n\nWe have:\n\\[\n\\begin{aligned}\n|D| & =\\left|PAP^{-1}\\right|\\\\\n& =|P|\\cdot|A|\\cdot\\left|P^{-1}\\right|\\\\\n& =|P|\\cdot|A|\\cdot\\cfrac{1}{|P|}\\\\\n& =|A|\n\\end{aligned}\n\\]\nThus, the determinants of \\(A\\) and \\(D\\) are equal. We now turn to the second part of the question. If \\(D=I\\), then:\n\\[\n\\begin{aligned}\nI & =PAP^{-1}\\\\\nP^{-1}IP & =A\\\\\nP^{-1}P & =A\\\\\nI & =A\n\\end{aligned}\n\\]\nThus, if \\(D=I\\), then \\(A=I\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.1",
      "Question-2"
    ]
  },
  {
    "objectID": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-10.html",
    "href": "courses/Maths-2/AQ/Week-2/2.1/AQ-2.1-10.html",
    "title": "Question-10",
    "section": "",
    "text": "If the diagonal entries of a \\(3\\times3\\) lower triangular matrix \\(A\\) are \\(1,2,3\\), find the sum of the roots of the equation \\(\\text{det}(A-xI)=0\\), where \\(I\\) is the identity matrix.\n\n\\(A-xI\\) is also lower triangular. Since the determinant of a lower triangular matrix is the product of the diagonal entries, we have:\n\\[\n\\begin{aligned}\n\\text{det}(A-xI) & =(1-x)(2-x)(3-x)\n\\end{aligned}\n\\]\nThe roots of \\(\\text{det}(A-xI)=0\\) are \\(1,2,3\\). So the sum of the roots is equal to \\(6\\).",
    "crumbs": [
      "Courses",
      "Maths-2",
      "Activity Questions",
      "Week 2",
      "2.1",
      "Question-10"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html",
    "href": "courses/MLF/notes/Part-1/Projections.html",
    "title": "Projections",
    "section": "",
    "text": "Question\n\n\n\nGeometrically, what is the relationship between the approximation \\(\\mathbf{X}\\widehat{\\mathbf{w}}\\) and the vector \\(\\mathbf{y}\\)?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#setting",
    "href": "courses/MLF/notes/Part-1/Projections.html#setting",
    "title": "Projections",
    "section": "Setting",
    "text": "Setting\nRecall that we are trying to solve the following:\n\\[\n\\begin{equation*}\n\\mathbf{Xw} \\approx \\mathbf{y}\n\\end{equation*}\n\\]\nThe best approximation is given by the solution to this equation:\n\\[\n\\begin{equation*}\n(\\mathbf{X}^{T}\\mathbf{X} )\\widehat{\\mathbf{w}} =\\mathbf{X}^{T}\\mathbf{y}\n\\end{equation*}\n\\]\n\\(\\mathbf{X}\\widehat{\\mathbf{w}}\\) is therefore the best approximation for \\(\\mathbf{y}\\). Now, how are these two vectors related? Specifically, note that both vectors reside in \\(\\mathbb{R}^{n}\\). To keep things simple, let us return to our favorite haunt, \\(\\mathbb{R}^{2}\\), with the following configuration:\n\\[\n\\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\n2 & 6\\\\\n1 & 3\n\\end{bmatrix} ,\\ \\mathbf{y} =\\begin{bmatrix}\n3\\\\\n4\n\\end{bmatrix}\n\\end{equation*}\n\\]",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#back-to-column-space",
    "href": "courses/MLF/notes/Part-1/Projections.html#back-to-column-space",
    "title": "Projections",
    "section": "Back to Column space",
    "text": "Back to Column space\nWe look for an approximation only when \\(\\mathbf{y}\\) does not lie in the column space of \\(\\mathbf{X}\\). So, first, we see what the column space is:\n\\[\n\\begin{equation*}\n\\mathcal{R} (\\mathbf{X} )=\\text{span}\\left(\\left\\{\\begin{bmatrix}\n2\\\\\n1\n\\end{bmatrix}\\right\\}\\right)\n\\end{equation*}\n\\]\nThe second column of \\(\\mathbf{X}\\) is just three times the first column. The rank of the matrix is \\(1\\). The column space of \\(\\mathbf{X}\\) is a one-dimensional subspace of \\(\\mathbb{R}^{2}\\) . Geometrically, what does this mean?\n\nThe column space is a line passing through the origin and the point \\((2,1)\\). Clearly, the vector \\(\\mathbf{y}\\) does not lie in the column space of \\(\\mathbf{X}\\). So, we are justified in looking for an approximation.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#projections-2-dimensions",
    "href": "courses/MLF/notes/Part-1/Projections.html#projections-2-dimensions",
    "title": "Projections",
    "section": "Projections: 2-dimensions",
    "text": "Projections: 2-dimensions\nThe key idea to remember is that the approximation is going to lie in the column space of \\(\\mathbf{X}\\). What vector in \\(\\mathcal{R} (\\mathbf{X} )\\) is closest to to \\(\\mathbf{y}\\)? Even before going there, what do we mean by closest? Recall that the Euclidean distance between the two vectors is our measure of distance. In our 2D case, this is nothing but the distance between the point \\(\\mathbf{y}\\) and some point on the line \\(\\mathcal{R} (\\mathbf{X} )\\). The point on the line which is going to have the shortest distance is the projection of \\(\\mathbf{y}\\) onto the line! Why is that the case? Among all line segments from a point to a line, the perpendicular to it is the shortest.\n\nGeometric intuition therefore suggests that \\(\\mathbf{X}\\widehat{\\mathbf{w}} =\\begin{bmatrix}\n4\\\\\n2\n\\end{bmatrix}\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#back-to-normal-equations",
    "href": "courses/MLF/notes/Part-1/Projections.html#back-to-normal-equations",
    "title": "Projections",
    "section": "Back to Normal Equations",
    "text": "Back to Normal Equations\nLet us see if algebra agrees with geometry:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n(\\mathbf{X}^{T}\\mathbf{X} )\\widehat{\\mathbf{w}} & =\\mathbf{X}^{T}\\mathbf{y}\\\\\n& \\\\\n\\begin{bmatrix}\n2 & 1\\\\\n6 & 3\n\\end{bmatrix}\\begin{bmatrix}\n2 & 6\\\\\n1 & 3\n\\end{bmatrix}\\begin{bmatrix}\n\\widehat{w}_{1}\\\\\n\\widehat{w}_{2}\n\\end{bmatrix} & =\\begin{bmatrix}\n2 & 1\\\\\n6 & 3\n\\end{bmatrix}\\begin{bmatrix}\n3\\\\\n4\n\\end{bmatrix}\\\\\n& \\\\\n\\begin{bmatrix}\n5 & 15\\\\\n15 & 45\n\\end{bmatrix}\\begin{bmatrix}\n\\widehat{w}_{1}\\\\\n\\widehat{w}_{2}\n\\end{bmatrix} & =\\begin{bmatrix}\n10\\\\\n30\n\\end{bmatrix}\\\\\n& \\\\\n\\begin{bmatrix}\n1 & 3\\\\\n1 & 3\n\\end{bmatrix}\\begin{bmatrix}\n\\widehat{w}_{1}\\\\\n\\widehat{w}_{2}\n\\end{bmatrix} & =\\begin{bmatrix}\n2\\\\\n2\n\\end{bmatrix}\n\\end{aligned}\n\\end{equation*}\n\\]\nWe see that \\(\\mathbf{X}^{T}\\mathbf{X}\\) is singular. But thankfully, the system is still solvable. One such solution is:\n\\[\n\\begin{equation*}\n\\widehat{\\mathbf{w}} =\\begin{bmatrix}\n-1\\\\\n1\n\\end{bmatrix}\n\\end{equation*}\n\\]\nTherefore, the approximation is:\n\\[\n\\begin{equation*}\n\\mathbf{X}\\widehat{\\mathbf{w}} =\\begin{bmatrix}\n2 & 6\\\\\n1 & 3\n\\end{bmatrix}\\begin{bmatrix}\n-1\\\\\n1\n\\end{bmatrix} =\\begin{bmatrix}\n4\\\\\n2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nAlgebra does agree with geometry!",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#projections-higher-dimensions",
    "href": "courses/MLF/notes/Part-1/Projections.html#projections-higher-dimensions",
    "title": "Projections",
    "section": "Projections: higher dimensions",
    "text": "Projections: higher dimensions\nThe main takeaway from the 2D case is this: the vector closest to \\(\\mathbf{y}\\) in the column space of \\(\\mathbf{X}\\) is its projection onto the column space of \\(\\mathbf{X}\\). This can be extended to any higher dimensional space. First, we note that for a projection, the error vector is orthogonal to the column space of \\(\\mathbf{X}\\):\n\nThe error vector \\(\\mathbf{e}\\) is:\n\\[\n\\begin{equation*}\n\\mathbf{e} =\\mathbf{y} -\\mathbf{X}\\widehat{\\mathbf{w}}\n\\end{equation*}\n\\]\nThis is orthogonal to the column space of \\(\\mathbf{X}\\). This is the same as saying that it is orthogonal to each column of \\(\\mathbf{X}\\). If we let \\(\\mathbf{X}\\) to be \\([\\mathbf{c}_{1} ,\\cdots ,\\mathbf{c}_{d} ]\\). Then for \\(1\\leqslant i\\leqslant d\\):\n\\[\n\\begin{equation*}\n\\mathbf{c}_{i}^{T}\\mathbf{e} =0\n\\end{equation*}\n\\]\nThis can be neatly expressed as:\n\\[\n\\begin{equation*}\n\\mathbf{X}^{T}\\mathbf{e} =\\mathbf{0}\n\\end{equation*}\n\\]\nThis means that the error vector is in the nullspace of \\(\\mathbf{X}^{T}\\). Replacing \\(\\mathbf{e} =\\mathbf{y} -\\mathbf{X}\\widehat{\\mathbf{w}}\\), we get:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\mathbf{X}^{T} (\\mathbf{y} -\\mathbf{X}\\widehat{\\mathbf{w}} ) & =0\\\\\n\\Longrightarrow (\\mathbf{X}^{T}\\mathbf{X} )\\widehat{\\mathbf{w}} =\\mathbf{X}^{T}\\mathbf{y} &\n\\end{aligned}\n\\end{equation*}\n\\]\nThe normal equations again!",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Projections.html#summary",
    "href": "courses/MLF/notes/Part-1/Projections.html#summary",
    "title": "Projections",
    "section": "Summary",
    "text": "Summary\nIf the parameters of the linear model obtained by solving the normal equations is \\(\\widehat{\\mathbf{w}}\\), then the vector \\(X\\widehat{\\mathbf{w}}\\) is the projection of the vector \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Projections"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/XW~y.html",
    "href": "courses/MLF/notes/Part-1/XW~y.html",
    "title": "\\(\\mathbf{Xw} \\approx \\mathbf{y}\\)",
    "section": "",
    "text": "Question\n\n\n\nHow do we solve for \\(\\mathbf{w}\\) in the equation \\(\\mathbf{Xw} \\approx \\mathbf{y}\\)?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} \\approx \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/XW~y.html#approximation",
    "href": "courses/MLF/notes/Part-1/XW~y.html#approximation",
    "title": "\\(\\mathbf{Xw} \\approx \\mathbf{y}\\)",
    "section": "Approximation",
    "text": "Approximation\nSo far we have seen the well behaved case of \\(\\mathbf{Xw} =\\mathbf{y}\\). What if \\(\\mathbf{y}\\) is not in the column space of \\(\\mathbf{X}\\)? Then \\(\\mathbf{Xw} \\neq \\mathbf{y}\\). This is by far the most interesting and relevant scenario from the perspective of the linear regression problem. Recall that we are trying to estimate a weight vector \\(\\mathbf{w}\\), given the labeled dataset \\((\\mathbf{X} ,\\mathbf{y} )\\). The relationship between inputs and outputs is assumed to be linear. But the real world doesn’t behave exactly like we want it to. There are going to be obvious deviations from our ideal assumptions. In other words, our models can never be an entirely accurate representation of reality.\nThis is a typical scenario observed in engineering and the sciences. We don’t abandon the problem because it doesn’t admit an exact solution. Instead, we turn to a powerful weapon in our armory: approximations. Can we find a \\(\\mathbf{\\widehat{w}}\\) such that \\(\\mathbf{X}\\widehat{\\mathbf{w}} \\approx \\mathbf{y}\\)? What do we mean by the symbol \\(\\approx\\) here? We are dealing with two vectors on either side of the symbol. Let us first understand what the symbol means when we have scalars.\nLet us look at the following statements:\n\n\\(1.234\\approx 1\\)\n\\(1.234\\approx 1.2\\)\n\\(1.234\\approx 1.23\\)\n\nThese are three approximations. From experience, we know that the second approximation is better than the first and the third better than the second. If we plot these points on a real line, the goodness of an approximation can be interpreted using the distance between the original point and its approximation: smaller the distance, better the approximation.\nThis very idea is extended to an \\(n\\) dimensional vector space. If \\(\\mathbf{y}\\) and \\(\\mathbf{\\widehat{y}}\\) are two vectors in \\(\\mathbb{R}^{n}\\), then the Euclidean distance between them is a measure of the goodness of the approximation. To avoid taking square roots, we write down the expression for the squared distance:\n\\[\n\\begin{equation*}\n||\\widehat{\\mathbf{y}} -\\mathbf{y} ||^{2} =(\\widehat{y}_{1} -y_{1} )^{2} +\\cdots +(\\widehat{y}_{n} -y_{n} )^{2}\n\\end{equation*}\n\\]\nThe key to solving the problem \\(\\mathbf{Xw} \\approx \\mathbf{y}\\), is to find a vector \\(\\widehat{\\mathbf{w}}\\) such that \\(\\mathbf{X}\\widehat{\\mathbf{w}}\\) is as close to \\(\\mathbf{y}\\) as possible. This can be framed as an optimization problem:\n\\[\n\\begin{equation*}\n\\widehat{\\mathbf{w}} =\\underset{\\mathbf{w}}{\\arg\\min} \\ \\ ||\\mathbf{Xw} -\\mathbf{y} ||^{2}\n\\end{equation*}\n\\]\nIf you are seeing \\(\\arg\\min\\) for the first time, think about it like a function (in the programming sense):\n\nFind the value that minimizes the expression\nReturn this value to the user",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} \\approx \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/XW~y.html#least-squares-problem",
    "href": "courses/MLF/notes/Part-1/XW~y.html#least-squares-problem",
    "title": "\\(\\mathbf{Xw} \\approx \\mathbf{y}\\)",
    "section": "Least Squares Problem",
    "text": "Least Squares Problem\nLet us go ahead and solve the minimization problem.\n\\[\n\\begin{equation*}\n\\underset{\\mathbf{w}}{\\min} \\ \\ \\ ||\\mathbf{Xw} -\\mathbf{y} ||^{2}\n\\end{equation*}\n\\]\nLet us call the expression to be minimized the loss, \\(L(\\mathbf{w})\\), which is also called the objective function of the optimization problem. It can be simplified as follows:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nL(\\mathbf{w}) & =||\\mathbf{Xw} -\\mathbf{y} ||^{2}\\\\\n& \\\\\n& =(\\mathbf{Xw} -\\mathbf{y} )^{T} (\\mathbf{Xw} -\\mathbf{y} )\\\\\n&\n\\end{aligned}\n\\end{equation*}\n\\]\nThis is often called the least squares problem. To see why this name is used, we will look at the loss function from a slightly different perspective. Since \\(\\mathbf{Xw}\\) is an approximation of \\(\\mathbf{y}\\), \\(\\mathbf{e} =\\mathbf{Xw} -\\mathbf{y}\\) can be seen as an error vector. So:\n\\[\n\\begin{equation*}\nL(\\mathbf{w}) =\\mathbf{e}^{T}\\mathbf{e} =e_{1}^{2} +\\cdots +e_{n}^{2}\n\\end{equation*}\n\\]\nFrom a ML standpoint, recall the housing dataset with which we started. \\(e_{i}\\) is the difference between the actual selling price of the \\(i^{th}\\) house and its predicted selling price as given by our linear model. To be more specific, we see that \\(\\displaystyle e_{i} =\\mathbf{w}^{T}\\mathbf{x}_{i} -y_{i}\\). Plugging this into the loss function:\n\\[\n\\begin{equation*}\nL(\\mathbf{w}) =\\sum\\limits _{i=1}^{n}\\left(\\mathbf{w}^{T}\\mathbf{x}_{i} -y_{i}\\right)^{2}\n\\end{equation*}\n\\]\nIn a sense, we are justified in calling \\(\\mathbf{e}\\) the error vector. We are minimizing the sum of the squared errors, hence the term “least squares”. \\(\\widehat{\\mathbf{w}}\\) is that weight vector for the linear model which will give us the best possible fit.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} \\approx \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/XW~y.html#normal-equations",
    "href": "courses/MLF/notes/Part-1/XW~y.html#normal-equations",
    "title": "\\(\\mathbf{Xw} \\approx \\mathbf{y}\\)",
    "section": "Normal Equations",
    "text": "Normal Equations\nWe now follow the usual procedure of finding the minimum of a function. Take the derivatives and set them to zero. Since we are dealing with a multivariable function, we have to consider all the partial derivatives. The vector of such partial derivatives is called the gradient.\n\\[\n\\begin{equation*}\n\\nabla L(\\mathbf{w}) =\\begin{bmatrix}\n\\cfrac{\\partial L(\\mathbf{w})}{\\partial w_{1}}\\\\\n\\vdots \\\\\n\\cfrac{\\partial L(\\mathbf{w})}{\\partial w_{d}}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nLet us now compute the gradient and set it to zero. To simplify the gradient computation, let us go back to that form of the loss function which uses the sum of the squared errors:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nL(\\mathbf{w}) & =\\sum\\limits _{i=1}^{n}\\left(\\mathbf{w}^{T}\\mathbf{x}_{i} -y_{i}\\right)^{2}\n\\end{aligned}\n\\end{equation*}\n\\]\nTaking the gradient with respect to \\(\\displaystyle \\mathbf{w}\\) and using the chain rule we get:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\nabla L(\\mathbf{w}) & =2\\cdot \\sum\\limits _{i=1}^{n}\\left(\\mathbf{w}^{T}\\mathbf{x}_{i} -y_{i}\\right) \\cdot \\mathbf{x}_{i}\\\\\n& \\\\\n& =2\\cdot \\mathbf{X}^{T}(\\mathbf{Xw} -\\mathbf{y})\n\\end{aligned}\n\\end{equation*}\n\\]\nSetting the gradient equal to zero, we get:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\mathbf{X}^{T}(\\mathbf{Xw} -\\mathbf{y}) & =0\n\\end{aligned}\n\\end{equation*}\n\\]\nThis gives us the following equation for \\(\\displaystyle \\mathbf{w}\\):\n\\[\n\\begin{equation*}\n\\mathbf{X}^{T}\\mathbf{Xw} =\\mathbf{X}^{T}\\mathbf{y}\n\\end{equation*}\n\\]\nThis system is called the normal equations. If the matrix \\(\\mathbf{X}^{T}\\mathbf{X}\\) is invertible, then we have the following solution:\n\\[\n\\begin{equation*}\n\\widehat{\\mathbf{w}} =(\\mathbf{X}^{T}\\mathbf{X} )^{-1}\\mathbf{X}^{T}\\mathbf{y}\n\\end{equation*}\n\\]\nWe still don’t know if this corresponds to a minima. Rest assured that this is indeed a minima. But we won’t take up the proof now. The other worrying part is what happens if the matrix \\(\\mathbf{X}^{T}\\mathbf{X}\\) is singular or non-invertible. It turns out that even in this situation, this system is always consistent. The proof of consistency is a bit involved. It has been moved to the appendix on the matrix \\(\\mathbf{X}^T \\mathbf{X}\\).\n\n\n\n\n\n\nRemarks\n\n\n\n\n\nA few more technical points regarding this system which can be safely skipped for now:\n\nThere is the question concerning the uniqueness of the solution. A unique solution is guaranteed if \\(\\mathcal{N}(\\mathbf{X}^T\\mathbf{X}) = \\{\\mathbf{0}\\}\\) which happens when the columns of \\(\\mathbf{X}\\) are linearly independent. Since the columns represent the various features, in practice, this means that the features are all independent of each other. That is, each feature adds some unique information for a data-point and cannot be obtained by combining the other features linearly.\nThere is an interesting particular solution to the normal equations given in terms of a quantity called the pseudoinverse, which exits for all matrices. The pseudoinverse of the matrix \\(\\mathbf{X}\\) is denoted by \\(\\mathbf{X}^{\\dagger}\\). The dimension of \\(\\mathbf{X}^{\\dagger}\\) is \\(d \\times n\\). The “least squares solution” to the system \\(\\mathbf{Xw} = \\mathbf{y}\\) is given by:\n\n\\[\n\\mathbf{X}^{\\dagger} \\mathbf{y}\n\\]\n\nIf \\(\\mathbf{X}\\) has full column rank, then the pseudoinverse becomes \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\\).\nThis solution has the wonderful property that it has the least norm among all solutions to the normal equations.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} \\approx \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/XW~y.html#summary",
    "href": "courses/MLF/notes/Part-1/XW~y.html#summary",
    "title": "\\(\\mathbf{Xw} \\approx \\mathbf{y}\\)",
    "section": "Summary",
    "text": "Summary\nWhen \\(\\mathbf{y}\\) is not in the column space of \\(\\mathbf{X}\\), we look for an approximate solution. One way to specify a good approximation is to find a \\(\\widehat{\\mathbf{w}}\\) that minimizes the following loss function:\n\\[\n\\begin{equation*}\nL(\\mathbf{w}) =(\\mathbf{Xw} -\\mathbf{y} )^{T} (\\mathbf{Xw} -\\mathbf{y} )\n\\end{equation*}\n\\]\nThis \\(\\mathbf{\\widehat{w}}\\) is given by the solution to the normal equations:\n\\[\n\\begin{equation*}\n(\\mathbf{X}^{T}\\mathbf{X} )\\widehat{\\mathbf{w}} =\\mathbf{X}^{T}\\mathbf{y}\n\\end{equation*}\n\\]\nThis system always has a solution. In the next few units, we shall try to understand the least squares problem from the lens of geometry.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "$\\mathbf{Xw} \\approx \\mathbf{y}$"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Introduction.html",
    "href": "courses/MLF/notes/Part-1/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Question\n\n\n\nWhy should we study linear algebra in a course on data science?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Introduction"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Introduction.html#data",
    "href": "courses/MLF/notes/Part-1/Introduction.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\nThe simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it:\n\nlatitude\nlongitude\nage\nnumber of rooms\narea\ndistance from nearest school\n\nThe selling price of a house depends on these features. For example, a house with more number of rooms may have a higher selling price and a house that is very far from the nearest school may have a lower selling price. A natural question that we might want to ask is this:\n\n\n\n\n\n\nQuestion\n\n\n\nGiven the features of a house can we train a machine to predict its selling price?\n\n\nThis is called a regression problem: given a set of features, map it to a real number. In our example problem, the real number which we wish to predict is the selling price.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Introduction"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Introduction.html#vectors",
    "href": "courses/MLF/notes/Part-1/Introduction.html#vectors",
    "title": "Introduction",
    "section": "Vectors",
    "text": "Vectors\nLet us take a concrete example of a single house. Since each house represents one instance of the data we are working with, we will call it a data-point:\n\nA Data-Point\n\n\nName\nValues\n\n\n\n\nlatitude\n12.9\n\n\nlongitude\n80.2\n\n\nage\n3\n\n\nnumber of rooms\n2\n\n\narea\n1000\n\n\ndistance from nearest school\n3\n\n\n\nLatitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs of rupees. But none of these “units” really matter for an ML algorithm: it is going to abstract out the details and look at this as a column of numbers, which is nothing but a vector:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n12.9\\\\\n80.2\\\\\n3\\\\\n2\\\\\n1000\\\\\n3\n\\end{bmatrix}\n\\end{equation*}\n\\]\nThe selling price is not included as an element in the vector as that is usually unknown to us. This unknown quantity which we have to estimate or predict is called the target.\n\n\n\n\n\n\nRemarks\n\n\n\n\nVectors are usually represented as column vectors or \\(d \\times 1\\) matrices.\nAn alternative terminology for features/target is independent/dependent variables. The term independent variables stands for the features and dependent variable for the label. You can think about the dependent variable as a function of the independent variables.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Introduction"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Introduction.html#matrices",
    "href": "courses/MLF/notes/Part-1/Introduction.html#matrices",
    "title": "Introduction",
    "section": "Matrices",
    "text": "Matrices\nWe cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tabular form. For now, we only focus on the features:\n\n\n\n\n\n\n\n\n\n\n\n\n\nhouse number\nlatitude\nlongitude\nage\nrooms\narea\ndistance\nselling price\n\n\n\n\n1\n12.9\n80.2\n3\n2\n1000\n3\n40\n\n\n\\(\\cdots\\)\n\n\n\n\n\n\n\n\n\n50\n14.3\n75.9\n30\n2\n1200\n5\n20\n\n\n\\(\\cdots\\)\n\n\n\n\n\n\n\n\n\n100\n20.8\n90.5\n1\n3\n1500\n2\n35\n\n\n\nThis data for \\(100\\) houses is nothing but a \\(100\\times 6\\) matrix:\n\\[\n\\begin{equation*}\n\\begin{bmatrix}\n12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\\n\\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots \\\\\n14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\\n\\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots  & \\vdots \\\\\n20.8 & 90.5 & 1 & 3 & 1500 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nEach row of this matrix corresponds to one data-point. In general, if a dataset has \\(d\\) features and \\(n\\) data-points, it is represented as a \\(n \\times d\\) or a \\(d \\times n\\) data-matrix or design matrix.\n\n\n\n\n\n\nNote\n\n\n\nIf you find yourself lost when working with matrices, remember that a matrix is a way to represent a collection of data-points (dataset).",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Introduction"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Introduction.html#summary",
    "href": "courses/MLF/notes/Part-1/Introduction.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nData is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra — the study of vectors and matrices — if we wish to understand how ML algorithms work.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Introduction"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html",
    "href": "courses/MLF/notes/Part-1/ML Problem.html",
    "title": "ML Problem",
    "section": "",
    "text": "Question\n\n\n\nWhat does a typical ML problem look like?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html#analogy",
    "href": "courses/MLF/notes/Part-1/ML Problem.html#analogy",
    "title": "ML Problem",
    "section": "Analogy",
    "text": "Analogy\nThink about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers:\n\n\\(103+205=308\\)\n\\(123+409=532\\)\n\\(185+483=668\\)\n\nDuring the instructional hours, the student has access to both the questions and the answers. In the exam, she will not have access to the answers. But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. She needs to have a mental model of addition. In other words, she would have to learn a function from the input (question) to the output (answer).\nThis is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels. A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset. A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html#data-representation",
    "href": "courses/MLF/notes/Part-1/ML Problem.html#data-representation",
    "title": "ML Problem",
    "section": "Data Representation",
    "text": "Data Representation\nWe are given a collection of \\(n\\) data-points and \\(n\\) labels. Each data-point is described by \\(d\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as latitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(d\\). Arranging the \\(n\\) data-points in a matrix, we get a \\(n\\times d\\) data-matrix. Let us call this matrix \\(\\mathbf{X}\\):\n\\[\n\\begin{equation*}\n\\mathbf{X} =\\begin{bmatrix}\nx_{11} & -  & x_{1d}\\\\\n|  & x_{ij} & | \\\\\nx_{n1} & -  & x_{nd}\n\\end{bmatrix}\n\\end{equation*}\n\\]\nEach row of this matrix is the feature vector for one data-point. The element \\(x_{ij}\\) is the \\(j^{th}\\) feature of the \\(i^{th}\\) data-point. The labels can be put together in a vector of size \\(n\\). Let us call this \\(\\mathbf{y}\\):\n\\[\n\\begin{equation*}\n\\mathbf{y} =\\begin{bmatrix}\ny_{1}\\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n\\end{equation*}\n\\]\n\n\n\n\n\n\nRemark\n\n\n\nWe will use the following conventions to represent scalars, vectors and matrices:\n\nScalars are represented by small letters in normal font: \\(\\displaystyle x,y,z\\).\nVectors are represented by small letters in bold-faced font: \\(\\displaystyle \\mathbf{x} ,\\ \\mathbf{y} ,\\ \\mathbf{z}\\).\nMatrices are represented by capital letters in bold-faced font: \\(\\displaystyle \\mathbf{X} ,\\mathbf{Y} ,\\mathbf{Z}\\)",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html#model",
    "href": "courses/MLF/notes/Part-1/ML Problem.html#model",
    "title": "ML Problem",
    "section": "Model",
    "text": "Model\nAs stated earlier, a regression model can be viewed as a function that transforms a data-point into a label. Formally:\n\\[\n\\begin{equation*}\nf:\\mathbb{R}^{d}\\rightarrow \\mathbb{R}\n\\end{equation*}\n\\]\nEach feature-vector is of size \\(d\\). So, the feature-vectors reside in the \\(d\\) dimensional space \\(\\mathbb{R}^{d}\\). The labels are real numbers, so they reside in \\(\\mathbb{R}\\). Mathematically, this is the action of a model on a data-point \\(\\mathbf{x}\\):\n\\[\n\\begin{equation*}\ny=f(\\mathbf{x} )\n\\end{equation*}\n\\]\nPictorially:\n\n\n\n\n\nflowchart LR\n  A[Feature Vector] --&gt; B[Model]\n  B[Model] --&gt; C[Label]\n\n\n\n\n\n\nWhat is so special about a ML model? All models take some input and produce a corresponding output. The key difference is that in a classical programming setting, we are given the input and the function and are asked to find the output. In machine learning we are given both the input and the output using which have to learn a model \\(f\\). The function or the model is the unknown. This is what has to be learnt. This is one reason machine learning is often associated with the phrase “learning from data”.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html#learning",
    "href": "courses/MLF/notes/Part-1/ML Problem.html#learning",
    "title": "ML Problem",
    "section": "Learning",
    "text": "Learning\nML is all about learning from data. But who or what is learning? More importantly, who or what enables the learning? There is a learning algorithm which drives the learning. We can think of the model as the outcome of the learning process. During the learning stage, the dataset is fed as input to a learning algorithm, which in turn outputs a model.\n\n\n\n\n\nflowchart LR\n  A[Labeled Dataset] --&gt; B[Learning Algorithm]\n  B[Learning Algorithm] --&gt; C[Model]\n\n\n\n\n\n\nThere is one important detail that is missing in this diagram. There are several models that we could choose from. Going back to our analogy, there are different ways to understand three digit addition:\n\nrepresenting numbers as counts of physical objects\nrepresenting numbers as abstract entities that can be manipulated\n\nTeachers might choose the first model to help kids understand addition. We all would have come across this example at some point: “If I have two chocolates in my left hand and five in my right hand, how many do I have in total?” As kids grow up, teachers might move to the second model, which is more sophisticated. Something similar happens in ML. We are the teachers for the machines. Our responsibility is to choose a family of models and present it to the machine.\n\n\n\n\n\nflowchart LR\n  A[Labeled Dataset] --&gt; B[Learning Algorithm]\n  D[Model Family] --&gt; B[Learning Algorithm]\n  B[Learning Algorithm] --&gt; C[Model]\n\n\n\n\n\n\nThus there are two inputs to the learning algorithm:\n\nlabeled dataset\nfamily of models\n\nThe task of the algorithm is to explore the space of models and pick the one that best fits the labeled dataset. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in subsequent chapters.\n\n\n\n\n\n\nRemark\n\n\n\nFor some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. This representation is what we call a model.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/ML Problem.html#summary",
    "href": "courses/MLF/notes/Part-1/ML Problem.html#summary",
    "title": "ML Problem",
    "section": "Summary",
    "text": "Summary\nRegression is a classic ML problem that uses labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(\\mathbf{X}\\). The labels are arranged in a label vector called \\(\\mathbf{y}\\). A model is a function that transforms a feature-vector to a label.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "ML Problem"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Best-fit.html",
    "href": "courses/MLF/notes/Part-1/Best-fit.html",
    "title": "Best-Fit Hyperplane",
    "section": "",
    "text": "Question\n\n\n\nWhat does a linear model look like?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Best-Fit Hyperplane"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Best-fit.html#d-problem",
    "href": "courses/MLF/notes/Part-1/Best-fit.html#d-problem",
    "title": "Best-Fit Hyperplane",
    "section": "1-D Problem",
    "text": "1-D Problem\nLet us return to the housing dataset. We are trying to predict the selling price of a house based on some features or attributes. We started with six of these attributes. Along with the constant term, we have seven parameters for the linear model: \\(\\mathbf{w} =[w_{0} ,\\cdots ,w_{6} ]^{T}\\). If we think about the points in the labeled dataset, each of them can be represented as a point in seven dimensional space: \\([x_{1} ,\\cdots ,x_{6} ,y]^{T}\\). This is impossible to visualize. To make the visualization problem more tractable, let us reduce the number of features to one. In this case, the dataset is a set of points in 2D space. If there are seven houses in the dataset, then we have seven points:\n\\[\n\\begin{equation*}\n(x_{1} ,y_{1} ),\\cdots ,(x_{7} ,y_{7} )\n\\end{equation*}\n\\]\nEach house is represented by a pair of numbers: \\((x,y)\\). Let us say that \\(x\\) is the area and \\(y\\)​ is the selling price. If we plot these points in 2D space, we have:\n\nThe linear model is:\n\\[\n\\begin{equation*}\ny=w_{0} +w_{1} x\n\\end{equation*}\n\\]\nHere, \\(\\displaystyle y\\) and \\(\\displaystyle x\\) are used in the sense of general variables and not particular values. So, what does this model look like? This is nothing but the equation of a line:\n\nThe linear model is a line. Recall that we are trying to find that line (model) which minimizes the sum of the squared errors. First, let us see what the errors look like in this diagram. The error is the difference between the predicted selling price (\\(w_{0} +w_{1} x\\)) and the actual selling price \\((y)\\):\n\\[\n\\begin{equation*}\ne= |y-( w_{0} +w_{1} x)|\n\\end{equation*}\n\\]\n\nThe red points represent the predicted selling prices. The errors are the lengths of the vertical line segments connecting the blue points and the corresponding red points.\n\n\n\n\n\n\nRemark\n\n\n\nThe error is parallel to the \\(y\\)-axis and is not perpendicular to the line.\n\n\nFor various values of \\(\\mathbf{w} =[ w_{0} ,w_{1}]^{T}\\), we will get different lines. The line for which the sum of the squared errors is minimum is called the best-fit line. For a problem having one feature:\n\nthe dataset resides in 2D space\nthe model is a line, a one-dimensional object embedded in a two-dimensional space.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Best-Fit Hyperplane"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Best-fit.html#higher-dimensions",
    "href": "courses/MLF/notes/Part-1/Best-fit.html#higher-dimensions",
    "title": "Best-Fit Hyperplane",
    "section": "Higher dimensions",
    "text": "Higher dimensions\nWhat about higher dimensions? If there are two features, we will have a best-fit plane:\n\\[\n\\begin{equation*}\ny=w_{0} +w_{1} x_{1} +w_{2} x_{2}\n\\end{equation*}\n\\]\nHere, \\(x_{i}\\)s are the features, \\(y\\) is the predicted selling price of a house. Again, note that we are using \\(y\\) and \\(x_{i}\\)s in the sense of variables to define the equation of a curve. This plane is a two-dimensional object that is embedded in 3D space. In general, if there are \\(d\\) features, we have a \\(d\\)-dimensional hyperplane embedded in \\(d+1\\) dimensional space. The equation of this hyperplane is given by:\n\\[\n\\begin{equation*}\ny=w_{0} +w_{1} x_{1} +\\cdots +w_{d} x_{d}\n\\end{equation*}\n\\]\nVisualizing this hyperplane is beyond the scope of human abilities. So we typically restrict ourselves to 2 or 3 dimensions for building visual intuition.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Best-Fit Hyperplane"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-1/Best-fit.html#summary",
    "href": "courses/MLF/notes/Part-1/Best-fit.html#summary",
    "title": "Best-Fit Hyperplane",
    "section": "Summary",
    "text": "Summary\nIn a linear regression problem that has \\(d\\) features and a real valued label, we can think of the points as residing in a \\(d+1\\) dimensional space. We try to find a \\(d\\)-dimensional hyperplane embedded in this \\(d+1\\) dimensional space that minimizes the sum of the squared errors between the predicted labels and the actual labels. This hyperplane is the best-fit for the points in the labeled dataset.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 1",
      "Best-Fit Hyperplane"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Eigenbusiness.html",
    "href": "courses/MLF/notes/Part-2/Eigenbusiness.html",
    "title": "Eigenvalues and Eigenvectors",
    "section": "",
    "text": "Question\n\n\n\nWhat are eigenvectors and eigenvalues?",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Eigenvalues and Eigenvectors"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Eigenbusiness.html#motivation",
    "href": "courses/MLF/notes/Part-2/Eigenbusiness.html#motivation",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Motivation",
    "text": "Motivation\nLet us consider the following linear transformation from \\(\\mathbb{R}^{2}\\) to itself:\n\\[\n\\begin{equation*}\n\\mathbf{T} =\\begin{bmatrix}\n3 & 1\\\\\n0 & 2\n\\end{bmatrix}\n\\end{equation*}\n\\]\nWe will look at how this linear transformation operates on vectors from the geometric point of view. Specifically, we will record these two quantities:\n\ndirection of the vector before the transformation\ndirection of the vector after the transformation\n\n\nCase-1\nThe vector \\(\\mathbf{u} =\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix}\\) is transformed into \\(\\mathbf{Tu} =\\begin{bmatrix}\n4\\\\\n2\n\\end{bmatrix}\\).\n\nThe vector \\(\\mathbf{u}\\) was initially pointing in a particular direction. After the transformation, it points in a different direction. This case corresponds to vectors whose direction changes after the linear transformation.\n\n\nCase-2\nThe vector \\(\\mathbf{u} =\\begin{bmatrix}\n-1\\\\\n1\n\\end{bmatrix}\\) is transformed into \\(\\mathbf{Tu} =\\begin{bmatrix}\n-2\\\\\n2\n\\end{bmatrix}\\)​.\n\nThe direction of the vector \\(\\mathbf{Tu}\\) is the same as the direction of the vector \\(\\mathbf{u}\\). In other words, the linear transformation has preserved the direction of this vector. However, its magnitude has changed. In this case, the vector \\(\\mathbf{u}\\) has been stretched by a factor of \\(2\\).\n\\[\n\\begin{equation*}\n\\mathbf{Tu} =\\begin{bmatrix}\n-2\\\\\n2\n\\end{bmatrix} =2\\mathbf{u}\n\\end{equation*}\n\\]\nAnother example for this case is the basis vector \\(\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\\). For this vector:\n\\[\n\\begin{equation*}\n\\mathbf{Tu} =\\begin{bmatrix}\n3\\\\\n0\n\\end{bmatrix} =3\\mathbf{u}\n\\end{equation*}\n\\]\nVisually:\n\nThis case corresponds to vectors whose direction remains unchanged after the linear transformation.\n\n\n\n\n\n\nRemark\n\n\n\nWhat we mean by direction here is a line passing through the origin. Each vector \\(\\displaystyle \\mathbf{u}\\) specifies a direction. Every scalar multiple of this vector, \\(\\displaystyle k\\mathbf{u}\\), can be said to be in the same direction as \\(\\displaystyle \\mathbf{u}\\). Note that this holds even if \\(\\displaystyle k\\)​ is negative or zero.\n\n\nFor the remainder of this section, we will start using the symbol \\(\\mathbf{A}\\) instead of \\(\\mathbf{T}\\) and the symbol \\(\\mathbf{x}\\) instead of \\(\\mathbf{u}\\). This shift in notation is to emphasize that we will primarily focus on matrices while keeping linear transformation underlying them hidden from our view.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Eigenvalues and Eigenvectors"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Eigenbusiness.html#eigenvectors-and-eigenvalues",
    "href": "courses/MLF/notes/Part-2/Eigenbusiness.html#eigenvectors-and-eigenvalues",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Eigenvectors and Eigenvalues",
    "text": "Eigenvectors and Eigenvalues\nA non-zero vector which points in the same direction before and after the linear transformation is called an eigenvector. Since the direction of an eigenvector is unchanged by the transformation, it makes sense to look at the magnitude by which it is scaled after the transformation. This scalar value is called the eigenvalue. Formally:\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\mathbf{A}\\) be a square matrix of shape \\(n \\times n\\). A non-zero vector \\(\\mathbf{x} \\in \\mathbb{R}^{n}\\) is called an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\) if: \\[\n\\mathbf{Ax} = \\lambda \\mathbf{x}\n\\]\n\n\nThe eigenvector and the corresponding eigenvalue make up an eigenpair. For the matrix (linear transformation) that we have been working with, \\(\\left( 2,\\begin{bmatrix}\n-1\\\\\n1\n\\end{bmatrix}\\right)\\) and \\(\\left( 3,\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\\right)\\) are two eigenpairs.\n\nEigenspace\nConsider an arbitrary \\(n\\times n\\) matrix \\(\\mathbf{A}\\). How big is the space of eigenvectors? Let \\(\\mathbf{A}\\) be a matrix. If \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\), what can we say about the vector \\(2\\mathbf{x}\\)?\n\\[\n\\begin{equation*}\n\\mathbf{A} (2\\mathbf{x} )=2\\mathbf{Ax} =2\\lambda \\mathbf{x} =\\lambda (2\\mathbf{x} )\n\\end{equation*}\n\\]\nWe see that \\(2\\mathbf{x}\\) is also an eigenvector with eigenvalue \\(\\lambda\\). In fact, \\(k\\mathbf{x}\\) is an eigenvector for every non-zero \\(k\\). Coming from another direction, let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be two eigenvectors for the same eigenvalue \\(\\lambda\\). Then:\n\\[\n\\begin{equation*}\n\\mathbf{A} (\\mathbf{x} +\\mathbf{y} )=\\mathbf{Ax} +\\mathbf{Ay} =\\lambda \\mathbf{u} +\\lambda \\mathbf{v} =\\lambda (\\mathbf{x} +\\mathbf{y} )\n\\end{equation*}\n\\]\nTherefore, \\(\\mathbf{x} +\\mathbf{y}\\) is also an eigenvector of \\(\\mathbf{A}\\) with eigenvalue of \\(\\lambda\\). From these two observations, we see that the set of all eigenvectors with eigenvalue \\(\\lambda\\), along with the zero vector, is a subspace of \\(\\mathbb{R}^{n}\\).\n\n\n\n\n\n\nRemark\n\n\n\n\\(\\mathbf{0}\\) can never be an eigenvector. This is because if \\(\\mathbf{A0} =\\lambda \\mathbf{0}\\) then there is no fixed \\(\\lambda\\) that we can associate with \\(\\mathbf{0}\\). Therefore, eigenvectors are non-zero vectors.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Eigenvalues and Eigenvectors"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-2/Eigenbusiness.html#summary",
    "href": "courses/MLF/notes/Part-2/Eigenbusiness.html#summary",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Summary",
    "text": "Summary\nFor a square matrix \\(\\mathbf{A}\\) of shape \\(n \\times n\\), a non-zero vector \\(\\mathbf{x}\\) is an eigenvector with eigenvalue \\(\\lambda\\) if \\(\\mathbf{Ax} =\\lambda \\mathbf{x}\\). Eigenvectors are those directions of \\(\\displaystyle \\mathbb{R}^{n}\\) that are preserved under the linear transformation corresponding to this matrix.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 2",
      "Eigenvalues and Eigenvectors"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-3/Complex Numbers.html",
    "href": "courses/MLF/notes/Part-3/Complex Numbers.html",
    "title": "Complex Numbers",
    "section": "",
    "text": "A complex number is of the form \\(\\displaystyle a+ib\\), where \\(\\displaystyle a\\) and \\(\\displaystyle b\\) are real numbers and \\(\\displaystyle i^{2} =-1\\). Some examples are:\n\n\\(\\displaystyle 2+3i\\)\n\\(\\displaystyle 5-10i\\)\n\\(\\displaystyle -i\\)\n\\(\\displaystyle 5i\\)\n\\(\\displaystyle 4\\)\n\nThe set of complex numbers is denoted by \\(\\displaystyle \\mathbb{C}\\). Every real number is a complex number. But every complex number need not necessarily be a real number. In terms of set theoretic notation, \\(\\displaystyle \\mathbb{R} \\subset \\mathbb{C}\\) but \\(\\displaystyle \\mathbb{C} \\not \\subset \\mathbb{R}\\).\nA complex number has two parts to it: real and imaginary part. For the complex number \\(\\displaystyle 2+3i\\), the real part is \\(\\displaystyle 2\\) and the imaginary part is \\(\\displaystyle 3\\). So, any complex number \\(\\displaystyle z\\) can be written as \\(\\displaystyle z=\\text{Re}( z) +\\text{Im}( z) \\cdot i\\).\nWe can understand complex numbers geometrically by plotting the real part on the x-axis and the imaginary part on the y-axis.\n\nThis plane is called the complex plane, also called the Argand plane or Gauss plane.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 3",
      "Complex Numbers"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-3/Complex Numbers.html#introduction",
    "href": "courses/MLF/notes/Part-3/Complex Numbers.html#introduction",
    "title": "Complex Numbers",
    "section": "",
    "text": "A complex number is of the form \\(\\displaystyle a+ib\\), where \\(\\displaystyle a\\) and \\(\\displaystyle b\\) are real numbers and \\(\\displaystyle i^{2} =-1\\). Some examples are:\n\n\\(\\displaystyle 2+3i\\)\n\\(\\displaystyle 5-10i\\)\n\\(\\displaystyle -i\\)\n\\(\\displaystyle 5i\\)\n\\(\\displaystyle 4\\)\n\nThe set of complex numbers is denoted by \\(\\displaystyle \\mathbb{C}\\). Every real number is a complex number. But every complex number need not necessarily be a real number. In terms of set theoretic notation, \\(\\displaystyle \\mathbb{R} \\subset \\mathbb{C}\\) but \\(\\displaystyle \\mathbb{C} \\not \\subset \\mathbb{R}\\).\nA complex number has two parts to it: real and imaginary part. For the complex number \\(\\displaystyle 2+3i\\), the real part is \\(\\displaystyle 2\\) and the imaginary part is \\(\\displaystyle 3\\). So, any complex number \\(\\displaystyle z\\) can be written as \\(\\displaystyle z=\\text{Re}( z) +\\text{Im}( z) \\cdot i\\).\nWe can understand complex numbers geometrically by plotting the real part on the x-axis and the imaginary part on the y-axis.\n\nThis plane is called the complex plane, also called the Argand plane or Gauss plane.",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 3",
      "Complex Numbers"
    ]
  },
  {
    "objectID": "courses/MLF/notes/Part-3/Complex Numbers.html#algebra",
    "href": "courses/MLF/notes/Part-3/Complex Numbers.html#algebra",
    "title": "Complex Numbers",
    "section": "Algebra",
    "text": "Algebra\nThe following are some of the operations that we can do on complex numbers:\n\naddition (subtraction)\nmultiplication (division)\nabsolute value or modulus\nconjugate\n\nWe will look at each one of these operations.\n\nAddition\nConsider two complex numbers \\(\\displaystyle z_{1} =a_{1} +ib_{1}\\) and \\(\\displaystyle z_{2} =a_{2} +ib_{2}\\). Then:\n\\[\n\\begin{equation*}\nz_{1} +z_{2} =( a_{1} +a_{2}) +i( b_{1} +b_{2})\n\\end{equation*}\n\\]\nTo add two complex numbers, we add the real part separately and the imaginary part separately. For example:\n\\[\n\\begin{gather*}\n\\left( 1+3i\\right) +\\left( -5-2i\\right) =\\left( 1-5\\right) +i\\left( 3-2\\right)\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =-4+i\n\\end{gather*}\n\\]\nSubtraction follows trivially. To compute \\(\\displaystyle z_{1} -z_{2}\\), we can just compute \\(\\displaystyle z_{1} +\\left( -z_{2}\\right)\\).\n\n\nMultiplication\nConsider two complex numbers \\(\\displaystyle z_{1} =a_{1} +ib_{1}\\) and \\(\\displaystyle z_{2} =a_{2} +ib_{2}\\). Then:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nz_{1} z_{2} & =\\left( a_{1} +ib_{1}\\right)\\left( a_{2} +ib_{2}\\right)\\\\\n& =a_{1} a_{2} +a_{1}\\left( ib_{2}\\right) +\\left( ib_{1}\\right) a_{2} +i^{2} b_{1} b_{2}\\\\\n& =a_{1} a_{2} +i\\left( a_{1} b_{2}\\right) +i\\left( a_{2} b_{1}\\right) -b_{1} b_{2}\\\\\n& =\\left( a_{1} a_{2} -b_{1} b_{2}\\right) +i\\left( a_{1} b_{2} +a_{2} b_{1}\\right)\n\\end{aligned}\n\\end{equation*}\n\\]\nAs an example, if \\(\\displaystyle z_{1} =3-2i\\) and \\(\\displaystyle z_{2} =5+i\\), then:\n\\[\n\\begin{gather*}\n\\begin{aligned}\nz_{1} z_{2} & =\\left( 3\\times 5-\\left( -2\\right) \\times 1\\right) +i\\left( 3\\times 1+\\left( -2\\right) \\times 5\\right)\\\\\n& =17-7i\n\\end{aligned}\\\\\n\\end{gather*}\n\\]\nBefore moving to division, let us look at the idea of the absolute value of a complex number.\n\n\nAbsolute value\nThe absolute value of a complex number \\(\\displaystyle z=a+ib\\) is given by:\n\\[\n\\begin{equation*}\n|z|=\\sqrt{a^{2} +b^{2}}\n\\end{equation*}\n\\]\nGeometrically, we can think about it as the distance of \\(\\displaystyle z\\) from the origin. For example, if \\(\\displaystyle z=3+2i\\), then \\(\\displaystyle |z|=\\sqrt{3^{2} +2^{2}} =\\sqrt{13}\\). The complex number (blue dot) is at a distance of \\(\\displaystyle \\sqrt{13}\\) units from the origin.\n\nAnother term for the absolute value is modulus. The absolute value of a complex number is always going to be a non-negative real number.\n\n\nConjugate\nThe conjugate of a complex number \\(\\displaystyle z=a+ib\\) is denoted by \\(\\displaystyle \\overline{z}\\) and given as:\n\\[\n\\begin{equation*}\n\\overline{z} =a-ib\n\\end{equation*}\n\\]\nFor example, if \\(\\displaystyle z=3+2i\\) then \\(\\displaystyle \\overline{z} =3-2i\\). Geometrically, \\(\\displaystyle \\overline{z}\\) is the reflection of \\(\\displaystyle z\\)​ around the x-axis:\n\nThe following is an interesting relation:\n\\[\n\\begin{equation*}\nz\\overline{z} =|z|^{2}\n\\end{equation*}\n\\]\nTo see why this is true, consider any complex number \\(\\displaystyle z=a+ib\\). Then:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nz\\overline{z} & =\\left( a+ib\\right)\\left( a-ib\\right)\\\\\n& =a^{2} -a\\left( ib\\right) +\\left( ib\\right) a-i^{2} b^{2}\\\\\n& =a^{2} -i\\left( ab\\right) +i\\left( ab\\right) +b^{2}\\\\\n& =a^{2} +b^{2}\n\\end{aligned}\n\\end{equation*}\n\\]\nHere is an interesting observation related to conjugates that will be used quite extensively in subsequent lectures: \\(\\displaystyle z=\\overline{z}\\) if and only if \\(\\displaystyle z\\) is a real number. To see why this is true, let \\(\\displaystyle z=a+ib\\). If \\(\\displaystyle z\\) is a real number, then \\(\\displaystyle b=0\\), and it is obvious that \\(\\displaystyle z=\\overline{z} =a\\). On the other hand, if \\(\\displaystyle z=\\overline{z}\\), then we have:\n\\[\n\\begin{equation*}\n\\begin{aligned}\na+ib & =a-ib\\\\\ni\\left( 2b\\right) & =0\\\\\n\\Longrightarrow b & =0\n\\end{aligned}\n\\end{equation*}\n\\]\nIt follows that \\(\\displaystyle z=a\\) and hence a real number.\n\n\nDivision\nLet us try to divide two complex numbers \\(\\displaystyle z_{1} =a_{1} +ib_{1}\\) and \\(\\displaystyle z_{2} =a_{2} +ib_{2}\\) with \\(\\displaystyle z_{2} \\neq 0\\):\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\cfrac{z_{1}}{z_{2}} & =\\cfrac{a_{1} +ib_{1}}{a_{2} +ib_{2}}\\\\\n& =\\cfrac{a_{1} +ib_{1}}{a_{2} +ib_{2}{}} \\cdot \\cfrac{a_{2} -ib_{2}}{a_{2} -ib_{2}}\\\\\n& =\\cfrac{\\left( a_{1} +ib_{1}\\right)\\left( a_{2} -ib_{2}\\right)}{a_{2}^{2} +b_{2}^{2}}\\\\\n& =\\cfrac{a_{1} a_{2} +b_{1} b_{2}}{a_{2}^{2} +b_{2}^{2}} +i\\cdot \\cfrac{a_{2} b_{1} -a_{1} b_{2}}{a_{2}^{2} +b_{2}^{2}}\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nPolar Coordinates\nConsider a complex number \\(\\displaystyle z=a+ib\\)​\n\nUsing basic trigonometry, we have the following relations:\n\\[\n\\begin{gather*}\n\\begin{aligned}\nr & =\\sqrt{a^{2} +b^{2}}\\\\\n& \\\\\n\\cfrac{a}{r} & =\\cos \\theta \\\\\n& \\\\\n\\cfrac{b}{r} & =\\sin \\theta\n\\end{aligned}\\\\\n\\end{gather*}\n\\]\nAlternatively, we have:\n\\[\n\\begin{equation*}\n\\begin{aligned}\na & =r\\cos \\theta \\\\\nb & =r\\sin \\theta\n\\end{aligned}\n\\end{equation*}\n\\]\nSo, the complex number \\(\\displaystyle z\\) can be written as:\n\\[\n\\begin{equation*}\n\\begin{aligned}\nz & =a+ib\\\\\n& =r\\cos \\theta +i( r\\sin \\theta )\\\\\n& =r(\\cos \\theta +i\\sin \\theta )\n\\end{aligned}\n\\end{equation*}\n\\]\nThe following result is stated without proof. If \\(\\displaystyle e\\) is the familiar Euler’s number, then:\n\\[\n\\begin{equation*}\ne^{i\\theta } =\\cos \\theta +i\\sin \\theta\n\\end{equation*}\n\\]\nUsing this result, we can write \\(\\displaystyle z\\) as:\n\\[\n\\begin{equation*}\nz=re^{i\\theta }\n\\end{equation*}\n\\]\n\\(\\displaystyle r\\) is the absolute value of \\(\\displaystyle z\\) and \\(\\displaystyle \\theta\\) is called the argument of \\(\\displaystyle z\\). This way of representing a complex number using its modulus (absolute value) and argument is called the polar coordinate representation. Using this representation, the conjugate of \\(\\displaystyle z=a+ib\\) can be written as follows:\n\\[\n\\begin{equation*}\n\\begin{aligned}\n\\overline{z} & =a-ib\\\\\n& =r(\\cos \\theta -i\\sin \\theta )\\\\\n& =r[\\cos( -\\theta ) +i\\sin( -\\theta )]\\\\\n& =re^{-i\\theta }\n\\end{aligned}\n\\end{equation*}\n\\]\nWe have used the fact that \\(\\displaystyle \\cos( -\\theta ) =\\cos \\theta\\) and \\(\\displaystyle \\sin( -\\theta ) =-\\sin \\theta\\). The geometric interpretation of the conjugate under the polar coordinates is as follows:",
    "crumbs": [
      "Courses",
      "MLF",
      "Notes",
      "Part 3",
      "Complex Numbers"
    ]
  },
  {
    "objectID": "random/solving_equations.html",
    "href": "random/solving_equations.html",
    "title": "Reversible Transformations",
    "section": "",
    "text": "Solving equations can sometimes be tricky.\n\\[\nx^{2}=4\n\\]\nThe right approach to solve this question is as follows:\n\\[\n\\begin{aligned}\nx^{2}-4 & =0\\\\\n(x-2)(x+2) & =0\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nThe more complicated approach is to apply the transformation “take square root on both sides”, which is admissible since both sides are positive. Though this is not quite elegant, this will give us the same answer:\n\\[\n\\begin{aligned}\nx^{2} & =4\\\\\n|x| & =2\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nThe square root can be viewed as a function from \\([0,\\infty)\\) to \\([0,\\infty)\\). So each non-negative input has exactly one non-negative output. With this, the square root of \\(x^{2}\\) has to be \\(|x|\\), a non-negative quantity.\nSolving equations can get tricky when you start transforming equations. For example, consider the following sequence of transformations:\n\\[\n\\begin{aligned}\nx & =2\\\\\nx^{2} & =4 & \\text{squaring}\\\\\n|x| & =2 & \\text{square root}\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nHow can \\(x\\) be \\(2\\) and \\(-2\\)? The problem is with squaring both sides. The equation \\(x=2\\) and \\(x^{2}=4\\) are not the same. That is, \\(x=2\\implies x^{2}=4\\), but \\(x^{2}=4\\) does not lead to \\(x=2\\). Squaring is not a “reversible transformation”. We usually ignore \\(x=-2\\), calling it an extraneous solution, but what makes it extraneous is the fact that transformations may not be reversible.",
    "crumbs": [
      "Random",
      "Reversible Transformations"
    ]
  },
  {
    "objectID": "random/solving_equations.html#simple-equations",
    "href": "random/solving_equations.html#simple-equations",
    "title": "Reversible Transformations",
    "section": "",
    "text": "Solving equations can sometimes be tricky.\n\\[\nx^{2}=4\n\\]\nThe right approach to solve this question is as follows:\n\\[\n\\begin{aligned}\nx^{2}-4 & =0\\\\\n(x-2)(x+2) & =0\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nThe more complicated approach is to apply the transformation “take square root on both sides”, which is admissible since both sides are positive. Though this is not quite elegant, this will give us the same answer:\n\\[\n\\begin{aligned}\nx^{2} & =4\\\\\n|x| & =2\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nThe square root can be viewed as a function from \\([0,\\infty)\\) to \\([0,\\infty)\\). So each non-negative input has exactly one non-negative output. With this, the square root of \\(x^{2}\\) has to be \\(|x|\\), a non-negative quantity.\nSolving equations can get tricky when you start transforming equations. For example, consider the following sequence of transformations:\n\\[\n\\begin{aligned}\nx & =2\\\\\nx^{2} & =4 & \\text{squaring}\\\\\n|x| & =2 & \\text{square root}\\\\\nx & =\\pm2\n\\end{aligned}\n\\]\nHow can \\(x\\) be \\(2\\) and \\(-2\\)? The problem is with squaring both sides. The equation \\(x=2\\) and \\(x^{2}=4\\) are not the same. That is, \\(x=2\\implies x^{2}=4\\), but \\(x^{2}=4\\) does not lead to \\(x=2\\). Squaring is not a “reversible transformation”. We usually ignore \\(x=-2\\), calling it an extraneous solution, but what makes it extraneous is the fact that transformations may not be reversible.",
    "crumbs": [
      "Random",
      "Reversible Transformations"
    ]
  },
  {
    "objectID": "random/solving_equations.html#gaussian-elimination",
    "href": "random/solving_equations.html#gaussian-elimination",
    "title": "Reversible Transformations",
    "section": "Gaussian Elimination",
    "text": "Gaussian Elimination\nA classic example of a reversible transformations is row reduction that is employed in Gaussian elimination. Each row reduction operation is equivalent to pre-multiplying by an invertible (elementary) matrix. Going from a matrix \\(A\\) to its RREF form, \\(R\\), involves a sequence of matrix multiplications:\n\\[\nE_{m}\\cdots E_{1}A=R\n\\]\nThis can be written as \\(EA=R\\), where \\(E=E_{m}\\cdots E_{1}\\), and \\(E\\) is invertible. Solving \\(Ax=b\\), is therefore equivalent to solving \\(EAx=Eb\\). The reversibility of the transformation is what allows us to “temporarily forget” the original system and focus on the reduced system.",
    "crumbs": [
      "Random",
      "Reversible Transformations"
    ]
  },
  {
    "objectID": "random/solving_equations.html#optimization",
    "href": "random/solving_equations.html#optimization",
    "title": "Reversible Transformations",
    "section": "Optimization",
    "text": "Optimization\nA third example of a reversible transformation appears in optimization problems. Let us say we wish to find a rectangle with the longest diagonal with a given perimeter:\n\\[\n\\begin{aligned}\n\\max\\,\\,\\, & \\sqrt{x^{2}+y^{2}}\\\\\n\\\\2(x+y) & =p\\\\\nx,y & \\geq0\n\\end{aligned}\n\\]\nIt is easier to maximize \\(x^{2}+y^{2}\\) rather than \\(\\sqrt{x^{2}+y^{2}}\\):\n\\[\n\\begin{aligned}\n\\max\\,\\,\\, & x^{2}+y^{2}\\\\\n\\\\2(x+y) & =p\\\\\nx,y & \\geq0\n\\end{aligned}\n\\]\nWhat makes these two optimization problems equivalent? If we wish to be really rigorous, we can start by looking at the feasible set. This remains the same for both the problems as the constraints haven’t changed. Let us call this set \\(F\\). If \\((x_{1},y_{1})\\) maximizes the first version, what does it mean?\n\\[\n\\sqrt{x_{1}^{2}+y_{1}^{2}}\\geq\\sqrt{x^{2}+y^{2}},\\,\\,\\forall\\,(x,y)\\in F\n\\]\nSince both sides are positive, this means:\n\\[\nx_{1}^{2}+y_{1}^{2}\\geq x^{2}+y^{2},\\,\\forall\\,(x,y)\\in F\n\\]\nThus \\((x_{1},y_{1})\\) maximizes the second version. We can now go in the other direction and the result will be the same. The operation of “squaring” the objective happened to be reversible in this case. Another reversible transformation in the context of optimization is to maximize the log-likelihood instead of the likelihood.",
    "crumbs": [
      "Random",
      "Reversible Transformations"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA_variants.html",
    "href": "topics/machine_learning/notes/PCA_variants.html",
    "title": "PCA Variants",
    "section": "",
    "text": "Let \\(X\\) be a data-matrix of dimensions \\(d\\times n\\) with \\(d\\gg n\\). The covariance matrix is of size \\(d\\times d\\). The computational cost of eigen-decomposition of a \\(d\\times d\\) matrix is \\(O(d^{3})\\). So running PCA as it is may be prohibitively large for datasets with a huge number of dimensions. To get around this problem, we make use of the following fact:\n\n\n\n\n\n\nLemma\n\n\n\nIf \\((\\lambda,v)\\) is an eigenpair of \\(X^{T}X\\) with \\(\\lambda\\neq0\\), then \\((\\lambda,Xv)\\) is an eigenpair of \\(XX^{T}\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\left(X^{T}X\\right)v & =\\lambda v\\\\\nX\\left(X^{T}X\\right)v & =\\lambda(Xv)\\\\\n\\left(XX^{T}\\right)\\left(Xv\\right) & =\\lambda(Xv)\n\\end{aligned}\n\\]\nIf \\(Xv\\) is non-zero, then \\((\\lambda,Xv)\\) is an eigenpair of \\(XX^{T}\\). \\(Xv\\) is indeed non-zero. If \\(Xv=0\\), then \\(X^{T}Xv=0\\), which would mean that \\(v\\) is an eigenvector of \\(X^{T}X\\) with eigenvalue \\(0\\), which contradicts the fact that \\(\\lambda\\neq0\\).\n\n\n\n\n\nThis result is useful because it provides a way to get the eigenpairs of \\(XX^{T}\\) using the eigenpairs of \\(X^{T}X\\). Since \\(X^{T}X\\) is \\(n\\times n\\) and \\(n\\ll d\\), the cost of the eigen-decomposition of \\(X^{T}X\\) would be \\(O(n^{3})\\), which is more efficient than the \\(O(d^{3})\\) for \\(XX^{T}\\).\nLet us now direct our attention to the matrix \\(X^{T}X\\). First, we express \\(X\\) and \\(X^{T}\\) as follows:\n\\[\n\\begin{aligned}\nX & =\\begin{bmatrix}\\vert &  & \\vert\\\\\nx_{1} & \\cdots & x_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix} & X^{T}=\\begin{bmatrix}- & x_{1}^{T} & -\\\\\n& \\vdots\\\\\n- & x_{n}^{T} & -\n\\end{bmatrix}\n\\end{aligned}\n\\]\nEach element of \\(X^{T}X\\) then is just the inner product (dot product here) between two data-points. If this is not clear, go back to the usual way of multiplying two matrices. The \\(i^{th}\\) row of \\(X^{T}\\) with the \\(j^{th}\\) row of \\(X\\) combine to give the \\(ij^{th}\\) element of \\(X^{T}X\\):\n\\[\n\\left(X^{T}X\\right)_{ij}=x_{i}^{T}x_{j}\n\\]\nWe call this matrix \\(K\\):\n\\[\nK=X^{T}X\n\\]\nThis is often called the Gram matrix and is the matrix of pairwise inner products between the data-points and is of shape \\(n\\times n\\). Using a suitable algorithm, we can obtain the non-zero eigenvalues and the corresponding eigenvectors of \\(K\\). If the rank of \\(K\\) is \\(r\\), then there will be \\(r\\) non-zero eigenvalues. We can list the eignepairs in decreasing order of eigenvalues as:\n\\[\n(\\lambda_{1},v_{1}),\\cdots,(\\lambda_{r},v_{r})\n\\]\nNote that the eigenvectors are orthonormal here. Specifically, \\(||v_{i}||=1\\). Also note that \\(r\\leq\\min(d,n)\\) and for the specific case we are looking at, that of \\(d\\gg n\\), we have \\(r\\leq n\\). Once we have the eigenpairs of the Gram matrix, we can compute the eigenpairs of \\(XX^{T}\\) using the lemma proved in the beginning:\n\\[\n(\\lambda_{1},Xv_{1}),\\cdots,(\\lambda_{r},Xv_{r})\n\\]\nThese eigenvectors are orthogonal but not yet orthonormal. Hence we need to normalize them. Let us call the \\(i^{th}\\) normalized eigenvectors of \\(XX^{T}\\) \\(w_{i}\\):\n\\[\n\\begin{aligned}\nw_{i} & =\\cfrac{Xv_{i}}{\\left|\\left|Xv_{i}\\right|\\right|}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}X^{T}Xv_{i}}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}(X^{T}Xv_{i})}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}(\\lambda_{i}v_{i})}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{\\lambda_{i}}}\n\\end{aligned}\n\\]\nwhere we have used the fact that \\(v_{i}\\) is a normalized eigenvector of \\(X^{T}X\\) with a non-zero eigenvalue \\(\\lambda_{i}\\). Therefore, the eigenpairs of \\(XX^{T}\\) with normalized eigenvectors are:\n\\[\n\\left(\\lambda_{1},\\cfrac{Xv_{1}}{\\sqrt{\\lambda_{1}}}\\right),\\cdots,\\left(\\lambda_{r},\\cfrac{Xv_{r}}{\\sqrt{\\lambda_{r}}}\\right)\n\\]\nBut we are not interested in just \\(XX^{T}\\), but the covariance matrix \\(C\\), which is \\(C=\\cfrac{1}{n}XX^{T}\\). Scaling a matrix retains the eigenvectors but scales the eigenvalues. So the eigenpairs of \\(C\\) are:\n\\[\n\\left(\\cfrac{\\lambda_{1}}{n},\\cfrac{Xv_{1}}{\\sqrt{\\lambda_{1}}}\\right),\\cdots,\\left(\\cfrac{\\lambda_{r}}{n},\\cfrac{Xv_{r}}{\\sqrt{\\lambda_{r}}}\\right)\n\\]",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA Variants"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA_variants.html#datasets-with-huge-dimensions",
    "href": "topics/machine_learning/notes/PCA_variants.html#datasets-with-huge-dimensions",
    "title": "PCA Variants",
    "section": "",
    "text": "Let \\(X\\) be a data-matrix of dimensions \\(d\\times n\\) with \\(d\\gg n\\). The covariance matrix is of size \\(d\\times d\\). The computational cost of eigen-decomposition of a \\(d\\times d\\) matrix is \\(O(d^{3})\\). So running PCA as it is may be prohibitively large for datasets with a huge number of dimensions. To get around this problem, we make use of the following fact:\n\n\n\n\n\n\nLemma\n\n\n\nIf \\((\\lambda,v)\\) is an eigenpair of \\(X^{T}X\\) with \\(\\lambda\\neq0\\), then \\((\\lambda,Xv)\\) is an eigenpair of \\(XX^{T}\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\left(X^{T}X\\right)v & =\\lambda v\\\\\nX\\left(X^{T}X\\right)v & =\\lambda(Xv)\\\\\n\\left(XX^{T}\\right)\\left(Xv\\right) & =\\lambda(Xv)\n\\end{aligned}\n\\]\nIf \\(Xv\\) is non-zero, then \\((\\lambda,Xv)\\) is an eigenpair of \\(XX^{T}\\). \\(Xv\\) is indeed non-zero. If \\(Xv=0\\), then \\(X^{T}Xv=0\\), which would mean that \\(v\\) is an eigenvector of \\(X^{T}X\\) with eigenvalue \\(0\\), which contradicts the fact that \\(\\lambda\\neq0\\).\n\n\n\n\n\nThis result is useful because it provides a way to get the eigenpairs of \\(XX^{T}\\) using the eigenpairs of \\(X^{T}X\\). Since \\(X^{T}X\\) is \\(n\\times n\\) and \\(n\\ll d\\), the cost of the eigen-decomposition of \\(X^{T}X\\) would be \\(O(n^{3})\\), which is more efficient than the \\(O(d^{3})\\) for \\(XX^{T}\\).\nLet us now direct our attention to the matrix \\(X^{T}X\\). First, we express \\(X\\) and \\(X^{T}\\) as follows:\n\\[\n\\begin{aligned}\nX & =\\begin{bmatrix}\\vert &  & \\vert\\\\\nx_{1} & \\cdots & x_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix} & X^{T}=\\begin{bmatrix}- & x_{1}^{T} & -\\\\\n& \\vdots\\\\\n- & x_{n}^{T} & -\n\\end{bmatrix}\n\\end{aligned}\n\\]\nEach element of \\(X^{T}X\\) then is just the inner product (dot product here) between two data-points. If this is not clear, go back to the usual way of multiplying two matrices. The \\(i^{th}\\) row of \\(X^{T}\\) with the \\(j^{th}\\) row of \\(X\\) combine to give the \\(ij^{th}\\) element of \\(X^{T}X\\):\n\\[\n\\left(X^{T}X\\right)_{ij}=x_{i}^{T}x_{j}\n\\]\nWe call this matrix \\(K\\):\n\\[\nK=X^{T}X\n\\]\nThis is often called the Gram matrix and is the matrix of pairwise inner products between the data-points and is of shape \\(n\\times n\\). Using a suitable algorithm, we can obtain the non-zero eigenvalues and the corresponding eigenvectors of \\(K\\). If the rank of \\(K\\) is \\(r\\), then there will be \\(r\\) non-zero eigenvalues. We can list the eignepairs in decreasing order of eigenvalues as:\n\\[\n(\\lambda_{1},v_{1}),\\cdots,(\\lambda_{r},v_{r})\n\\]\nNote that the eigenvectors are orthonormal here. Specifically, \\(||v_{i}||=1\\). Also note that \\(r\\leq\\min(d,n)\\) and for the specific case we are looking at, that of \\(d\\gg n\\), we have \\(r\\leq n\\). Once we have the eigenpairs of the Gram matrix, we can compute the eigenpairs of \\(XX^{T}\\) using the lemma proved in the beginning:\n\\[\n(\\lambda_{1},Xv_{1}),\\cdots,(\\lambda_{r},Xv_{r})\n\\]\nThese eigenvectors are orthogonal but not yet orthonormal. Hence we need to normalize them. Let us call the \\(i^{th}\\) normalized eigenvectors of \\(XX^{T}\\) \\(w_{i}\\):\n\\[\n\\begin{aligned}\nw_{i} & =\\cfrac{Xv_{i}}{\\left|\\left|Xv_{i}\\right|\\right|}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}X^{T}Xv_{i}}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}(X^{T}Xv_{i})}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{v_{i}^{T}(\\lambda_{i}v_{i})}}\\\\\n\\\\ & =\\cfrac{Xv_{i}}{\\sqrt{\\lambda_{i}}}\n\\end{aligned}\n\\]\nwhere we have used the fact that \\(v_{i}\\) is a normalized eigenvector of \\(X^{T}X\\) with a non-zero eigenvalue \\(\\lambda_{i}\\). Therefore, the eigenpairs of \\(XX^{T}\\) with normalized eigenvectors are:\n\\[\n\\left(\\lambda_{1},\\cfrac{Xv_{1}}{\\sqrt{\\lambda_{1}}}\\right),\\cdots,\\left(\\lambda_{r},\\cfrac{Xv_{r}}{\\sqrt{\\lambda_{r}}}\\right)\n\\]\nBut we are not interested in just \\(XX^{T}\\), but the covariance matrix \\(C\\), which is \\(C=\\cfrac{1}{n}XX^{T}\\). Scaling a matrix retains the eigenvectors but scales the eigenvalues. So the eigenpairs of \\(C\\) are:\n\\[\n\\left(\\cfrac{\\lambda_{1}}{n},\\cfrac{Xv_{1}}{\\sqrt{\\lambda_{1}}}\\right),\\cdots,\\left(\\cfrac{\\lambda_{r}}{n},\\cfrac{Xv_{r}}{\\sqrt{\\lambda_{r}}}\\right)\n\\]",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA Variants"
    ]
  },
  {
    "objectID": "topics/machine_learning/notes/PCA_variants.html#non-linear-datasets",
    "href": "topics/machine_learning/notes/PCA_variants.html#non-linear-datasets",
    "title": "PCA Variants",
    "section": "Non-linear Datasets",
    "text": "Non-linear Datasets\nMost of the times, datasets are too complex to comfortably fit into a lower dimensional subspace. Datasets are in general structurally non-linear. Vanilla PCA may thus fail to capture a good representation. Or we may have to include a large number of principal components to capture \\(95\\%\\) of the variance, defeating the whole purpose of PCA.\nOne way to get around this problem is to think of a non-linear transformation of the features and hope that the transformed dataset has a linear structure in this larger feature space. This is not too naive an expectation. To see why this might work, consider the following dataset in \\(\\mathbb{R}^{2}\\):\n\n\n\nimage\n\n\nThere is a relationship between \\(x_{2}\\) and \\(x_{1}\\) which can roughly be captured as \\(x_{2}=x_{1}^{2}\\). Consider the transformation below:\n\\[\n\\phi(x_{1},x_{2})=\\begin{bmatrix}0\\\\\nx_{1}\\\\\nx_{1}^{2}-x_{2}\n\\end{bmatrix}\n\\]\nThis maps the data-points from \\(\\mathbb{R}^{2}\\) to \\(\\mathbb{R}^{3}\\), the plane to the space. But given the nature of the dataset, the transformed dataset would lie roughly along the y-axis in the 3d plane. Thus, the transformation \\(\\phi\\) has managed to convert a non-linear dataset in the original feature space to a linear dataset in the transformed space. In reality, things might not be so straightforward. Nevertheless, this gives us some intuition as to what transformations can do.\nWe can also look at one more transformation:\n\\[\n\\phi(x_{1,}x_{2})=\\begin{bmatrix}x_{1}\\\\\nx_{1}^{2}\\\\\nx_{2}\n\\end{bmatrix}\n\\]\nConsider the line \\(w^{T}x=0\\), where \\(w=\\begin{bmatrix}0\\\\\n1\\\\\n-1\n\\end{bmatrix}\\) in the transformed space. Most of the transformed data-points are going to lie roughly along this line.\nProceeding with this idea, let us assume that there is a (potentially non-linear) map \\(\\phi\\,\\,:\\,\\,\\)\\(\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{D}\\) that maps the feature space \\(\\mathbb{R}^{d}\\) to \\(\\mathbb{R}^{D}\\). The transformed data-matrix is \\(\\phi(X)\\):\n\\[\n\\phi(X)=\\begin{bmatrix}\\vert &  & \\vert\\\\\n\\phi(x_{1}) & \\cdots & \\phi(x_{n})\\\\\n\\vert &  & \\vert\n\\end{bmatrix}\n\\]\nThis is a \\(D\\times n\\) matrix. We can now proceed with PCA on the data-matrix \\(\\phi(X)\\). But what if \\(D\\) is huge? After all, we are trying to move to a higher dimension and it is quite likely that \\(D\\) would be huge for really complex datasets. So we go back to working with the Gram matrix like in section-1. There is another great advantage of this which will be discussed later. We will now define the Gram matrix as:\n\\[\nK=\\phi(X)^{T}\\phi(X)\n\\]\nThe eigenpairs of the Gram matrix are:\n\\[\n(\\lambda_{1},v_{1}),\\cdots,(\\lambda_{r},v_{r})\n\\]\nwhere \\(r\\) is the rank of \\(K\\). Note that \\(r\\leq\\min(D,n)\\). From this, we can get hold of the eigenvalues and eigenvectors of the covariance matrix \\(C=\\frac{1}{n}\\phi(X)\\phi(X)^{T}\\) as:\n\\[\n\\left(\\cfrac{\\lambda_{1}}{n},\\cfrac{\\phi(X)v_{1}}{\\sqrt{\\lambda_{1}}}\\right),\\cdots,\\left(\\cfrac{\\lambda_{r}}{n},\\cfrac{\\phi(X)v_{r}}{\\sqrt{\\lambda_{r}}}\\right)\n\\]\nAll this is good if we know the precise form of the transformation \\(\\phi\\). But most often, we may not know what this transformation is. Even if we know this, we would have to compute it for every data-point.\nThe computationally intensive step here is the eigen-decomposition of the Gram matrix. So let us take a closer look at it. Notice that it has the form \\(\\phi(X)^{T}\\phi(X)\\). For computing the Gram matrix, what we need is the pair-wise inner products in the transformed space. Is there any tool that will give us these inner products without having to explicitly compute \\(\\phi(x)\\)? There is such a tool and it is called a kernel.\nA kernel is a function \\(k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\), such that there exists a \\(\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{D}\\)\n\\[\nk(x_{1},x_{2})=\\phi(x_{1})^{T}\\phi(x_{2})\n\\]\nThis is a loose definition as the transformed space need not necessarily be finite dimensional. Besides, the inner product in the transformed space may not necessarily be the dot product.",
    "crumbs": [
      "Topics",
      "Machine Learning",
      "Notes",
      "PCA Variants"
    ]
  },
  {
    "objectID": "topics/probability_statistics/notes/Poisson.html",
    "href": "topics/probability_statistics/notes/Poisson.html",
    "title": "Poisson Distribution",
    "section": "",
    "text": "The support of the distribution is the set of non-negative integers: \\(\\{0,1,2,\\cdots\\}\\). The PMF is given as follows:\n\\[\nP(x)=\\cfrac{\\lambda^{x}}{x!}e^{-\\lambda}\n\\]\nHere \\(\\lambda\\) is a parameter governing the distribution. To verify that this is a valid PMF:\n\\[\n\\sum\\limits_{x=0}^{\\infty}\\cfrac{\\lambda^{x}}{x!}e^{-\\lambda}=e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}\\cfrac{\\lambda^{x}}{x!}=1\n\\]\nWe have used the Taylor series expansion for \\(e^{\\lambda}\\).",
    "crumbs": [
      "Topics",
      "Probability Statistics",
      "Notes",
      "Poisson Distribution"
    ]
  },
  {
    "objectID": "topics/probability_statistics/notes/Poisson.html#pmf",
    "href": "topics/probability_statistics/notes/Poisson.html#pmf",
    "title": "Poisson Distribution",
    "section": "",
    "text": "The support of the distribution is the set of non-negative integers: \\(\\{0,1,2,\\cdots\\}\\). The PMF is given as follows:\n\\[\nP(x)=\\cfrac{\\lambda^{x}}{x!}e^{-\\lambda}\n\\]\nHere \\(\\lambda\\) is a parameter governing the distribution. To verify that this is a valid PMF:\n\\[\n\\sum\\limits_{x=0}^{\\infty}\\cfrac{\\lambda^{x}}{x!}e^{-\\lambda}=e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}\\cfrac{\\lambda^{x}}{x!}=1\n\\]\nWe have used the Taylor series expansion for \\(e^{\\lambda}\\).",
    "crumbs": [
      "Topics",
      "Probability Statistics",
      "Notes",
      "Poisson Distribution"
    ]
  },
  {
    "objectID": "topics/probability_statistics/notes/Poisson.html#bionomial-to-poisson",
    "href": "topics/probability_statistics/notes/Poisson.html#bionomial-to-poisson",
    "title": "Poisson Distribution",
    "section": "Bionomial to Poisson",
    "text": "Bionomial to Poisson\nConsider the Binomial distribution, \\(\\text{Binomial}(n,p)\\):\n\\[\nP(x)={n \\choose x}p^{x}(1-p)^{n-x}\n\\]\nWe allow \\(n\\rightarrow\\infty\\) and \\(p\\rightarrow0\\) such that \\(np=\\lambda\\). Rewriting the Binomial distribution, we get:\n\\[\n\\begin{aligned}\nP(x) & =\\cfrac{n!}{x!(n-x)!}\\,p^{x}(1-p)^{n-x}\\\\\n\\\\ & =\\cfrac{n!}{x!(n-x)!}\\,\\left(\\cfrac{\\lambda}{n}\\right)^{x}\\left(1-\\cfrac{\\lambda}{n}\\right)^{n-x}\\\\\n\\\\ & =\\cfrac{\\lambda^{x}}{x!}\\cdot\\cfrac{(n-x+1)\\cdots n}{n^{x}}\\cdot\\left(1-\\cfrac{\\lambda}{n}\\right)^{n}\\left(1-\\cfrac{\\lambda}{n}\\right)^{-x}\n\\end{aligned}\n\\]\nWe can now compute the limits:\n\\[\n\\begin{aligned}\n\\lim_{n\\rightarrow\\infty}\\frac{(n-x+1)\\cdots n}{n^{x}} & =\\lim_{n\\rightarrow\\infty}\\left[1-\\left(\\cfrac{x-1}{n}\\right)\\right]\\cdots\\left[1-\\left(\\cfrac{x-x}{n}\\right)\\right]\\\\\n\\\\ & =1\\\\\n\\\\\\lim_{n\\rightarrow\\infty}\\left(1-\\cfrac{\\lambda}{n}\\right)^{n} & =e^{-\\lambda}\\\\\n\\\\\\lim_{n\\rightarrow\\infty}\\left(1-\\cfrac{\\lambda}{n}\\right)^{-x} & =1\n\\end{aligned}\n\\]\nPlugging these limits into the expression for \\(P(x)\\), we get the PMF of the Poisson distribution.",
    "crumbs": [
      "Topics",
      "Probability Statistics",
      "Notes",
      "Poisson Distribution"
    ]
  },
  {
    "objectID": "topics/linear_algebra/problems/problem-2.html",
    "href": "topics/linear_algebra/problems/problem-2.html",
    "title": "Problem-2",
    "section": "",
    "text": "Let \\(U\\) and \\(V\\) be two three dimensional subspaces of \\(\\mathbb{R}^{5}.\\) Show that there exists a non-zero vector \\(v\\in\\mathbb{R}^{5}\\) which lies in both \\(U\\) and \\(V\\).\n\nWe have:\n\\[\n\\text{dim}(U\\cap V)=\\text{dim}(U)+\\text{dim}(V)-\\text{dim}(U+V)\n\\]\nThe maximum value of \\(\\text{dim}(U+V)\\) is \\(5\\), since \\(U+V\\) is a subspace of \\(\\mathbb{R}^{5}.\\) Therefore, the minimum value of \\(\\text{dim}(U\\cap V)\\) is \\(3+3-5=1\\). Therefore, \\(U\\cap V\\) is a non-trivial subspace of \\(\\mathbb{R}^{5}\\). It follows that there exists some non-zero \\(v\\in\\mathbb{R}^{5}\\) that is in both \\(U\\) and \\(V\\).",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Problems",
      "Problem-2"
    ]
  },
  {
    "objectID": "topics/linear_algebra/notes/spectral.html",
    "href": "topics/linear_algebra/notes/spectral.html",
    "title": "Spectral Theorem (real version)",
    "section": "",
    "text": "If \\(A\\) is a real symmetric matrix, then it is orthogonally diagonalizable. In particular, there exists an orthogonal matrix \\(Q\\) and a diagonal matrix \\(D\\) such that:\n\\[\nA=QDQ^{T}\n\\]\nIf the order of \\(A\\) is \\(n\\), we can express \\(Q\\) and \\(D\\) as:\n\\[\nQ=\\begin{bmatrix}\\vert &  & \\vert\\\\\nq_{1} & \\cdots & q_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix},\\,\\,\\,\\,D=\\begin{bmatrix}\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{bmatrix}\n\\]\nThe columns of \\(Q\\) are the eigenvectors of \\(A\\). The corresponding eigenvalues are to be found on the diagonals of \\(D\\). Therefore, \\((\\lambda_{i},q_{i})\\) is an eigenpair of \\(A\\), that is, \\(Aq_{i}=\\lambda_{i}q_{i}\\). Also, \\(q_{1},\\cdots,q_{n}\\) form an orthonormal basis for \\(\\mathbb{R}^{n}\\). Recall that:\n\\[\nQ^{T}Q=QQ^{T}=I\n\\]\nWe can also express \\(A\\) as the sum of \\(n\\) outer products:\n\\[\nA=\\sum\\limits_{i=1}^{n}\\lambda_{i}q_{i}q_{i}^{T}\n\\]\nOne way of seeing this is to treat the product \\(QDQ^{T}\\) as \\((QD)Q^{T}\\). Since \\(D\\) is diagonal, \\(QD\\) would just result in scaling the columns of \\(Q\\) by the corresponding diagonal entries in \\(D\\):\n\\[\nQD=\\begin{bmatrix}\\vert &  & \\vert\\\\\n\\lambda_{1}q_{1} & \\cdots & \\lambda_{n}q_{n}\\\\\n\\vert &  & \\vert\n\\end{bmatrix}\n\\]\nThe outer product now follows.\nHere is an example of a symmetric matrix and its spectral decomposition:\n\\[\nA=\\begin{bmatrix}2 & 0 & -1\\\\\n0 & 2 & 0\\\\\n-1 & 0 & 2\n\\end{bmatrix},\\,\\,\\,\\,Q=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 0 & 1\\\\\n0 & 1 & 0\\\\\n1 & 0 & -1\n\\end{bmatrix},\\,\\,\\,\\,D=\\begin{bmatrix}1 & 0 & 0\\\\\n0 & 2 & 0\\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Topics",
      "Linear Algebra",
      "Notes",
      "Spectral Theorem (real version)"
    ]
  }
]